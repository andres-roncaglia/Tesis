---
title: "Tesis"
format: pdf
echo: False
warning: False
---


```{python}
# CARGA DE LIBRERIAS ------------------

# Para el manejo de estructuras de datos
import pandas as pd
import numpy as np

# Para dar formato fecha
from datetime import datetime

# Para graficos
import matplotlib.pyplot as plt
import seaborn as sns

# Para la transformacion de box y cox
from scipy import stats

# Para el calculo de autocorrelaciones
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.stattools import acf, pacf

# Para ajustar modelos arima
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.arima.model import ARIMAResults
from pmdarima import auto_arima

# Para calcular el test de ljung box
import statsmodels.api as sm

# Para el test de kolmogorov-smirnov
import scipy.stats as stats

# Para calcular el error medio cuadratico
from sklearn.metrics import mean_squared_error

# Para cargar las claves
import creds

# Para time GPT
from nixtla import NixtlaClient
nixtla_client = NixtlaClient(api_key= creds.api_key)

# Para realizar consultas a la base de datos
import urllib.parse
import requests

# Para guardar y cargar los modelos
from joblib import dump, load

# Para estandarizar los datos
from sklearn.preprocessing import MinMaxScaler

# Para medir el tiempo que tarda en ajustar los modelos
import time
```



```{python}
# Creamos una función para realizar llamadas a la API de datos argentina
def get_api_call(ids, **kwargs):
    API_BASE_URL = "https://apis.datos.gob.ar/series/api/"
    kwargs["ids"] = ",".join(ids)
    return "{}{}?{}".format(API_BASE_URL, "series", urllib.parse.urlencode(kwargs))
```


```{python}
# Llamada a la API y carga de datos

api_call = get_api_call(["364.3_LITORAL_GAGAS__11"], start_date="2016-01")

json = requests.get(api_call).json()

datos = pd.DataFrame(json['data'], columns = ['fecha', 'consumo'])

datos['fecha'] = pd.to_datetime(datos['fecha'], format='%Y-%m-%d')

datos.columns = ['ds', 'y']
```


```{python}
datos = pd.read_csv(filepath_or_buffer= 'Datos/exportaciones-actividad-saldocomercial-rangos-exportacion-empresa-exportadora-mensual.csv')

datos = datos[['indice_tiempo','litoral_gas']]

datos.columns = ['fecha', 'consumo']

datos['fecha'] = pd.to_datetime(datos['fecha'], format='%Y-%m-%d')

datos.dropna(inplace=True)
```

```{python}
# Tabla con los primeros y ultimos datos
print(datos.head(3),datos.tail(3))

# Grafico los datos
plt.plot(datos['ds'], datos['y'], marker = '.')
plt.title('Consumo de gas natural')
plt.xlabel('Año')
plt.ylabel('Consumo (millones de metros cúbicos)')
plt.show()

```


# Predicciones con machine learning


```{python}
# Prueba con sktime

from sktime.forecasting.base import ForecastingHorizon

from sktime.forecasting.fbprophet import Prophet

from sktime.forecasting.arima import AutoARIMA

from sktime.forecasting.chronos import ChronosForecaster

from sktime.forecasting.neuralforecast import NeuralForecastLSTM

from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

from sktime.split import temporal_train_test_split

from neuralforecast import NeuralForecast

from neuralforecast.models import LSTM

from neuralforecast.losses.pytorch import MQLoss

```

```{python}
# Creamos un dataframe donde vasmos a guardar todos los resultados
metricas = pd.DataFrame(columns=(['Modelo', 'MAPE', 'Interval Score', 'Tiempo']))
```

```{python}
# Interval Score

def interval_score(obs, lower, upper, alpha):

    upper = upper.values
    lower = lower.values
    obs = obs.values

    # Ancho del intervalo
    W = upper - lower

    # Penalización por sobre-estimación
    O = 2/alpha * np.maximum(lower - obs, 0)

    # Penalización por sub-estimación
    U = 2/alpha * np.maximum(obs - upper, 0)

    # Interval Score
    score = np.average(W + O + U)

    return score



```

```{python}
# Tuner de hiperparametros

def tuner(forecaster_fun, datos, parametros = '', metrica = 'MAPE', alpha = 0.05, long_pred = 12):

    # Dividimos el conjunto de datos que queremos pronosticar
    corte = len(datos)-long_pred

    datos.columns = ['ds', 'y']

    datos_train = datos[:corte]
    datos_test = datos[corte:]

    # Dado que estamos ajustando parametros, no podemos usar el conjunto de entrenamiento en su totalidad, debemos particionarlo para evitar el sobreajuste
    train_y, test_y = temporal_train_test_split(datos_train['y'], test_size=0.1)

    num_pred = ForecastingHorizon(test_y.index, is_relative=False)

    # Si no se definen parametros, simplemente se devuelve el modelo base ajustado
    if not isinstance(parametros, dict):
        
        fh = ForecastingHorizon(datos_test.index, is_relative=False)

        timer_comienzo = time.time() # Empiezo a medir cuanto tarda en ajustar
        forecaster = forecaster_fun().fit(datos_train['y'])
          
        # Obtenemos predicciones
        pred = forecaster.predict(fh)
        pred_int = forecaster.predict_interval(fh, coverage=1-alpha*2)
        timer_final = time.time()
        tiempo = timer_final - timer_comienzo

        # Calculamos MAPE
        mape_final = mean_absolute_percentage_error(datos_test['y'], pred)

        # Calculamos Interval Score
        pred_int.columns = ['lower', 'upper']
        score_final = interval_score(obs=datos_test['y'], lower=pred_int['lower'], upper=pred_int['upper'], alpha = alpha)

        # Graficamos el pronostico
        datos = pd.concat([datos_train, datos_test])
        plt.plot(datos['ds'], datos['y'])
        sns.lineplot(x = datos['ds'], y= pred, color = 'red', label = 'Prediccion')
        plt.fill_between(datos_test['ds'], pred_int['lower'], pred_int['upper'], color = 'red', alpha = 0.3, label = f'IC: {1-alpha*2}%')

        # Devolvemos las predicciones
        return pred, mape_final, score_final, tiempo, []


    
    mapes = []
    scores = []
    nombre_cols = list(parametros.keys())

    # Expandimos la grilla de parametros para evaluar todas las opciones
    ## Paso 1: Pasar los valores del diccionario como vectores de una lista
    grilla_lista = list(parametros.values())

    ## Paso 2: Expandimos la lista por todos los parametros. '*' sirve para desempaquetar los elementos de la lista, en lugar de pasarse como '[elem1, elem2]' se pasan como 'elem1, elem2'
    grilla_expan = list(map(np.ravel, np.meshgrid(*grilla_lista)))

    ## Paso 3: Guardamos todo como un Dataframe
    grilla = pd.DataFrame(np.array(grilla_expan).T, columns= nombre_cols)

    # Vamos a probar cada combinacion de filas 
    for j in range(0,grilla.shape[0]):

        # Primero pasamos la fila como diccionario para usar los argumentos
        kwargs = grilla.iloc[j].to_dict()
            

        # TimeGPT no necesita una funcion para ajustar el modelo
        
        if forecaster_fun.__name__ == 'forecast':
            gpttrain_y = pd.DataFrame({'y': train_y, 'ds': datos_train['ds'][:len(train_y)]})
            forecaster = forecaster_fun(df = gpttrain_y, h = len(test_y), time_col= 'ds',
            target_col= 'y', freq= 'M', level=[1-alpha*2], **kwargs)

            # Calculamos MAPE
            mape = mean_absolute_percentage_error(test_y, forecaster['TimeGPT'])
            mapes.append(mape)

            # Calculamos interval score
            score = interval_score(obs=test_y, lower= forecaster[f'TimeGPT-lo-{1-alpha*2}'], upper= forecaster[f'TimeGPT-hi-{1-alpha*2}'], alpha = alpha)
            scores.append(score)

            continue


        # Luego especificamos el pronosticador
        forecaster = forecaster_fun(**kwargs)


        # Ajustamos el modelo
        try:
            forecaster.fit(train_y)
          
            # Obtenemos predicciones
            pred = forecaster.predict(num_pred)
            pred_int = forecaster.predict_interval(num_pred, coverage=1-alpha*2)

            # Calculamos MAPE
            mape = mean_absolute_percentage_error(test_y, pred)
            mapes.append(mape)

            # Calculamos Interval Score
            pred_int.columns = ['lower', 'upper']
            score = interval_score(obs=test_y, lower=pred_int['lower'], upper=pred_int['upper'], alpha = 1-alpha*2)
            scores.append(score)
        except:
            # Si el modelo falla en ajustar, asignamos NaN
            mapes.append(np.NaN)
            scores.append(np.NaN)

    # Una vez probamos todas las opciones, vemos con cual modelo se obtuvo el menor error
    if metrica == 'MAPE': 
        mejor_combinacion = mapes.index(np.nanmin(mapes))
    else :
        mejor_combinacion = scores.index(np.nanmin(scores))
        

    # Por ultimo ajustamos el mejor modelo con todo el conjunto de entrenamiento:

    kwargs = grilla.iloc[mejor_combinacion].to_dict()

    # Time GPT no necesita llamar a una funcion para ajustar el modelo

    if forecaster_fun.__name__ == 'forecast':
        timer_comienzo = time.time() # Empiezo a medir cuanto tarda en ajustar
        
        forecaster = forecaster_fun(df = datos_train, h = len(datos_test), time_col= 'ds',
            target_col= 'y', freq= 'M', level=[1-alpha*2], **kwargs)
        
        timer_final = time.time()
        tiempo = timer_final - timer_comienzo

        pred = forecaster['TimeGPT']
        pred_int = forecaster[[f'TimeGPT-lo-{1-alpha*2}', f'TimeGPT-hi-{1-alpha*2}']]
    
    else: 
        forecaster = forecaster_fun(**kwargs)
        
        timer_comienzo = time.time() # Empiezo a medir cuanto tarda en ajustar
        forecaster.fit(datos_train['y'])
            
        # Obtenemos predicciones
        fh = ForecastingHorizon(datos_test.index, is_relative=False)
        pred = forecaster.predict(fh)
        pred_int = forecaster.predict_interval(num_pred, coverage=1-alpha*2)

        timer_final = time.time()
        tiempo = timer_final - timer_comienzo
    
    # Agregamos a la grilla los mapes de cada combinacion
    grilla['MAPE'] = mapes
    mape_final = mean_absolute_percentage_error(datos_test['y'], pred)

    # Agregamos a la grilla los scores de cada combinacion
    grilla['Interval Scores'] = scores
    pred_int.columns = ['lower', 'upper']
    score_final = interval_score(obs=datos_test['y'], lower=pred_int['lower'], upper=pred_int['upper'], alpha = alpha)

    if metrica == 'MAPE':
        grilla['Seleccionado'] = mapes == np.nanmin(mapes)
    else:
        grilla['Seleccionado'] = scores == np.nanmin(scores)

    # Graficamos el pronostico
    datos = pd.concat([datos_train, datos_test])
    plt.plot(datos['ds'], datos['y'])
    if forecaster_fun.__name__ == 'forecast':
        sns.lineplot(x = forecaster['ds'], y= pred, color = 'red', label = 'Prediccion')
        plt.fill_between(forecaster['ds'], pred_int['lower'], pred_int['upper'], color = 'red', alpha = 0.3)

        print(pred_int)   
    else : 
        sns.lineplot(x = datos_test['ds'], y= pred, color = 'red', label = 'Prediccion')
        plt.fill_between(datos_test['ds'], pred_int['lower'], pred_int['upper'], color = 'red', alpha = 0.3)

    # Devolvemos las predicciones
    return pred, mape_final, score_final, tiempo, grilla

```

# Tradicionales

```{python}
# AUTOARIMA

parametros = {'start_p': [0], 'start_q' : [0],
    'sp':[12], 'max_p':[2], 'max_q':[2],  'suppress_warnings':[True]}

pred, mape, score, tiempo, resultados = tuner(forecaster_fun=AutoARIMA, parametros=parametros, datos_train=datos_train, datos_test=datos_test)

metricas.loc[len(metricas)] = ['AutoARIMA', mape, score, tiempo]
```


```{python}
# PROPHET

pred, mape, score, tiempo, resultados = tuner(forecaster_fun=Prophet, datos=datos)

metricas.loc[len(metricas)] = ['Prophet', mape, score, tiempo]
```

# Machine learning

```{python}
# XGBoost
from xgboost import XGBRegressor
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV

# Creamos variables que pueden ayudar a pronosticar
datos_ml = datos.copy()

datos_ml['month'] = datos_ml['ds'].dt.month
datos_ml['year'] = datos_ml['ds'].dt.year
datos_ml["promedio_3_meses"] = datos_ml["y"].shift(1).rolling(window=3).mean() # Promedio en una ventana de 3 meses
datos_ml["desvio_3_meses"] = datos_ml["y"].shift(1).rolling(window=3).std() # Desvio en una ventana de 3 meses
datos_ml["lag_1"] = datos_ml["y"].shift(1)
datos_ml["lag_2"] = datos_ml["y"].shift(2)
datos_ml["lag_12"] = datos_ml["y"].shift(12)

# Dividimos en conjunto de entrenamiento y testeo

corte = len(datos_ml)-12

datos_ml_train = datos_ml[:corte]
datos_ml_test = datos_ml[corte:]

X_train = datos_ml_train[['month', 'year', 'promedio_3_meses', 'desvio_3_meses', 'lag_1', 'lag_2', 'lag_12']]
X_test = datos_ml_test[['month', 'year', 'promedio_3_meses', 'desvio_3_meses', 'lag_1', 'lag_2', 'lag_12']]

# XGBoost
cv_split = TimeSeriesSplit(n_splits=4)
model = XGBRegressor()
parameters = {
    "max_depth": [3, 4, 5, 7, 10],
    "learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3],
    "n_estimators": [10, 20, 50, 100, 300, 500, 700],
    "colsample_bytree": [0.3, 0.5, 0.7]
}


grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)
grid_search.fit(X_train, datos_ml_train['y'])

# Prediccion
y_pred_xgb = grid_search.predict(X_test)
y_ic_xgb = grid_search.predict_proba(X_test)

# MAPE y Interval Score
mape = mean_absolute_percentage_error(datos_ml_test['y'], y_pred_xgb)
score = interval_score(datos_ml_test['y'], )

metricas.loc[len(metricas)] = ['XGBoost', mape]
```

```{python}
# LightGBM
cv_split = TimeSeriesSplit(n_splits=3)
model = lgb.LGBMRegressor()
parameters = {
    "max_depth": [3, 4, 5, 7, 10],
    "learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3],
    "n_estimators": [10, 20, 50, 100, 300, 500, 700],
    "colsample_bytree": [0.3, 0.5, 0.7]
}


grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)
grid_search.fit(X_train, datos_train['y'])

# Prediccion
y_pred_lgb = grid_search.predict(X_test)

mape = mean_absolute_percentage_error(datos_test['y'], y_pred_lgb)

metricas.loc[len(metricas)] = ['LightGBM', mape]
```

# Deep learning

```{python}
# LSTM

corte = len(datos)-12

datos.columns = ['ds', 'y']

datos_train = datos[:corte]
datos_test = datos[corte:]


datos_train['unique_id'] = 0

# Definición del modelo

nf = NeuralForecast(
    models=[LSTM(h=12, 
                 input_size=3*12,
                 max_steps = 200,
                 loss = MQLoss(level = [90])
                 )
    ],
    freq='M'
)

# Ajuste del modelo
c = time.time()
nf.fit(df=datos_train)
f = time.time()

tiempo = f-c

# Predicción

Y_hat_df = nf.predict()

# Calculo de MAPE e Interval Score

mape = mean_absolute_percentage_error(datos_test['y'], Y_hat_df['LSTM-median'])
score = interval_score(obs = datos_test['y'], lower=Y_hat_df['LSTM-lo-90'], upper=Y_hat_df['LSTM-hi-90'], alpha=0.2)

metricas.loc[len(metricas)] = ['LightGBM', mape, score, tiempo]

```

```{python}
# TIME GPT

parametros = {
    'finetune_loss' : ['mape']}

pred, mape, score, tiempo, resultados = tuner(forecaster_fun=nixtla_client.forecast, parametros= parametros, datos_train=datos_train, datos_test=datos_test)

metricas.loc[len(metricas)] = ['TimeGPT', mape, score, tiempo]

```

```{python}
# Chronos

parametros = {'model_path': ['amazon/chronos-t5-tiny']}

pred, mape, tiempo, resultados = tuner(forecaster_fun=ChronosForecaster, parametros=parametros, datos_train=datos_train, datos_test=datos_test)

metricas.loc[len(metricas)] = ['Chronos', mape, tiempo]
```

```{python}
# Chronos - Autogluon

from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor

datos_gluon_train = datos_train.copy()

datos_gluon_train.columns = ['timestamp', 'target', 'item_id']

datos_gluon_train = TimeSeriesDataFrame(datos_gluon_train)

predictor = TimeSeriesPredictor(prediction_length=12).fit(datos_gluon_train, presets="bolt_tiny")

predictions = predictor.predict(datos_gluon_train)
```

```{python}

def is_pickleable(obj):
    """Check if an object can be pickled."""
    try:
        pickle.dumps(obj)
        return True
    except (pickle.PickleError, AttributeError, TypeError):
        return False

def save_env(filename="modelos_consumo_gas.pkl"):
    global_vars = {k: v for k, v in globals().items() if not k.startswith("__") and is_pickleable(v)}
    with open(filename, "wb") as f:
        pickle.dump(global_vars, f)

def load_env(filename="modelos_consumo_gas.pkl"):
    with open(filename, "rb") as f:
        global_vars = pickle.load(f)
        globals().update(global_vars)

# Usage
save_env()
load_env()


```


# ARREGLAR

- La funcion score interval a veces da error porquelos vectores son de distinto largo, nose que esta pasando

- Usar timegpt en la funcion de hiperparametros no grafica los intervalos

- Chronos no devuelve intervalos
