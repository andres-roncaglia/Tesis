---
title: "Tesis"
format: pdf
echo: False
warning: False
---


```{python}
# CARGA DE LIBRERIAS ------------------

# Para el manejo de estructuras de datos
import pandas as pd
import numpy as np

# Para dar formato fecha
from datetime import datetime

# Para graficos
import matplotlib.pyplot as plt
import seaborn as sns

# Para la transformacion de box y cox
from scipy import stats

# Para el calculo de autocorrelaciones
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.stattools import acf, pacf

# Para ajustar modelos arima
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.arima.model import ARIMAResults
from pmdarima import auto_arima

# Para calcular el test de ljung box
import statsmodels.api as sm

# Para el test de kolmogorov-smirnov
import scipy.stats as stats

# Para cargar las claves
import creds

# Para time GPT
from nixtla import NixtlaClient
nixtla_client = NixtlaClient(api_key= creds.api_key)

# Para realizar consultas a la base de datos
import urllib.parse
import requests

# Para guardar y cargar los modelos
from joblib import dump, load

# Para medir el tiempo que tarda en ajustar los modelos
import time

# Cargamos funciones
from Funciones import get_api_call, interval_score, save_env, load_env, dict_expand, TGPT_tune
```

```{python}
# Definimos una semilla
seed = 11072001
```


```{python}
# Llamada a la API y carga de datos

api_call = get_api_call(["364.3_LITORAL_GAGAS__11"], start_date="2016-01")

json = requests.get(api_call).json()

datos = pd.DataFrame(json['data'], columns = ['fecha', 'consumo'])

datos['fecha'] = pd.to_datetime(datos['fecha'], format='%Y-%m-%d')

datos.columns = ['ds', 'y']
```


```{python}
# datos = pd.read_csv(filepath_or_buffer= 'Datos/exportaciones-actividad-saldocomercial-rangos-exportacion-empresa-exportadora-mensual.csv')

# datos = datos[['indice_tiempo','litoral_gas']]

# datos.columns = ['fecha', 'consumo']

# datos['fecha'] = pd.to_datetime(datos['fecha'], format='%Y-%m-%d')

# datos.dropna(inplace=True)
```

```{python}
# Tabla con los primeros y ultimos datos
print(datos.head(3),datos.tail(3))

# Grafico los datos
plt.plot(datos['ds'], datos['y'], marker = '.')
plt.title('Consumo de gas natural')
plt.xlabel('Año')
plt.ylabel('Consumo (millones de metros cúbicos)')
plt.show()

```


```{python}
# Opciones de pronóstico

# Largo del pronóstico

long_pred = 12

# Nivel de significación del intervalo de predicción

alpha = 0.2



# Dividimos el conjunto de datos que queremos pronosticar
corte = len(datos)-long_pred

datos.columns = ['ds', 'y']

datos_train = datos[:corte]
datos_test = datos[corte:]


```

# Predicciones con machine learning


```{python}
# Prueba con sktime

from sktime.forecasting.base import ForecastingHorizon

from sktime.forecasting.fbprophet import Prophet

from sktime.forecasting.arima import AutoARIMA

from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

from sktime.split import temporal_train_test_split

from neuralforecast import NeuralForecast

from neuralforecast.models import LSTM

from neuralforecast.losses.pytorch import MQLoss

```

```{python}
# Creamos un dataframe donde vamos a guardar todos los resultados
metricas = pd.DataFrame(columns=(['Modelo', 'MAPE', 'Interval Score', 'Tiempo']))
```


# Tradicionales

```{python}
# AUTOARIMA

# Definimos el horizonte de pronostico
fh = ForecastingHorizon(datos_test.index, is_relative=False)


# Ajustamos el modelo
timer_comienzo = time.time() # Empiezo a medir cuanto tarda en ajustar
forecaster = AutoARIMA(
    start_p= 0, start_q= 0, sp= 12, max_p=3, max_q= 3, suppress_warnings= True, max_d=2, max_D= 2
).fit(datos_train['y'])
  
# Obtenemos predicciones
pred = forecaster.predict(fh)
pred_int = forecaster.predict_interval(fh, coverage=1-alpha/2)
timer_final = time.time()
tiempo = timer_final - timer_comienzo

# Calculamos MAPE
mape = mean_absolute_percentage_error(datos_test['y'], pred)

# Calculamos Interval Score
pred_int.columns = ['lower', 'upper']
score = interval_score(obs=datos_test['y'], lower=pred_int['lower'], upper=pred_int['upper'], alpha = alpha)

# Graficamos el pronostico
plt.plot(datos['ds'], datos['y'])
sns.lineplot(x = datos['ds'], y= pred, color = 'red', label = 'Prediccion')
plt.fill_between(
    datos_test['ds'], pred_int['lower'], pred_int['upper'], color = 'red', alpha = 0.3, label = f'IC: {1-alpha/2}%')


metricas.loc[len(metricas)] = ['AutoARIMA', mape, score, tiempo]

```


```{python}
# PROPHET

# Definimos el horizonte de pronostico
fh = ForecastingHorizon(datos_test.index, is_relative=False)


# Ajustamos el modelo
timer_comienzo = time.time() # Empiezo a medir cuanto tarda en ajustar
forecaster = Prophet().fit(datos_train['y'])
  
# Obtenemos predicciones
pred = forecaster.predict(fh)
pred_int = forecaster.predict_interval(fh, coverage=1-alpha/2)
timer_final = time.time()
tiempo = timer_final - timer_comienzo

# Calculamos MAPE
mape = mean_absolute_percentage_error(datos_test['y'], pred)

# Calculamos Interval Score
pred_int.columns = ['lower', 'upper']
score = interval_score(obs=datos_test['y'], lower=pred_int['lower'], upper=pred_int['upper'], alpha = alpha)

# Graficamos el pronostico
plt.plot(datos['ds'], datos['y'])
sns.lineplot(x = datos['ds'], y= pred, color = 'red', label = 'Prediccion')
plt.fill_between(
    datos_test['ds'], pred_int['lower'], pred_int['upper'], color = 'red', alpha = 0.3, label = f'IC: {1-alpha/2}%')


metricas.loc[len(metricas)] = ['Prophet', mape, score, tiempo]
```

# Machine learning

```{python}
# XGBoost
from xgboost import XGBRegressor
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV

# Creamos variables que pueden ayudar a pronosticar
datos_ml = datos.copy()

datos_ml['month'] = datos_ml['ds'].dt.month
datos_ml['year'] = datos_ml['ds'].dt.year
datos_ml["promedio_3_meses"] = datos_ml["y"].shift(1).rolling(window=3).mean() # Promedio en una ventana de 3 meses
datos_ml["desvio_3_meses"] = datos_ml["y"].shift(1).rolling(window=3).std() # Desvio en una ventana de 3 meses
datos_ml["lag_1"] = datos_ml["y"].shift(1)
datos_ml["lag_2"] = datos_ml["y"].shift(2)
datos_ml["lag_12"] = datos_ml["y"].shift(12)

# Dividimos en conjunto de entrenamiento y testeo

corte = len(datos_ml)-12

datos_ml_train = datos_ml[:corte]
datos_ml_test = datos_ml[corte:]

X_train = datos_ml_train[['month', 'year', 'promedio_3_meses', 'desvio_3_meses', 'lag_1', 'lag_2', 'lag_12']]
X_test = datos_ml_test[['month', 'year', 'promedio_3_meses', 'desvio_3_meses', 'lag_1', 'lag_2', 'lag_12']]

# XGBoost
cv_split = TimeSeriesSplit(n_splits=4)
model = XGBRegressor(parameters = {
    "max_depth": 3,
    "learning_rate": 0.1,
    "n_estimators": 100
})
# parameters = {
#     "max_depth": [3, 4, 5, 7, 10],
#     "learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3],
#     "n_estimators": [10, 20, 50, 100, 300, 500, 700],
#     "colsample_bytree": [0.3, 0.5, 0.7]
# }

model.fit(X_train, datos_ml_train['y'])
model.predict(X_test)

grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)
grid_search.fit(X_train, datos_ml_train['y'])

# Prediccion
y_pred_xgb = grid_search.predict(X_test)

# MAPE y Interval Score
mape = mean_absolute_percentage_error(datos_ml_test['y'], y_pred_xgb)
score = interval_score(datos_ml_test['y'], )

metricas.loc[len(metricas)] = ['XGBoost', mape]
```

```{python}
def quantile_loss(q):
    def objective(y_pred, d_train):
        y_true = dtrain.get_label()
        err = y_true - y_pred
        grad = np.where(err < 0, -q, 1 - q)
        hess = np.ones_like(y_true)  
        return grad, hess
    return objective

from xgboost import XGBRegressor, DMatrix, train

q = 0.9

dtrain = DMatrix(X_train, label=datos_train['y'])

params = {
    "objective": "reg:squarederror",
    "max_depth": 3,
    "base_score": np.median(datos_train['y']), # Valor inicial para mejorar la performance
    "learning_rate": 0.1,
    "n_estimators": 100
}

model10 = train(params, dtrain, num_boost_round=100, obj=quantile_loss(0.1))

pred_90 = model90.predict(DMatrix(X_test))
pred_10 = model10.predict(DMatrix(X_test))


# Graficamos el pronostico
plt.plot(datos['ds'], datos['y'])
# sns.lineplot(x = datos['ds'], y= pred, color = 'red', label = 'Prediccion')
plt.fill_between(
    datos_test['ds'], pred_10, pred_90, color = 'red', alpha = 0.3)
```

```{python}
# LightGBM
cv_split = TimeSeriesSplit(n_splits=3)
model = lgb.LGBMRegressor()
parameters = {
    "max_depth": [3, 4, 5, 7, 10],
    "learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3],
    "n_estimators": [10, 20, 50, 100, 300, 500, 700],
    "colsample_bytree": [0.3, 0.5, 0.7]
}


grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)
grid_search.fit(X_train, datos_train['y'])

# Prediccion
y_pred_lgb = grid_search.predict(X_test)

mape = mean_absolute_percentage_error(datos_test['y'], y_pred_lgb)

metricas.loc[len(metricas)] = ['LightGBM', mape]
```

# Deep learning

```{python}
# LSTM

corte = len(datos)-12

datos.columns = ['ds', 'y']

datos_train = datos[:corte]
datos_test = datos[corte:]


datos_train['unique_id'] = 0

# Definición del modelo

nf = NeuralForecast(
    models=[LSTM(h=12, 
                 input_size=3*12,
                 max_steps = 200,
                 loss = MQLoss(level = [90])
                 )
    ],
    freq='M'
)

# Ajuste del modelo
c = time.time()
nf.fit(df=datos_train)
f = time.time()

tiempo = f-c

# Predicción

Y_hat_df = nf.predict()

# Calculo de MAPE e Interval Score

mape = mean_absolute_percentage_error(datos_test['y'], Y_hat_df['LSTM-median'])
score = interval_score(obs = datos_test['y'], lower=Y_hat_df['LSTM-lo-90'], upper=Y_hat_df['LSTM-hi-90'], alpha=0.2)

metricas.loc[len(metricas)] = ['LightGBM', mape, score, tiempo]

```

```{python}
# TIME GPT

parametros = {
    'finetune_loss' : ['mape']}

pred, mape, score, tiempo, resultados = TGPT_tune(datos=datos, parametros= parametros)

metricas.loc[len(metricas)] = ['TimeGPT', mape, score, tiempo]

pred

plt.plot(datos['ds'], datos['y'])
sns.lineplot(x = pred['ds'], y= pred['TimeGPT'], color = 'red', label = 'Prediccion')
plt.fill_between(pred['ds'], pred['TimeGPT-lo-0.975'], pred['TimeGPT-hi-0.975'], color = 'red', alpha = 0.3)

```

# ARREGLAR

- averiguar como funciona el coverage, (es un intervalo probabilistico siquiera?)

- Los intervalos de TimeGPt estan muy chicos

- Implementar grid search para XGB sin usar grid_search