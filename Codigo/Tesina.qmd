---
format: 
  pdf:
    papersize: A4
    crossref:
      fig-prefix: ""
      tbl-prefix: ""
engine: python
metadata:
  quarto-python:
    python: ".venv/Scripts/python.exe"
fig-pos: H
toc: false
lang: es
echo: False
warning: False
message: False
geometry: 
  - top= 25.4mm
  - left= 25.4mm
  - right = 25.4mm
  - bottom = 25.4mm
header-includes: 
  - \usepackage{multirow}
  - \usepackage{float}
---

```{python}
# LIBRERIAS

# Para mostrar los resultados
from Funciones import plot_forecast, load_env, autocorr_plot, resid_check, interval_score

# Para calcular MAPE
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

# Para la transformacion de box y cox
from scipy import stats

# Para graficar
from plotnine import ggplot, aes, geom_line, geom_ribbon, geom_histogram, geom_rect, geom_segment, geom_hline, geom_point, scale_x_continuous, scale_x_date, theme, theme_bw, element_blank, labs, scale_color_manual, scale_fill_manual, scale_y_continuous, element_text, theme_set, annotate, after_stat, geom_boxplot
from plotnine.options import set_option, get_option

theme_set(theme_bw() + theme(figure_size=(6, 2)))
set_option("figure_size", (6, 2))

# Para manejo de datos
import pandas as pd
import numpy as np
```


\newgeometry{margin=0mm}

\pagenumbering{gobble}
 
\thispagestyle{empty}

\begin{figure}[htbp]
    \noindent
    \includegraphics[width=\paperwidth, height=\paperheight]{../Imgs/Portada_tesina.png}
\end{figure}


\newpage

\newpage


\restoregeometry


\section*{Agradecimientos}

\newpage

\section*{Resumen}

La predicción de series temporales desempeña un rol crucial en contextos como la salud, la economía, la energía y la gestión de recursos, donde anticipar el comportamiento futuro de una variable resulta fundamental para la toma de decisiones. Esta tesina compara el desempeño de distintos enfoques para el pronóstico de series temporales, incluyendo modelos estadísticos tradicionales (ARIMA, SARIMA), algoritmos de aprendizaje automático (XGBoost, LightGBM), redes neuronales recurrentes (LSTM) y modelos fundacionales preentrenados basados en transformadores (TimeGPT y Chronos).

La evaluación se llevó a cabo sobre tres series reales representativas, con diferentes estructuras temporales: (i) número mensual de atenciones por patologías respiratorias en un hospital pediátrico, (ii) empleo privado en el sector educativo, y (iii) temperatura horaria durante el mes de marzo. Para cada caso, se implementaron y ajustaron los modelos mencionados, comparando sus resultados mediante métricas puntuales (MAPE) y probabilísticas (*Interval Score*), así como tiempos de cómputo y facilidad de implementación.

Los resultados muestran que no existe un modelo universalmente superior: mientras los modelos estadísticos ofrecen interpretabilidad y precisión en contextos con fuerte estacionalidad, los algoritmos de boosting presentan buena capacidad predictiva con bajo costo computacional. Las redes LSTM, aunque potentes, requieren entrenamiento cuidadoso y muestran sensibilidad al sobreajuste. Por su parte, los modelos fundacionales ofrecen ventajas en automatización y escalabilidad, aunque su comportamiento varía según la naturaleza de la serie y su estructura interna no siempre es accesible.

Este trabajo aporta evidencia empírica y conceptual para una selección informada de modelos de pronóstico en función del contexto, destacando el potencial de las herramientas recientes como TimeGPT y Chronos, así como la importancia de incorporar métricas probabilísticas y técnicas de conformal prediction para mejorar la estimación de la incertidumbre.

Palabras clave: series temporales, predicción, *conformal predictions*, aprendizaje automático, redes neuronales, *transformers*, TimeGPT, *interval score*, Chronos. 

\newpage
\renewcommand{\contentsname}{Índice}
\tableofcontents
\newpage
\pagenumbering{arabic}

# 1. Introducción


La predicción de valores futuros en series de tiempo es una herramienta clave en múltiples ámbitos, tales como la economía, el comercio, la salud, la energía y el medio ambiente. En estos contextos, anticipar el comportamiento de una variable permite mejorar la planificación, asignar recursos de forma más eficiente y reducir la incertidumbre.

Actualmente, la ciencia de datos se encuentra en una etapa de constante expansión e innovación, impulsada por la gran cantidad de datos generados diariamente, por lo que en un contexto creciente de complejidad y exigencia temporal, resulta conveniente contar con herramientas que faciliten y acorten los tiempos de trabajo. Si bien los métodos más conocidos para trabajar series de tiempo son precisos, los modelos tradicionales como ARIMA son difíciles de automatizar y requieren de amplios conocimientos para encontrar un buen ajuste, mientras que los algoritmos de aprendizaje automatizado que se utilizan actualmente pueden demandar largos tiempos de entrenamiento y un gran coste computacional. Frente a estas limitaciones, en los últimos años se han desarrollado modelos capaces de seleccionar de forma automática el mejor ajuste para una serie temporal dada, sin requerir entrenamiento previo ni conocimientos especializados en análisis de series de tiempo. Estos son los denominados modelos fundacionales preentrenados, tales como TimeGPT o Chronos.

Sin embargo, aún persisten interrogantes sobre el desempeño de estos nuevos modelos y la falta de acceso al código fuente de algunos de estos limita la posibilidad de auditar sus resultados o replicar su implementación. Por ello, esta tesina propone realizar una comparación sistemática de modelos de pronóstico para series de tiempo, abordando tres enfoques metodológicos: modelos estadísticos tradicionales, algoritmos de *machine learning* y modelos de aprendizaje profundo. El objetivo es evaluar su desempeño en distintos contextos, utilizando métricas como el porcentaje del error absoluto medio (MAPE) y el *Interval Score*, con el fin de analizar ventajas, limitaciones y potenciales usos de cada uno.

Este análisis busca aportar una mirada crítica e informada sobre el uso de nuevas tecnologías en la predicción de series de tiempo, contribuyendo a la toma de decisiones metodológicas más sólidas desde una perspectiva estadística.

\newpage

# 2. Objetivos

## 2.1 Objetivo general

El objetivo de esta tesina es, en primer lugar, comparar la precisión, eficiencia y facilidad de pronosticar series de tiempo con distintos modelos, incluyendo enfoques estadísticos clásicos, algoritmos de *machine learning* y modelos de *deep learning*, analizando al mismo tiempo sus ventajas, limitaciones y condiciones de uso más apropiadas.

## 2.2 Objetivos específicos

- Implementar modelos clásicos de series de tiempo, como ARIMA y SARIMA, explicando y garantizando el cumplimiento de los fundamentos teóricos y supuestos que los sostienen.

- Aplicar modelos de aprendizaje automático supervisado, como XGBoost y LightGBM, explorando distintas configuraciones para garantizar el mejor ajuste.

- Desarrollar modelos de aprendizaje profundo, en particular redes LSTM, dando introducción a las redes neuronales y modelos de pronóstico más complejos.

- Realizar pronósticos con modelos fundacionales(TimeGPT,Chronos) y comprender su funcionamiento.

- Definir y aplicar métricas de evaluación (MAPE, *Interval Score*) para comparar el rendimiento de todos los modelos bajo un mismo conjunto de datos.

- Reflexionar valorativamente sobre los criterios de selección de modelos en función del contexto de aplicación, la complejidad computacional y la interpretabilidad de los resultados.

\newpage

# 3. Metodología {#metodologia}

El enfoque metodológico adoptado en esta tesina consiste en comparar el desempeño de distintos modelos de pronóstico aplicados a series temporales. Para ello, se seleccionan modelos representativos de tres enfoques principales: modelos estadísticos tradicionales, algoritmos de aprendizaje automático (*machine learning*) y modelos de aprendizaje profundo (*deep learning*).

El análisis se estructura en tres componentes fundamentales: una descripción conceptual de los modelos, su implementación práctica sobre series con diferentes características, y una evaluación cuantitativa comparativa a través de métricas de error.

## 3.1 Conceptos básicos de series de tiempo

Se denomina serie de tiempo a un conjunto de observaciones $\{z_1, z_2, ..., z_t, ..., z_n\}$ cuantitativas ordenadas en el tiempo, usualmente de forma equidistante, sobre una variable de interés. El análisis de series de tiempo tiene como objetivo sintetizar y extraer información estadística relevante, tanto para interpretar el comportamiento histórico de la variable como para generar pronósticos $\{ z_{n+1}, ...,z_{n+l}, ...., z_{n+h} \}$.

Dado que las series temporales pueden exhibir diversos patrones subyacentes, resulta útil descomponerlas en componentes separadas, cada una de las cuales representa una característica estructural específica del comportamiento de la serie.

- Estacionalidad: corresponde a las fluctuaciones periódicas que se repiten a intervalos regulares de tiempo. Un ejemplo típico es la temperatura, que tiende a disminuir en invierno y aumentar en verano, repitiendo este patrón anualmente.

- Tendencia (o tendencia-ciclo): refleja la evolución a largo plazo de la media de la serie, asociada a procesos de crecimiento o decrecimiento sostenido. Por ejemplo, la población mundial exhibe una tendencia creciente a lo largo del tiempo.

- Residuos: representa las variaciones no sistemáticas que no pueden ser explicadas por la tendencia ni la estacionalidad. Estas fluctuaciones, que suelen deberse a eventos impredecibles o factores exógenos, se asumen como aleatorias.

## 3.2 Modelos estadísticos tradicionales para series temporales

Son llamados modelos estadísticos tradicionales a aquellos que surgen antes del auge del *machine learning* y los modelos de aprendizaje profundo. Son caracterizados por sus fuertes fundamentos estadísticos y su capacidad en capturar dependencias temporales en los datos.

### 3.2.1 SARIMA

Se dice que una serie es débilmente estacionaria si la media y la variancia se mantienen constantes en el tiempo y la correlación entre distintas observaciones solo depende de la distancia en el tiempo entre estas. Por comodidad, cuando se mencione estacionariedad se estará haciendo referencia al cumplimiento de estas propiedades. 

Los modelos $ARIMA$ (*AutoRegresive Integrated Moving Average*) son unos de los modelos de pronostico tradicionales mejor establecidos. Son una generalización de los modelos autoregresivos (AR), que suponen que las observaciones futuras son función de las observaciones pasadas, y los modelos promedio móvil (MA), que pronostican las observaciones como funciones de los errores de observaciones pasadas. Además, estos modelos pueden adaptarse a series no estacionarias mediante la aplicación de diferenciaciones de orden $d$, las cuales implican restar a cada observación el valor registrado $d$ periodos anteriores.

Formalmente un modelo $ARIMA(p,d,q)$ se define como:

$$
\psi_p(B)(1-B)^dz_t = \theta_0 + \theta_q(B)\alpha_t
$${#eq-1}

Donde $z_t$ es la observación $t$-ésima, $\psi_p(B)$ y $\theta_q(B)$ son funciones de los rezagos ($B$), correspondientes a la parte autoregresiva y promedio móvil respectivamente, $d$ es el grado de diferenciación y $\alpha_t$ es el error de la $t$-ésima observación.

Se debe tener en cuenta estos aspectos importantes:

- Se dice que una serie es invertible si se puede escribir cada observación como una función de las observaciones pasadas más un error aleatorio. Por definición, todo modelo AR es invertible.

- Por definición, todo modelo MA es estacionario.

- $\psi_p(B) = 1 - \psi_1 B - \psi_2 B_2 - ... - \psi_p B^p$ es el polinomio característico de la componente AR y $\theta_q(B) = 1 - \theta_1 B - \theta_2 B^2 - ... - \theta_p B^q$ de la componente MA. Si las raíces de los polinomios característicos caen fuera del círculo unitario, entonces un proceso AR se puede escribir de forma MA y es estacionario, y a su vez un proceso MA se puede escribir de forma AR y es invertible.

- Un proceso $ARIMA$ es estacionario e invertible si su componente AR y MA lo son respectivamente.

Sin embargo este tipo de modelos no tienen en cuenta la posible estacionalidad que puede tener una serie, es por esto que se introducen los modelos $SARIMA(p,d,q)(P,D,Q)_s$ que agregan componentes AR, MA y diferenciaciones a la parte estacional de la serie con período $s$.

Se denomina función de autocorrelación a la función de los rezagos, entendiendo por rezago a la distancia ordinal entre dos observaciones, que grafica la autocorrelación entre pares de observaciones. Es decir que para cada valor $k$ se tiene la correlación entre todos los pares de observaciones a $k$ observaciones de distancia. En su lugar, la función de autocorrelación parcial calcula la correlación condicional de los pares de observaciones, removiendo la dependencia lineal de estas observaciones con las que se encuentran entre estas. Estas funciones son necesarias para poder identificar los modelos $SARIMA$ y se definen como:

$$
\rho_k = Corr(z_t, z_{t+k}) = \frac{Cov(z_t, z_{t+k})}{\sqrt{Var(z_t) \cdot Var(z_{t+k})}}
$${#eq-auto}

y
$$
\phi_{kk} = Corr(z_t, z_{t+k}|z_{t+1}, ..., z_{t+k-1})
$${#eq-autopar}

Los modelos $AR$ se caracterizan por tener autocorrelaciones significativas que decaen lentamente y autocorrelaciones parciales significativas únicas. Los modelos $MA$ se comportan de forma inversa, tienen autocorrelaciones significativas únicas y autocorrelaciones parciales que decaen progresivamente. Estos comportamientos también se evidencian en las componentes estacionales, presentandose en los rezagos estacionales.

```{python}

# Ejemplos de procedimientos AR Y MA

def plot_ejemplo(autocorrelaciones, atype = 'acf', lags = 20):
    if atype == 'acf':
        lags = np.arange(lags+1)
        ylab = r'$\rho_k$'
    else:
        lags = np.arange(lags+1)
        lags = lags[1:]
        ylab = r'$\phi_{kk}$'

    # Defino las bandas limite
    upper_bound = 0.25
    lower_bound = -0.25

    # Cambio el color del punto si supera el limite
    col = np.where(
        autocorrelaciones['y'] < lower_bound,'b',
        np.where(autocorrelaciones['y'] > upper_bound, 'b', 'green'))

    return(ggplot(autocorrelaciones) +
    aes(x = 'lag', y = 'y') +
    geom_rect(ymin = lower_bound, ymax = upper_bound, xmax = np.inf, xmin = -np.inf, alpha = 0.01 ,fill = "#8ED081") +
    geom_rect(aes(ymin = -1, ymax = -np.inf, xmax = np.inf, xmin = -np.inf), alpha = 0.01 ,fill = "grey") +
    geom_rect(aes(ymin = 1, ymax = np.inf, xmax = np.inf, xmin = -np.inf), alpha = 0.01 ,fill = "grey") +
    geom_segment(aes(x = 'lag', xend = autocorrelaciones['lag'], yend = 'y', y = 0)) +
    geom_hline(yintercept = 0) +
    geom_hline(color = "green",yintercept = upper_bound, linetype = "dashed") +
    geom_hline(color = "green",yintercept = lower_bound, linetype = "dashed") +
    geom_point(color = "#8ED081", size = 0.7) +
    geom_point(aes(x = 'lag', y = 'y'), color = col, size = 0.7) +
    scale_y_continuous(limits = (-1,1)) +
    scale_x_continuous(breaks = list(range(0,max(autocorrelaciones['lag']), 2)), limits = (0,max(autocorrelaciones['lag']))) +
    labs(x = "Rezago (k)", y = ylab)
    )


ar_acf = pd.DataFrame({"y":[1, 0.74, 0.62, 0.56, 0.5, 0.46, 0.41, 0.37, 0.33, 0.3, 0.26, 0.23, 0.2, 0.17, 0.14, 0.11, 0.08, 0.06, 0.03, 0.01],
"lag" : np.arange(0,20)
})

ma_pacf = pd.DataFrame({"y":ar_acf['y']*[1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1],
"lag" : np.arange(0,20)
})

pico = pd.DataFrame({"y":[1, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
"lag" : np.arange(0,20)
})

#ar
plot_ejemplo(ar_acf, atype='acf').save("../Imgs/plotnine/ejemplo_ar_1.png", width=6, height=4/2.1, dpi=700)
plot_ejemplo(pico.tail(19), atype='pacf').save("../Imgs/plotnine/ejemplo_ar_2.png", width=6, height=4/2.1, dpi=700)

#ma
plot_ejemplo(pico, atype='acf').save("../Imgs/plotnine/ejemplo_ma_1.png", width=6, height=4/2.1, dpi=700)
plot_ejemplo(ma_pacf.tail(19), atype='pacf').save("../Imgs/plotnine/ejemplo_ma_2.png", width=6, height=4/2.1, dpi=700)

```

::: {#fig-ejemar layout-nrow=2}

![](../Imgs/plotnine/ejemplo_ar_1.png)

![](../Imgs/plotnine/ejemplo_ar_2.png)

Ejemplo de las autocorrelaciones de un proceso $AR(1)$.
:::

::: {#fig-ejemma layout-nrow=2}

![](../Imgs/plotnine/ejemplo_ma_1.png)

![](../Imgs/plotnine/ejemplo_ma_2.png)

Ejemplo de las autocorrelaciones de un proceso $MA(1)$.
:::

Un buen modelo $SARIMA$ debe cumplir las siguientes propiedades:

- Sus residuos se comportan como ruido blanco, es decir, están incorrelacionados y siguen una distribución normal, con media y variancia constantes.

- Es admisible, es decir, es invertible y estacionario.

- Es parsimonioso, en el sentido de que sus parámetros son significativos.

- Es estable en los parámetros, que se cumple cuando las correlaciones entre los parámetros no son altas.

## 3.3 Modelos de aprendizaje automático

El aprendizaje automático (machine learning) es una rama de la inteligencia artificial que permite a las computadoras aprender de los datos y realizar tareas de forma autónoma. Aunque los métodos presentados no fueron diseñados específicamente para datos temporales, han demostrado ser útiles en múltiples contextos mediante diversas pruebas empíricas.

Los métodos de *machine learning*, a diferencia de los modelos tradicionales, se enfocan principalmente en identificar los patrones que describen el comportamiento del proceso que sean relevantes para pronosticar la variable de interés, y no se componen de reglas ni supuestos que tengan que seguir. Para la identificación de patrones, estos modelos requieren la generación de características. 

### 3.3.1 Introducción a árboles de decisión y ensamblado

Los árboles de decisión pueden ser explicados sencillamente como un conjunto extenso de estructuras condicionales *if-else*. El modelo pronosticará un cierto valor $x$ si una cierta condición es verdadera, u otro valor $y$ si es falsa. Es importante ver que no hay una tendencia lineal en este tipo de lógica, por lo que los árboles de decisión pueden ajustar tendencias no lineales. El resultado que se obtiene al aplicar esta técnica puede resumirse gráficamente como un camino que toma diferentes bifurcaciones (o un tronco con diferentes ramas), y de esta característica surge su nombre.

Un árbol puede tener distinta cantidad de divisiones en un mismo nivel, llamadas hojas, y profundidad, las cuales determinan en qué medida el modelo se ajusta a los datos con los que se entrena. Lógicamente árboles más profundos y con más hojas suelen generar sobreajuste, es decir, un modelo que se adapta demasiado a los datos de entrenamiento y generaliza mal.

![Ejemplo de árbol de decisión](../Imgs/arbol_decision.png)


Los métodos de ensamblaje combinan la predicción de varios estimadores base con el objetivo de mejorar la robustez de la predicción. Existen numerosos métodos de ensamblaje entre los cúales se encuentran los bosques de decisión y los árboles potenciados por gradiente (del inglés *Gradient-boosted trees*). 

*Boosting* es un proceso iterativo, que consiste en la construcción de árboles de forma secuencial donde cada nuevo árbol busca predecir los residuos de los árboles anteriores. Es así entonces que el primer árbol buscará predecir los valores futuros de la serie, mientras que el segundo intentará predecir los valores reales menos los pronosticados por el primer árbol, el tercero tratará de inferir la diferencia entre los valores reales y el valor pronosticado del primer árbol menos los errores del segundo, y así sucesivamente. En cada iteración se pesan los puntos y se corrigen aquellos que tengan un mayor error por medio del descenso del gradiente. 

![Proceso de ensamblado](../Imgs/Ensamblaje.png)

La dirección de máximo crecimiento para una función está determinada por su gradiente, y del mismo modo, la dirección contraria a este gradiente es la dirección de máximo decrecimiento. De este modo, el descenso del gradiente busca encontrar los valores más bajos para una función de pérdida. El algoritmo de descenso de gradiente propone calcular el gradiente de la función de costo bajo el valor actual de parámetros, para luego modificarlo moviéndose en la dirección de mayor descenso. Este resultado se basa en derivadas, tasas de cambio instantáneo, por lo que conviene desplazarse una magnitud pequeña $\eta$, llamada “tasa de aprendizaje”. Es un algoritmo iterativo, en el que en cada paso la regla de actualización consiste en calcular la derivada de la función de costo respecto a cada parámetro y desplazarse una cierta magnitud $\eta$ en la dirección contraria. Esto se repite un cierto número de veces hasta alcanzar la convergencia.

![Ejemplo del descenso del gradiente en una función de pérdida](../Imgs/desc_gradiente.png)

Sin embargo, los modelos no se construyen infinitamente, sino que se busca minimizar una función de pérdida que incluye una penalización por la complejidad del modelo, limitando así la cantidad de árboles que se producen. Existen múltiples métodos de ensamblaje (XGBoost, LightGBM, CatBoost, AdaBoost, entre otros) que se diferencian en la forma en la que se construyen los árboles. En esta tesina se usarán los algoritmos *eXtreme Gradient Boosting* (XGBoost) y *Light Gradient-Boosting Machine* (LightGBM).

### 3.3.2 Diferencias entre XGBoost y LightGBM

Las diferencias entre XGBoost y LightGBM radican en la forma en que cada uno identifica las mejores divisiones dentro de los árboles y de que forma los hacen crecer.

Mientras que XGBoost usa un método en el que se construyen histogramas para cada una de las características generadas para elegir la mejor división por característica, LightGBM usa un método más eficiente llamado *Gradient-Based One-Side Sample* (GOSS). GOSS calcula los gradientes para cada punto y lo usa para filtrar afuera aquellos puntos que tengan un bajo gradiente, ya que esto significaría que estos están mejor pronosticados que el resto y no es necesario enfocarse tanto en ellos. Además, LightGBM utiliza un procedimiento que acelera el ajuste cuando se tienen muchas características correlacionadas de las cuales elegir. 

A la hora de hacer crecer los árboles, XGBoost lo hace nivel a nivel, es decir que primero se crean todas las divisiones de un nivel, y luego se pasa al siguiente, priorizando que el árbol sea simétrico y tenga la misma profundidad en todas sus ramas. LigthGBM, en cambio, se expande a partir de la hoja que más reduce el error, mejorando la precisión y eficiencia en series largas, pero arriesgándose a posibles sobreajustes si no se limita correctamente la profundidad de los árboles.

### 3.3.3 Intervalos de confianza en algoritmos de aprendizaje automático

Una de las principales ventajas de los modelos de aprendizaje automático respecto de los enfoques estadísticos tradicionales es que no requieren supuestos distribucionales estrictos sobre los errores. Sin embargo, esta flexibilidad también implica que, en general, no generan pronósticos probabilísticos de forma directa, lo que dificulta la construcción de intervalos de confianza.

Para subsanar esta limitación, se han desarrollado diversos métodos que permiten acompañar las predicciones puntuales con medidas de incertidumbre. A continuación, se describen dos de los enfoques más utilizados.

- Regresión cuantil con *boosting*: consiste en entrenar modelos para estimar directamente cuantiles de la distribución condicional de la variable objetivo, en lugar de su valor esperado. De este modo, pueden construirse intervalos de predicción utilizando, por ejemplo, los percentiles 5 y 95. Esta técnica está implementada en algunas variantes como LightGBM, pero no es directamente aplicable en XGBoost, lo que restringe su uso en ciertos entornos.

- Conformal prediction: se trata de una familia de métodos no paramétricos que permiten construir intervalos de predicción válidos bajo el supuesto de intercambiabilidad de las observaciones. Dado que este supuesto no se cumple en series temporales, donde existe dependencia temporal, se han desarrollado adaptaciones específicas para este tipo de datos.

Una de las más destacadas es el método *Ensemble Batch Prediction Intervals* (EnbPI), que permite aplicar conformal prediction en series temporales sin asumir independencia entre observaciones. Su procedimiento consiste en:

1. Seleccionar un modelo por ensamblado (como XGBoost o LightGBM).

2. Generar B muestras *bootstrap* por bloques, manteniendo así la estructura temporal de los datos.

3. Ajustar un modelo sobre cada una de las B muestras.

4. Para cada observación del conjunto de entrenamiento, calcular el residuo utilizando únicamente aquellos modelos que no la incluyeron.

5. Obtener las predicciones puntuales promediando los resultados de los B modelos.

6. Construir los intervalos de predicción sumando y restando los cuantiles empíricos de los residuos a las predicciones.

$$
IC_{Z_{n+l}; 1-\alpha}= Z_{n}(l) \pm Q_{1-\alpha}(e)
$${#eq-2}

Este método será el utilizado en esta tesina para estimar los intervalos de confianza en los algoritmos de aprendizaje automático y se aplica con la librería `MAPIE`. 

## 3.4 Modelos de aprendizaje profundo

El *deep learning* (aprendizaje profundo) es una rama del *machine learning* que tiene como base un conjunto de algoritmos que intentan modelar niveles altos de abstracción en los datos usando múltiples capas de procesamiento, con complejas estructuras o compuestas de varias transformaciones no lineales. 

Entre estos algoritmos se encuentran las redes neuronales, que imitan el funcionamiento del cerebro humano usando procesos que simulan la forma biológica en la que trabajan las neuronas para identificar fenómenos, evaluar opciones y llegar a conclusiones. 

### 3.4.1 Introducción a redes neuronales

Una red neuronal esta compuesta en grandes rasgos de 3 capas: entrada, oculta y salida. Dentro de cada capa se pueden encontrar neuronas y conexiones entre estas, donde cada neurona representa una variable y cada conexión un peso, y es por esto que a estas conexiones las llamaremos así en adelante. La suma de los pesos y las neuronas que no formen parte de la capa de entrada dan el total de parámetros que tiene que ajustar el modelo.

![Red neuronal completamente conectada](../Imgs/red_conectada.png){#fig-redneuro}

En la capa de entrada se introducen las variables explicativas, y luego cada neurona fuera de esta capa es una función de las neuronas anteriores conectadas a la misma. Estas funciones son llamadas funciones de activación.

![Red neuronal completamente conectada](../Imgs/conexiones_redneuro.png){#fig-conexiones}

Los parámetros se estiman buscando minimizar una función de costo, esto se logra con el descenso del gradiente y por medio de retropropagación. La retropropagación consiste en realizar una estimación inicial de la variable respuesta con los valores iniciales de la red neuronal, que pueden estar dados por ejemplo por una distribución normal, y de manera inversa a la dirección de la red neuronal calcular derivadas para encontrar la dirección de máximo decrecimiento de la función de costo para cada parámetro en la red neuronal. 

Existen distintos tipos de redes neuronales según la forma en la que se conectan las neuronas. En esta tesina son de interés especialmente las *Convolutional Neural Networks* (CNN) y las *Recurrent Neural Networks* (RNN), en español, redes neuronales convolucionales y recurrentes respectivamente. Las primeras son útiles para el reconocimiento de patrones en los datos, mientras que las últimas son especialmente buenas en la predicción de datos secuenciales.


### 3.4.2 *Long Short Term Memory* (LSTM)

Lo que caracterizan a las redes neuronales recurrentes son los bucles de retroalimentación que se presentan en la figura @fig-rnn. Mientras que cada neurona de entrada en una red neuronal completamente conectada es independiente, en las redes neuronales recurrentes se relacionan entre ellas y se retroalimentan.

![Ejemplo de RNN](../Imgs/red_recurrente.png){#fig-rnn}


Un problema frecuente en las RNN es su dificultad para capturar dependencias de largo plazo. Esto puede tener 2 causas, el desvanecimiento o la explosión del gradiente. El desvanecimiento del gradiente ocurre cuando, iteración tras iteración, el gradiente se aproxima a cero y se estabiliza, evitando que la red siga aprendiendo. Por el contrario, cuando el gradiente crece exponencialmente se habla de una explosión, esto lleva a inestabilidades en el aprendizaje, haciendo que las actualizaciones de los parámetros sean erráticas e impredecibles.

Las redes neuronales con memoria a corto y largo plazo (LSTM) son un tipo de RNN que solucionan este problema mediante un algoritmo logístico de 3 puertas, y usando una neurona que guarda la información histórica necesaria para la red.

![Estructura *Long Short Term Memory*](../Imgs/lstm.png)

**Puerta de guardado**

La puerta de guardado se encarga de decidir si mantener o descartar la información actualmente guardada en la neurona de memoria de la red neuronal. Esta puerta recibe la entrada y el estado de la RNN, y las pasa como argumentos de una función sigmoide. 

$$
\sigma(x) =  \frac{1}{1+e^{-x}}
$${#eq-3}

$$
K_t = \sigma(W_k \times [S_{t-1}, x_t] + B_k)
$${#eq-4}

Si $K_t$ es igual a 1, significa que la información guardada debe ser mantenida perfectamente. Si $K_t$ fuera igual a 0, la información guardada debe ser descartada completamente.

Sean $S_{t-1}$ el estado actual de la RNN, $x_t$ la entrada actual, y $W_t$ y $B_t$ los pesos y sesgos de la puerta de guardado respectivamente:

$$
Old_t = K_t \times C_{t-1}
$${#eq-5}

Donde $C_{t-1}$ es la información guardada actualmente y $Old_t$ lo que se mantendrá para la próxima iteración de la red.

**Puerta de entrada**

La puerta de entrada controla que información añadir a la neurona de memoria. Sean $W_i$ y $B_i$ los pesos y sesgos de la puerta de guardado:

$$
I_t = \sigma(W_i\times[S_{t-1},x_t]+B_i)
$${#eq-6}

$$
New_t = I_t \times N_t
$${#eq-7}

Donde $N_t$ es el nuevo valor propuesto por la red neuronal y $New_t$ es la información que se va a agregar a la neurona de memoria. Luego, el nuevo valor de la neurona de memoria es:

$C_t = Old_t + New_t$

**Puerta de salida**

La puerta de salida se encarga de extraer la información más importante del estado actual de la neurona para usar como salida. Sean $W_o$ y $B_o$ los pesos y sesgos de la puerta de salida y $tanh(x)$ la función tangente:

$$
O_t = \sigma(W_o \times [S_{t-1},x_t] + B_o)
$${#eq-8}

$$
S_t = O_t \times tanh(C_t)
$${#eq-9}

Donde $S_t$ es el nuevo estado de la red neuronal.

Las 3 puertas son logísticas para que sea sencillo aplicar la retropropagación. Este sistema de puertas evita los problemas de desvanecimiento y explosión del gradiente, y evita que se acumulen muchos estados por largos períodos de tiempo eligiendo que información es relevante guardar.

### 3.4.3 Modelos transformadores

Otro tipo de modelo de aprendizaje profundo son los *transformer models* (modelos transformadores), los cuales son significativamente más eficientes al entrenar y realizar inferencias que las RNNs; gracias al uso de mecanismos de atención, presentados en la publicación '[*Attention is all you need*](https://arxiv.org/pdf/1706.03762)' de Google. La autoatención captura dependencias y relaciones en la secuencias de valores que se alimentan al modelo, logrando poner en contexto a cada observación. 

Los modelos transformadores fueron creados originalmente con el propósito de generar texto. Sin embargo, tanto TimeGPT como Chronos explotan esta tecnología para el pronóstico de series de tiempo. Ambos modelos son preentrenados, lo cual significa que la optimización de parámetros y pesos fue realizada antes de usarse el modelo. Esto se logra entrenando y generalizando el modelo en un conjunto de datos extenso, por lo general de fuentes públicas. El preentrenamiento permite que el modelo adquiera conocimientos generales sobre la estructura y los patrones de los datos, los cuales luego pueden ser reutilizados en tareas concretas mediante técnicas como *fine-tuning* (ajuste fino). Los modelos preentrenados constituyen una gran innovación, lo que mejora la accesibilidad, precisión, eficiencia computacional y velocidad del pronóstico.

Dado que los modelos de lenguaje de texto utilizan diccionarios de *tokens*, que son segmentos de caracteres representados vectorialmente según ciertos parámetros, es necesario tokenizar los valores de la serie temporal. El diccionario de *tokens* con el que operan los modelos de lenguaje no es infinito, por lo tanto es necesario proyectar las observaciones a un set finito de *tokens*. Para cumplir esto, Chronos escala y discretiza las observaciones. TimeGTP por su parte usa las mismas observaciones como *tokens*, esto dado que, si bien su arquitectura es la de un modelo transformador, esta no está basada en ningún modelo de lenguaje existente, y en cambio trabaja con un modelo especializado en series de tiempo entrenado para minimizar el error de pronóstico.

Para el escalado se aplica a las observaciones una transformación del tipo $f(x_i) = (x_i-m)/s$. Existen variadas técnicas de escalado eligiendo apropiadamente $m$ y $s$, pero se opta por elegir $m=0$ y $s = \frac{1}{C}\sum^C_{i=i}|x_i|$ debido a que preserva los valores iguales a cero, los cúales pueden ser importante de destacar en numerosas aplicaciones.

Sin embargo estos valores siguen siendo números reales y no pueden ser procesados directamente por un modelo de lenguaje. Es por esto que se discretizan las observaciones. Se seleccionan $B$ centros de intervalos en la recta real, $c_1, c_2, ..., c_B$, y $B-1$ extremos $b_i$ que los separen, $c_i < b_i < c_{i+1}$ para $i \in \{1,...,B-1 \}$. Las funciones de discretización $q:\Re \rightarrow \{1,2,...,B\}$, y de descuantificación $d: \{1,2,...,B\} \rightarrow \Re$ se definen como:

$$
q(x)= \left \{ \begin{matrix} 1 \ \ \ \ \ \ \ \ \text{si} \ \ -\infty \leq x < b_1 \hfill \\ 
2 \ \ \ \ \ \ \ \ \text{si} \ \ b_1 \leq x < b_2 \hfill \\
 \ \vdots \hfill \\
B \ \ \ \ \ \ \ \ \text{si} \ \ b_{B-1} \leq x < \infty \hfill \end{matrix} \right. \hspace{2, cm} \text{y} \hspace{2, cm} d(j) = c_j
$${#eq-10}

Una vez se hayan transformado las observaciones para poder ser leídas por el modelo, el funcionamiento de un transformador es el siguiente: 

![Diagrama de la estructura del modelo transformador de TimeGPT](../Imgs/modelo_timegpt.png){#fig-modelo}

*Codificador*

1. Representación vectorial: Cada *token* es transformado en un vector (vector de entrada) con muchas dimensiones ($\vec E$). Las dimensiones corresponden a diferentes características que el modelo definió en el preentrenado con una numerosa cantidad de parámetros.

2. Codificación posicional: Un set de valores adicionales o vectores son añadidos a los vectores de entrada antes de alimentarlos al modelo ($\vec E \Leftarrow \vec E + \vec P$). Estas codificaciones posicionales tienen patrones específicos que agregan la información posicional del token.

3. Atención multi-cabezal (del inglés *Multi-Head Attention*): La autoatención opera en múltiples 'cabezales de atención' para capturar los diferentes tipos de relaciones entre tokens. Una cabeza de atención verifica el contexto en el que se presenta el token, y manipula los valores del vector que lo representa para añadir esta información contextual.

La verificación del contexto funciona gracias a una matriz denominada *Query* ($W_Q$) que examina ciertas características definidas con aterioridad en el preentrenado. El vector de entrada ($\vec E$) es multiplicado por esta matriz, resultando en un vector de consultas ($\vec Q$) para cada token. Una matriz de claves ($W_K$) que comprueba las relaciones con las características en la matriz de consultas, y tiene las mismas dimensiones que esta, es también multiplicada por $\vec E$ generando así el vector de claves ($\vec K$). Luego, se forma una nueva matriz a partir de los productos cruzados entre los vectores $\vec K$ y $\vec Q$ de cada token, se divide por la raíz de la dimensión de los vectores^[Es útil para mantener estabilidad numérica, la cual describe cómo los errores en los datos de entrada se propagan a través del algoritmo. En un método estable, los errores debidos a las aproximaciones se atenúan a medida que la computación procede.] ($\sqrt{d_k}$) y se normaliza con softmax^[ $softmax(x) = \frac{e^{x_i/t}}{\sum_j e^{x_j/t}}$] por columna^[Aplicar softmax hace que cada columna se comporte como una distribución de probabilidad.] ($\vec S$), valores más altos indican que un token (de las columnas) esta siendo influenciado por el comportamiento de otro token (de las filas).

![Verificación del contexto](../Imgs/attention_matrix.png)

Ahora que se sabe que tokens son relevantes para otros tokens, es necesario saber como son afectados. Para esto existe otra matriz de valores ($W_V$) que es multiplicada por cada vector de entrada resultando en los vectores de valor ($\vec V$) que son multiplicados a cada columna. La suma por filas devuelven el vector ($\Delta \vec E$) que debe ser sumado al vector de entrada original de cada token.

![Influencia de tokens sobre otros luego de aplicar softmax](../Imgs/attention_matrix_softmax.png)

$$
\Delta \vec E_j = \sum_i S_j \cdot \vec V_i
$${#eq-11}

Con múltiples 'cabezas', cada una con sus propias matrices $W_K$, $W_Q$ y $W_V$, se generan varios $\Delta \vec E$ que se suman y se añaden al vector de entrada original. 

$$
\vec E \Leftarrow \vec E+ \sum_h \Delta \vec E_h
$${#eq-12}

Todo el mecanismo de atención se puede resumir con la siguiente función:

$$
\text{Atencion}(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$${#eq-13}

Es importante notar que todas las matrices $Q$, $K$ y $V$ son preentrenadas.


4. Sumar y normalizar (*Add and norm*): En lugar de simplemente pasar los datos por las capas, las conexiones residuales se añaden sobre el vector de entrada en la salida de cada capa. Esto es lo que se hizo cuando se sumaron los cambios al vector de entrada, en lugar de directamente modificar el vector.

Dado que las redes neuronales profundas sufren de inestabilidad en los pesos al actualizarlos, una normalización al vector estabiliza el entrenamiento y mejora la convergencia. 

5. Red neuronal convolucional (CNN): A diferencia de otros modelos transformadores tradicionales, TimeGPT incorpora *CNN*s para descubrir dependencias locales y patrones de corto plazo, tratando de minimizar una función de costo.

*Decodificador*

1. *Output embedding* (desplazado hacia la derecha): La entrada del decodificador son los tokens desplazados hacia la derecha.

2. Codificación posicional

3. *Masked multi-head attention* (Atención multicabezal enmascarada): Ahora que las predicciones deben hacerce únicamente con los valores previos a cada token, en el proceso de 'atención' y antes de aplicar la transformación softmax se debe reemplazar todos los valores debajo de la diagonal principal de la matriz $QK$ por $-\infty$ para prevenir que los tokens sean influenciados por tokens anteriores.

4. *Multi-head attention*: En este caso, se usan las matrices de claves y valores que da como salida el codificador y la matriz de consultas es la salida de la capa de atención multicabezal enmascarada.

5. Conexión lineal: Es una capa completamente conectada que traduce las representaciones de atributos aprendidas en predicciones relevantes.


### 3.4.4 Diferencias entre TimeGPT y Chronos

TimeGPT es un modelo transformer preentrenado (de aquí las siglas GPT, *generative pre-trained transformer*) para el pronóstico de series de tiempo que puede producir predicciones en diversas áreas y aplicaciones con gran precisión y sin entrenamiento adicional. El mismo fue desarrollado por Nixtla y tuvo su primera beta privada en Agosto de 2023, volviendose accesible a todo público desde el 18 de julio de 2024.

Chronos es una familia de modelos transformadores preentrenados para series de tiempo basados en arquitecturas de modelos de lenguaje, el cúal fue desarrollado por Amazon y lanzado en marzo de 2024.

La primera diferencia entre ambos modelos es la accesibilidad al código fuente, mientras que Chronos es de código abierto, el modelo desarrollado por Nixtla no lo es.

Tal vez la diferencia más importante es como operan los modelos. Se mencionó anteriormente que TimeGPT utiliza la arquitectura transformer para diseñar un modelo que pueda trabajar directamente con series de tiempo. Chronos en cambio, aprovecha los modelos de lenguaje existentes para aplicarlos a datos temporales. Esto conlleva a otras diferencias clave, como que Chronos debe transformar los datos antes de poder procesarlos, y por trabajar con datos discretos, está entrenado para minimizar la entropía cruzada entre las distribuciones de las categorías reales contra las predichas.

La función de pérdida utilizada por Chronos está dada por:
$$
\ell(\theta) = \sum^{h+1}_{l=1} \sum^{|\nu_{ts}|}_{i=1} \mathbfcal{1}_{z_{n+l+1}=i} log \ \mathcal{p}_\theta(z_{n+l+1}=i| z_{1:n+l})
$${#eq-14}

Donde $|\nu_{ts}|$ es el tamaño del diccionario de tokens, el cúal depende del número de intervalos creados. $z_{n+h+1}$ es la serie transformada en *tokens* de la cúal las primeras $n$ observaciones se usarán como entrenamiento para pronosticar las siguientes $h$, y se agrega al final un token $\texttt{EOS}$ que se utiliza comunmente en los modelos de lenguaje para denotar el final de la secuencia. $\mathcal{p}_\theta$ es la probabilidad estimada por el modelo bajo la parametrización $\theta$.

Es importante notar que no es una función que detecta distancias, por lo que se espera que el modelo asocie a los intervalos cercanos gracias a la información en el conjunto de entrenamiento. Es decir que Chronos aplica regresión por clasificación.

Otra diferencia entre TimeGPT y Chronos es el tipo de red neuronal que utilizan para detectar patrones en los datos, mientras que el primero hace uso de las CNN, el segundo aplica *Feed-Forward Networks*. Luego de la conexión lineal, Chronos necesita volver a aplicar softmax para obtener las probabilidades del pronóstico, procedimiento que no es necesario por parte de TimeGPT.   

## 3.5 Métricas de evaluación


Para comparar el rendimiento de los modelos se utilizan métricas cuantitativas. Para los pronósticos puntuales se usará el porcentaje del error absoluto medio (MAPE), mientras que para los pronósticos probabilísticos se aplicará el *Interval Score*, propuesto por Gneiting y Raftery (2007), que penaliza tanto la amplitud de los intervalos como la falta de cobertura. Estas comparaciones permiten evaluar la precisión, la robustez y la eficiencia de cada enfoque.

Sea $e_l = Z_{n+l} - \hat Z_n(l)$ el error de la l-ésima predicción, donde $\hat Z_n(l)$ representa el pronóstico $l$ pasos hacia adelante, algunas medidas del error para pronósticos $h$ pasos hacia adelante son:

- Error Cuadrático Medio (*Mean Square Error, MSE*):

$$
MSE = \frac{1}{h}\sum^h_{l=1}e_l^2
$${#eq-15}

- Error absoluto medio (*Mean Absolute Error, MAE*):

$$
MAE = \frac{1}{h}\sum^h_{l=1}|e_l|
$${#eq-16}

- Porcentaje del error absoluto medio (*Mean Absolute Percentage Error, MAPE*)

$$
MAPE = (\frac{1}{h}\sum^h_{l=1}|\frac{e_l}{Z_{n+l}}|)\cdot 100 \%
$${#eq-17}

El problema de estos errores es que solo tienen en cuenta la estimación puntual, y por lo general es buena idea trabajar con pronósticos probabilísticos para cuantificar la incertidumbre de los valores futuros de la variable. Gneiting y Raftery (2007, JASA) propusieron en [*Strictly Proper Scoring Rules, Prediction, and Estimation*](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf) una nueva medida del error que tiene en cuenta los intervalos probabilísticos de la estimación, llamándola *Interval Score*:

$$
S = \frac{1}{h}\sum_{l=1}^h (W_l + O_l + U_l) 
$${#eq-18}

Donde:

$$
W_l = IS_l - II_l \hspace{1, cm}
$${#eq-19}

$$
O_l = \left \{ \begin{matrix} \frac{2}{\alpha}(Z_n(l) - Z_{n+l}) \hspace{10,mm} \text{si } Z_n(l) > Z_{n+l} \\ 0 \hspace{1,cm} \text{en otro caso} \hfill \end{matrix} \right. \hspace{1, cm} U_l = \left \{ \begin{matrix} \frac{2}{\alpha}(Z_{n+l} - Z_n(l)) \hspace{10,mm} \text{si } Z_n(l) < Z_{n+l} \\ 0 \hspace{1,cm} \text{en otro caso} \hfill \end{matrix} \right.
$${#eq-20}

Siendo $IS_l$ e $II_l$ los extremos superior e inferior del intervalo del l-ésimo pronóstico respectivamente. Es fácil darse cuenta que $W$ es una penalización por el ancho del intervalo, y que $O$ y $U$ son penalizaciones por sobre y subestimación respectivamente.

Se considera mejor al modelo que minimiza estas métricas.

## 3.6 Selección de parámetros y validación del modelo

La correcta elección de los parámetros del modelo constituye una de las tareas más importantes para lograr un buen ajuste de los datos. No resulta conveniente utilizar todos los datos disponibles para este fin, ya que esto puede conducir a un sobreajuste (*overfitting*). Se habla de sobreajuste cuando el modelo ase adapta excesivamente a los datos de entrenamiento, lo que perjudica su capacidad para generalizar sobre datos nuevos. Para evitar este problema se aplican técnicas específicas de validación que permiten seleccionar los parámetros sin comprometer la capacidad predictiva del modelo.

La validación *holdout* consiste en reservar una parte del conjunto de entrenamiento como validación, e ir probando las métricas de evaluación de las distintas configuraciones de parámetros del modelo, ajustado sobre el resto de los datos de entrenamiento, sobre este nuevo conjunto. Luego, para ajustar el modelo con la mejor combinación de parámetros, se utilizarán tanto los datos de entrenamiento como de validación. Por la ordinalidad de los datos, el conjunto de validación tendrá que ser más reciente que el conjunto de entrenamiento. 

$$
\text{Conjunto de entrenamiento total : } \{\underbrace{z_1, ...,  z_c}_{\text{Entrenamiento}}, \underbrace{z_{c+1}, ..., z_n}_{\text{Validación}} \}
$${#eq-21}

Si alguna serie presentara estacionalidad, se priorizará que el conjunto de validación tenga el largo del ciclo, para poder evaluar el ajuste del modelo en todo el ciclo.

Para ARIMA se usará el método de Box-Jenkins. En este método se compararán aquellos modelos que cumplan con los supuestos, y se elegirá como mejor combinación de parámetros aquella que minimize el AIC, medida de ajuste que penaliza por la cantidad de parámetros.

\newpage

# 4. Aplicación

La aplicación empírica de esta tesina tuvo como objetivo implementar, ajustar y comparar los modelos presentados en la [sección 3](#metodologia), utilizando un conjunto de series temporales seleccionadas. Para ello, se empleó el lenguaje de programación Python, junto con diversas librerías de código abierto.

Se trabajó con series temporales reales, obtenidas de bases de datos públicas y confiables. Cada serie fue modelada desde tres enfoques metodológicos distintos: modelos estadísticos tradicionales, algoritmos de aprendizaje automático y modelos de aprendizaje profundo. Para cada uno de ellos, se exploraron distintas configuraciones de parámetros, explicando su significado y función en el ajuste.


  - Los modelos estadísticos clásicos, como ARIMA y SARIMA, serán ajustados utilizando la librería `pmdarima`. La selección de parámetros se realizó manualmente y mediante procedimientos automáticos, tomando como criterio principal el Criterio de Información de Akaike (AIC).

  - Para el enfoque de aprendizaje automático, se emplearon algoritmos de boosting, específicamente XGBoost y LightGBM, implementados con las librerías `xgboost` y `lightgbm` respectivamente.

  - En el caso de los modelos de aprendizaje profundo, se entrenaron redes LSTM utilizando la librería `scalecast`.

  - Finalmente, se exploraron dos modelos fundacionales preentrenados: TimeGPT, accedido a través de la API de Nixtla mediante la librería `nixtla`, y Chronos, una familia de modelos preentrenados desarrollada por *Amazon Web Services* cuya implementación se llevó a cabo con la librería `autogluon`.

Para cada modelo se obtuvieron tanto pronósticos puntuales como probabilísticos, con una confianza del 80%, y se evaluaron utilizando métricas como el error absoluto porcentual medio (MAPE) y el *Interval Score*. La mejor combinación de hiperparámetros para cada modelo se eligió en base al MAPE, con excepción de los modelos ARIMA. Además, se consideraron aspectos adicionales como el tiempo de cómputo, la facilidad de implementación y la interpretabilidad de los resultados. Los resultados fueron presentados en tablas comparativas y visualizaciones gráficas, acompañadas de un análisis crítico.

## 4.1 Series a utilizar

```{python}
# TEMP

atenciones_guardia = pd.read_excel(io='../Datos/Atenciones de guardia en el HNVV por patologías respiratorias (vigiladas por epidemiología).xlsx' )

# Aseguro que la columna fecha tenga el formato adecuado
atenciones_guardia['fec'] = pd.to_datetime(atenciones_guardia['fec'], format='%Y-%m-%d')

# Filtro las columnas importantes y las renombro
atenciones_guardia = atenciones_guardia[['fec', 'frec']]
atenciones_guardia.columns = ['ds','y']

# --------------------------------------------------------------------
# Cargamos los datos
trabajadores = pd.read_excel(io='../Datos/trabajoregistrado_2502_estadisticas.xlsx', sheet_name= 'A.2.1', thousands='.', decimal=',', header=1, usecols='A,M', skipfooter=5, skiprows=84)

# Renombramos las columnas
trabajadores.columns = ['ds', 'y']

# Asignamos formato fecha
meses = {
    'ene': '01', 'feb': '02', 'mar': '03', 'abr': '04', 
    'may': '05', 'jun': '06', 'jul': '07', 'ago': '08', 
    'sep': '09', 'oct': '10', 'nov': '11', 'dic': '12'
}

trabajadores['ds'] = trabajadores['ds'].str.replace('*','')
trabajadores['ds'] = trabajadores['ds'].apply(
    lambda x: '01-' + x.replace(x.split('-')[0], meses.get(x.split('-')[0].lower(), '')).replace(x.split('-')[1], '20' + x.split('-')[1])
)

trabajadores['ds'] = pd.to_datetime(trabajadores['ds'], format='%d-%m-%Y')

# Eliminamos los datos del 2025 para tener solo años completos
trabajadores = trabajadores[trabajadores['ds'].dt.year != 2025]

# --------------------------------------------------------------------

import glob

# Cargamos todos los archivos txt
ruta = glob.glob('../Datos/Datos meteorologicos/*.txt')
tiempo_region = pd.concat([pd.read_fwf(f, skiprows=[1], dtype={'FECHA' : str, 'HORA': str} , encoding='cp1252') for f in ruta], ignore_index=True)

# Filtramos los datos de rosario
tiempo_rosario = tiempo_region[tiempo_region['NOMBRE'] == 'ROSARIO AERO']

# Creamos la columna fecha y hora
tiempo_rosario['HORA'] = tiempo_rosario['HORA'].apply(lambda x: '0' + x if len(x) != 2 else x)
tiempo_rosario.loc[:,'ds'] = tiempo_rosario['FECHA'].apply(lambda x: x[0:2] + '-' + x[2:4] + '-' + x[4:len(x)])
tiempo_rosario['ds'] = pd.to_datetime(tiempo_rosario['ds'] + ' ' + tiempo_rosario['HORA'], format='%d-%m-%Y %H')

# Nos quedamos con las columnas utiles y renombramos la respuesta
tiempo_rosario = tiempo_rosario[['ds', 'TEMP', 'HUM', 'PNM']]
tiempo_rosario.columns = ['ds', 'y', 'HUM', 'PNM'] # % de Humedad y Presion a nivel del mar en hectopascales 

```

```{python}
# Creamos datasets con la info que conocemos de las series
atenciones_trunc = atenciones_guardia.head(len(atenciones_guardia)-12).copy()
trabajadores_trunc = trabajadores.head(len(trabajadores)-12).copy()
temperatura_trunc = tiempo_rosario.head(len(tiempo_rosario)-24).copy()
```

Con el objetivo de ver las capacidades de los modelos bajo distintas circunstancias, se decidió pronosticar sobre 3 series con distintas características. Estas son:

- Atenciones de guardia mensuales por patologías respiratorias en un hospital de Rosario.

- Personas registradas con empleo asalariado por mes en el sector educativo privado de Argentina.

- Temperatura horaria en la ciudad de Rosario.

La primera serie analizada corresponde al número mensual de atenciones de guardia por patologías respiratorias (Códigos CIE10: J09–J18, J21, J22 y J44) en el Hospital de Niños Víctor J. Vilela de la ciudad de Rosario. Esta información fue provista por la Dirección General de Estadística de la Municipalidad de Rosario. La serie mostró una marcada estacionalidad, con picos en los meses de invierno, y una fuerte caída en 2020, atribuida a las medidas sanitarias adoptadas durante la pandemia de COVID-19. Esta estacionalidad es más dicernible en el gráfico @fig-atenciones_estacionalidad del anexo.

```{python}
#| fig-cap: "Atenciones en guardia por enfermedades respiratorias"

(ggplot(atenciones_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Atenciones")
  )
```

La cantidad de personas en Argentina con empleo asalariado en el área de enseñanza y registradas en el sector privado aumenta casi constantemente año tras año. Presenta descensos drásticos en los meses de diciembre y enero. En esta serie también se evidencian las consecuencias de la pandemia. Los datos fueron relevados del informe de [Situación y evolución del Trabajo Registrado](https://www.argentina.gob.ar/trabajo/estadisticas/situacion-y-evolucion-del-trabajo-registrado) elaborado por el Instituto Nacional de Estadísticas y Censos (INDEC).

```{python}
#| fig-cap: "Personas con empleo asalariado en el área de enseñanza y registradas en el sector privado"

(ggplot(trabajadores_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#5299CB", size = 0.3) +
  geom_line(color = "#5299CB") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Trabajadores (miles)")
  )
```

Por último, se analizarán las temperaturas por hora a lo largo de los días del mes de marzo 2025, estudiando sus comportamientos en relación a la humedad relativa y a la presión atmosférica estándar. Gracias al gráfico @fig-temp_estacional del anexo, se puede observar el patrón estacional diario que tiene la temperatura, la cual se mantiene constante entre la noche y la mañana, pero a la tarde sube pronunciadamente. La información necesaria fue extraída de la página del [Servicio Meteorológico Nacional](https://www.smn.gob.ar/descarga-de-datos).

```{python}

(ggplot(temperatura_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#E44E58", size = 0.3) +
  geom_line(color = "#E44E58") +
  scale_x_date(date_labels = "%d", date_breaks = "1 day") +
  labs(x = "Día", y = "Temperatura (Cº)") +
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/temperatura1.png", width=6, height=4/3.1, dpi=700)

(ggplot(temperatura_trunc) +
  aes(x = 'ds', y  = 'HUM') + 
  geom_point(color = "black", size = 0.3) +
  geom_line(color = "black") +
  scale_x_date(date_labels = "%d", date_breaks = "1 day") +
  labs(x = "Día", y = "Humedad (%)") +
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/temperatura2.png", width=6, height=4/3.1, dpi=700)

(ggplot(temperatura_trunc) +
  aes(x = 'ds', y  = 'PNM') + 
  geom_point(color = "black", size = 0.3) +
  geom_line(color = "black") +
  scale_x_date(date_labels = "%d", date_breaks = "1 day") +
  labs(x = "Día", y = "Presión atmosférica (hPa)") +
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/temperatura3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-temp layout-nrow=3}

![](../Imgs/plotnine/temperatura1.png)

![](../Imgs/plotnine/temperatura2.png)

![](../Imgs/plotnine/temperatura3.png)

Temperatura (Cº), humedad (%) y presión atmosférica (hPa) en Rosario por hora.

:::

## 4.2 Ajuste y evaluación de modelos

```{python}

# Primero cargamos los ambientes donde tenemos todos los resultados

globals().update(load_env('../Codigo/Ambiente/Amb_Aplicacion.pkl'))
globals().update(load_env('../Codigo/Ambiente/Amb_Aplicacion_chronos.pkl'))

# Guardamos los resultados de Chronos en las metricas
metricas_1.loc[len(metricas_1)] = ['Chronos', 3, resultados_1_chronos3['mape'], resultados_1_chronos3['score'], resultados_1_chronos3['tiempo']]
metricas_1.loc[len(metricas_1)] = ['Chronos', 6, resultados_1_chronos6['mape'], resultados_1_chronos6['score'], resultados_1_chronos6['tiempo']]
metricas_1.loc[len(metricas_1)] = ['Chronos', 12, resultados_1_chronos['mape'], resultados_1_chronos['score'], resultados_1_chronos['tiempo']]

metricas_2.loc[len(metricas_2)] = ['Chronos', 3, resultados_2_chronos3['mape'], resultados_2_chronos3['score'], resultados_2_chronos3['tiempo']]
metricas_2.loc[len(metricas_2)] = ['Chronos', 6, resultados_2_chronos6['mape'], resultados_2_chronos6['score'], resultados_2_chronos6['tiempo']]
metricas_2.loc[len(metricas_2)] = ['Chronos', 12, resultados_2_chronos['mape'], resultados_2_chronos['score'], resultados_2_chronos['tiempo']]

metricas_3.loc[len(metricas_3)] = ['Chronos', 6, resultados_3_chronos6['mape'], resultados_3_chronos6['score'], resultados_3_chronos6['tiempo']]
metricas_3.loc[len(metricas_3)] = ['Chronos', 12, resultados_3_chronos12['mape'], resultados_3_chronos12['score'], resultados_3_chronos12['tiempo']]
metricas_3.loc[len(metricas_3)] = ['Chronos', 24, resultados_3_chronos['mape'], resultados_3_chronos['score'], resultados_3_chronos['tiempo']]
```

### 4.2.1 ARIMA

Debido a la complejidad que implica modelar series de alta frecuencia con variables exógenas mediante ARIMA, la serie de temperatura se analizará únicamente a través del método automático de selección de modelos, descrito más adelante.

Antes de proponer modelos, se debe hacer un breve análisis exploratorio de las series. En primer lugar se debe observar que la variancia en cada período es la misma. De lo contrario se debería transformar la variable que se desea pronosticar.

```{python}

(
  ggplot(atenciones_trunc) +
  aes(x = atenciones_trunc["ds"].dt.year, y = atenciones_trunc["y"], group = atenciones_trunc["ds"].dt.year) +
  geom_boxplot(fill = "#6BC78A") +
  labs(x = "Año", y = "Atenciones") + 
  scale_x_continuous(breaks = range(min(atenciones_trunc["ds"].dt.year), max(atenciones_trunc["ds"].dt.year)+1))+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
).save("../Imgs/plotnine/boxplot1.png", width=6, height=4/2.7, dpi=700)


(
  ggplot(trabajadores_trunc) +
  aes(x = trabajadores_trunc["ds"].dt.year, y = trabajadores_trunc["y"], group = trabajadores_trunc["ds"].dt.year) +
  geom_boxplot(fill = "#5299CB") +
  labs(x = "Año", y = "Trabajadores (miles)") + 
  scale_x_continuous(breaks = range(min(trabajadores_trunc["ds"].dt.year), max(trabajadores_trunc["ds"].dt.year)+1))+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
).save("../Imgs/plotnine/boxplot2.png", width=6, height=4/2.7, dpi=700)

```

::: {#fig-box layout-nrow=2}

![](../Imgs/plotnine/boxplot1.png)

![](../Imgs/plotnine/boxplot2.png)

Variancias periódicas de las series.

:::

```{python}
# Transformacion de box y cox
atenciones_ajust, atenciones_lambda = stats.boxcox(atenciones_trunc["y"])
temperatura_ajust, temperatura_lambda = stats.boxcox(temperatura_trunc["y"])

temp = atenciones_ajust
atenciones_ajust = atenciones_trunc.copy()
atenciones_ajust['y'] = temp
```

Dadas las diferencias en la variabilidad anual de las atenciones por año, se calculó $\lambda$ para la transformación de Box y Cox concluyendo que las atenciones en guardia por patologías respiratorias necesitan ser transformadas aplicando la función exponencial ($\lambda$ = `{python} round(atenciones_lambda, 2)`).

El primer paso de la modelización es estudiar las estacionalidades. Se mencionó anteriormente que las 2 series tienen estacionalidad, es por esto que se diferencian y se grafican, buscando que no sea visible ningún patrón estacional.

```{python}

atenciones_ajust["y_sin_est"] = atenciones_ajust['y'].diff(12)
trabajadores_trunc["y_sin_est"] = trabajadores_trunc['y'].diff(12)

(ggplot(atenciones_ajust) +
  aes(x = 'ds', y  = 'y_sin_est') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Atenciones (diferenciada)")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/diffe1.png", width=6, height=4/2.7, dpi=700)

(ggplot(trabajadores_trunc) +
  aes(x = 'ds', y  = 'y_sin_est') + 
  geom_point(color = "#5299CB", size = 0.3) +
  geom_line(color = "#5299CB") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Trabajadores (diferenciada)")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/diffe2.png", width=6, height=4/2.7, dpi=700)
  

```

::: {#fig-diffe layout-nrow=2}

![](../Imgs/plotnine/diffe1.png)

![](../Imgs/plotnine/diffe2.png)

Series diferenciadas estacionalmente.
:::

Una vez diferenciadas estacionalmente, es importante que no exista una tendencia clara, por lo que se estandarizan estacionariamente.

```{python}


atenciones_ajust["y_diff"] = atenciones_ajust['y_sin_est'].diff(1)
trabajadores_trunc["y_diff"] = trabajadores_trunc['y_sin_est'].diff(1)

(ggplot(atenciones_ajust) +
  aes(x = 'ds', y  = 'y_diff') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Atenciones (diferenciada)")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/diff1.png", width=6, height=4/2.7, dpi=700)

(ggplot(trabajadores_trunc) +
  aes(x = 'ds', y  = 'y_diff') + 
  geom_point(color = "#5299CB", size = 0.3) +
  geom_line(color = "#5299CB") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Trabajadores (diferenciada)")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/diff2.png", width=6, height=4/2.7, dpi=700)
  

```

::: {#fig-diff layout-nrow=2}

![](../Imgs/plotnine/diff1.png)

![](../Imgs/plotnine/diff2.png)

Series diferenciadas estacionariamente.
:::

Para descubrir las componentes AR y MA que afectan a las series se grafican las autocorrelaciones y autocorrelaciones parciales de cada una.

```{python}

autocorr_plot(atenciones_ajust["y_diff"], lags=25).save("../Imgs/plotnine/autocorr1.png", width=6, height=4/2.1, dpi=700)

autocorr_plot(atenciones_ajust["y_diff"], lags=25, atype='pacf').save("../Imgs/plotnine/autocorr2.png", width=6, height=4/2.1, dpi=700)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/autocorr1.png)

![](../Imgs/plotnine/autocorr2.png)

Autocorrelaciones de la serie de atenciones.
:::

En la serie de atenciones se puede ver que tanto en las autocorrelaciones como en las autocorrelaciones parciales, hay valores significativos en el quinto y doceavo rezago, lo que podrían significar que existen compotentes MA y AR en la parte estacional o estacionaria de la serie.

Se proponen entonces los modelos $SARIMA(0,1,1)(0,1,0)_{12}$ y $SARIMA(0,1,0)(0,1,1)_{12}$.

```{python}

autocorr_plot(trabajadores_trunc["y_diff"], lags=25).save("../Imgs/plotnine/autocorr3.png", width=6, height=4/2.1, dpi=700)

autocorr_plot(trabajadores_trunc["y_diff"], lags=25, atype='pacf').save("../Imgs/plotnine/autocorr4.png", width=6, height=4/2.1, dpi=700)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/autocorr3.png)

![](../Imgs/plotnine/autocorr4.png)

Autocorrelaciones de la serie de trabajadores.
:::

En las autocorrelaciones de la serie de trabajadores resultan significativos el primer y doceavo rezago, mientras que en las autocorrelaciones parciales se destacan los rezagos 1, 12, 13 y 24. Se puede suponer que el rezago 13 no es en realidad significativo y que la serie presenta una componente MA en la parte estacionaria y una AR en la estacional, formando el modelo $SARIMA(0,1,1)(1,1,0)_{12}$.

Además, por medio de la función `auto_arima` de la librería `pmdarima` se propusieron modelos automáticos. Se elegirá como mejor modelo aquel que minimice el Criterio de Información de Akaike (AIC) y cumpla con las propiedades del modelo ARIMA.

\begin{table}[H]
\centering
\caption{Cumplimiento de las condiciones de estacionariedad e invertibilidad de los modelos ajustados.}
\begin{tabular}{|clclcl|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Modelo}}     & \multicolumn{1}{c|}{\multirow{2}{*}{AIC}} & \multicolumn{2}{c|}{Parte regular}                                         & \multicolumn{2}{c|}{Parte estacional}                 \\ \cline{3-6} 
\multicolumn{1}{|c|}{}                            & \multicolumn{1}{c|}{}                     & \multicolumn{1}{c|}{Est.} & \multicolumn{1}{c|}{Inv.} & \multicolumn{1}{c|}{Est.} & \multicolumn{1}{c|}{Inv.} \\ \hline
\multicolumn{6}{|c|}{Atenciones en guardia}                                                                                                                                                                                        \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,1)(0,1,0)_{12}$} & \multicolumn{1}{l|}{857.6}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,0)(0,1,1)_{12}$} & \multicolumn{1}{l|}{852.1}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,0)(1,0,0)_{12}$} & \multicolumn{1}{l|}{1013.5}               & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             \\ \hline
\multicolumn{6}{|c|}{Trabajadores}                                                                                                                                                                                                 \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,1)(1,1,0)_{12}$} & \multicolumn{1}{l|}{359.9}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             \\ \hline
\multicolumn{1}{|c|}{$SARIMA(2,0,0)(2,1,0)_{12}$} & \multicolumn{1}{l|}{356.7}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             \\ \hline
\multicolumn{6}{|c|}{Temperatura}                                                                                                                                                                                                  \\ \hline
\multicolumn{1}{|c|}{$SARIMAX(1,1,1)(2,0,1)_{24}$} & \multicolumn{1}{l|}{1067.1}               & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{No}              & \multicolumn{1}{c|}{Si}             \\ \hline
\end{tabular}
\end{table}

Para la serie de atenciones se seguirá trabajando con el segundo modelo propuesto y con el seleccionado de manera automática, ya que el parámetro MA del primer modelo prupuesto no es significativo. Por otro lado, los dos modelos probados para la serie de trabajadores seguirán siendo utilizados. El modelo automático para la serie de temperaturas no goza de estacionariedad, por lo que se proponen los modelos $SARIMAX(1,1,1)(1,0,1)_{24}$, $SARIMAX(1,1,0)(2,0,1)_{24}$ y $SARIMAX(1,1,0)(1,1,0)_{24}$, de los cúales únicamente el último cumple con todas las propiedades buscadas, con un AIC de 1119.3.

Las salidas de los modelos se pueden ver en el [anexo](#salidas_arima) para comprobar los resultados.

Lo siguiente es comprobar que los residuos estandarizados se comporten como ruido blanco.

```{python}
resid_check(resultados_arima['resid_arima_atenciones_2'], ds = atenciones_trunc['ds'], time='%Y', name='resid_atenciones_2')
```

::: {#fig-pronarima2 layout-nrow=2}

![](../Imgs/plotnine/resid_atenciones_2_1.png)

![](../Imgs/plotnine/resid_atenciones_2_2.png)

![](../Imgs/plotnine/resid_atenciones_2_3.png)

![](../Imgs/plotnine/resid_atenciones_2_4.png)

Comprobación de supuestos del segundo modelo arima manual para la serie de atenciones.
:::

El histograma muestra la distribución de los residuos, pudiendo ver y comprobar con el test de Kolmogorov-Smirnov que es normal. La serie de los residuos muestra como estos no tienen un patrón particular ni variabilidad dependiente del tiempo, solo 2 *outlayers* fáciles de ignorar. Por último, los gráficos inferiores muestran las autocorrelaciones y autocorrelaciones parciales, esperando que no haya ninguna significativa. Por medio del test de Ljung-Box se comprueba que ninguna lo es y se concluye que los residuos no están correlacionados. Por lo tanto todos los supuestos se cumplen para este modelo. 

Con el propósito de no poblar de gráficos el documento, el resto de los gráficos para la comprobación de supuestos de los demás modelos se encuentran en el [anexo](#supuestos_arima). En estos se destaca que todos los modelos, con excepción del seleccionado automáticamente para los trabajadores, superaron la comprobación de supuestos. El problema de este modelo es la existencia de correlación en los residuos, supuesto que es sumamente importante al ajustar modelos ARIMA. Si bien los residuos del modelo seleccionado manualmente para los trabajadores no cumple con normalidad, es sencillo ver que esto se debe a unos poco valores extremos, y que sin estos es muy probable que sí se distribuyan normalmente.

```{python}

plot_forecast(data = atenciones_guardia, forecast = resultados_arima['pred_atenciones_2'], pred_color = '#6BC78A', line_color='black', label = 'ARIMA', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)).save("../Imgs/plotnine/arima1.png", width=6, height=4/2.7, dpi=700)

plot_forecast(data = atenciones_guardia, forecast = resultados_1_arima['pred'], pred_color = '#6BC78A', line_color='black', label = 'ARIMA', ylabel= 'Atenciones', legend_position = 'none', long = 36).save("../Imgs/plotnine/arima2.png", width=6, height=4/2.7, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_arima['pred_trabajadores_1'], line_color='black', pred_color = '#5299CB', label = 'ARIMA', long = 36, ylabel= 'Trabajadores (miles)', legend_position = (0.3,0.98)).save("../Imgs/plotnine/arima3.png", width=6, height=4/2.7, dpi=700)

plot_forecast(data = tiempo_rosario[['ds','y']], forecast = resultados_arima['pred_temperatura_3'], pred_color = '#E23C47', line_color='black', label = 'ARIMA', long = 72, xlabel='Día', ylabel= 'Atenciones', legend_position = 'none').save("../Imgs/plotnine/arima4.png", width=6, height=4/2.7, dpi=700)

```



```{python}
# CALCULO DE MAPES

# Atenciones guardia automatico
# Horizonte 3
mape_atenciones_auto_3 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], y_pred=resultados_1_arima['pred']['pred'].head(3))
# Horizonte 6
mape_atenciones_auto_6 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], y_pred=resultados_1_arima['pred']['pred'].head(6))

# Atenciones guardia modelo 2
mape_atenciones_2 = mean_absolute_percentage_error(y_true=atenciones_guardia.tail(12)['y'], y_pred=resultados_arima['pred_atenciones_2']['pred'])
# Horizonte 3
mape_atenciones_2_3 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], y_pred=resultados_arima['pred_atenciones_2']['pred'].head(3))
# Horizonte 6
mape_atenciones_2_6 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], y_pred=resultados_arima['pred_atenciones_2']['pred'].head(6))

# Trabajadores modelo 1
mape_trabajadores_1 = mean_absolute_percentage_error(y_true=trabajadores.tail(12)['y'], y_pred=resultados_arima['pred_trabajadores_1']['pred'])
# Horizonte 3
mape_trabajadores_1_3 = mean_absolute_percentage_error(y_true=(trabajadores.head(len(trabajadores)-9)).tail(3)['y'], y_pred=resultados_arima['pred_trabajadores_1']['pred'].head(3))
# Horizonte 6
mape_trabajadores_1_6 = mean_absolute_percentage_error(y_true=(trabajadores.head(len(trabajadores)-6)).tail(6)['y'], y_pred=resultados_arima['pred_trabajadores_1']['pred'].head(6))

# Temperatura modelo 3
mape_temperatura_3 = mean_absolute_percentage_error(y_true=tiempo_rosario.tail(24)['y'], y_pred=resultados_arima['pred_temperatura_3']['pred'])
# Horizonte 6
mape_temperatura_3_6 = mean_absolute_percentage_error(y_true=(tiempo_rosario.head(len(tiempo_rosario)-18)).tail(6)['y'], y_pred=resultados_arima['pred_temperatura_3']['pred'].head(6))
# Horizonte 12
mape_temperatura_3_12 = mean_absolute_percentage_error(y_true=(tiempo_rosario.head(len(tiempo_rosario)-12)).tail(12)['y'], y_pred=resultados_arima['pred_temperatura_3']['pred'].head(12))


# ----------------- CALCULO DE INTERVAL SCORES -----------------------
# Atenciones automatico
# Horizonte 3
is_atenciones_auto_3 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], lower = resultados_1_arima['pred']['lower'].head(3), upper = resultados_1_arima['pred']['upper'].head(3), alpha = 0.2)
# Horizonte 6
is_atenciones_auto_6 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], lower = resultados_1_arima['pred']['lower'].head(6), upper = resultados_1_arima['pred']['upper'].head(6), alpha = 0.2)

# Atenciones modelo 2
is_atenciones_2 = interval_score(obs=atenciones_guardia.tail(12)['y'], lower = resultados_arima['pred_atenciones_2']['lower'], upper = resultados_arima['pred_atenciones_2']['upper'], alpha = 0.2)
# Horizonte 3
is_atenciones_2_3 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], lower = resultados_arima['pred_atenciones_2']['lower'].head(3), upper = resultados_arima['pred_atenciones_2']['upper'].head(3), alpha = 0.2)
# Horizonte 6
is_atenciones_2_6 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], lower = resultados_arima['pred_atenciones_2']['lower'].head(6), upper = resultados_arima['pred_atenciones_2']['upper'].head(6), alpha = 0.2)

# Trabajadores
is_trabajadores_1 = interval_score(obs=trabajadores.tail(12)['y'], lower = resultados_arima['pred_trabajadores_1']['lower'], upper = resultados_arima['pred_trabajadores_1']['upper'], alpha = 0.2)
# Horizonte 3
is_trabajadores_1_3 = interval_score(obs=(trabajadores.head(len(trabajadores)-9)).tail(3)['y'], lower = resultados_arima['pred_trabajadores_1']['pred'].head(3), upper = resultados_arima['pred_trabajadores_1']['pred'].head(3), alpha = 0.2)
# Horizonte 6
is_trabajadores_1_6 = interval_score(obs=(trabajadores.head(len(trabajadores)-6)).tail(6)['y'], lower = resultados_arima['pred_trabajadores_1']['lower'].head(6), upper = resultados_arima['pred_trabajadores_1']['upper'].head(6), alpha = 0.2)

# Temperatura
is_temperatura_3 = interval_score(obs=tiempo_rosario.tail(24)['y'], lower = resultados_arima['pred_temperatura_3']['lower'], upper = resultados_arima['pred_temperatura_3']['upper'], alpha = 0.2)
# Horizonte 6
is_temperatura_3_6 = interval_score(obs=(tiempo_rosario.head(len(tiempo_rosario)-18)).tail(6)['y'], lower = resultados_arima['pred_temperatura_3']['lower'].head(6), upper = resultados_arima['pred_temperatura_3']['upper'].head(6), alpha = 0.2)
# Horizonte 12
is_temperatura_3_12 = interval_score(obs=(tiempo_rosario.head(len(tiempo_rosario)-12)).tail(12)['y'], lower = resultados_arima['pred_temperatura_3']['lower'].head(12), upper = resultados_arima['pred_temperatura_3']['upper'].head(12), alpha = 0.2)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/arima1.png)

![](../Imgs/plotnine/arima2.png)

Pronóstico de atenciones en guardia con ARIMA.
:::


| Modelo                      | Horizonte | MAPE    | *Interval Score* |
|-----------------------------|-----------|---------|------------------|
|                             | 3         | `{python} round(mape_atenciones_2_3, 4)` | `{python} round(is_atenciones_2_3, 4)`          |
| $SARIMA(0,1,0)(0,1,1)_{12}$ | 6         | `{python} round(mape_atenciones_2_6, 4)` | `{python} round(is_atenciones_2_6, 4)`          |
|                             | 12        | `{python} round(mape_atenciones_2, 4)` | `{python} round(is_atenciones_2, 4)`          |
|    | |  | |
|                             | 3         | `{python} round(mape_atenciones_auto_3, 4)` | `{python} round(is_atenciones_auto_3, 4)`          |
| $SARIMA(0,1,0)(1,0,0)_{12}$ | 6         | `{python} round(mape_atenciones_auto_6, 4)` | `{python} round(is_atenciones_auto_6, 4)`          |
|                             | 12        | `{python} round(resultados_1_arima['mape'], 4)` | `{python} round(resultados_1_arima['score'], 4)` |
: Métricas de evaluación para la serie de atenciones por guardia con modelos ARIMA. {#tbl-arimaAtenciones}


Se puede observar en la tabla @tbl-arimaAtenciones que para la serie de atenciones en guardia por patologías respiratorias el modelo automático parece pronosticar mejor en los primeros 6 meses. Sin embargo, este no capta el aumento de casos en invierno, y es por esto que en el pronóstico a 12 meses el segundo modelo manual es mejor.

![Pronóstico para la serie de trabajadores.](../Imgs/plotnine/arima3.png)



| Modelo                      | Horizonte | MAPE    | *Interval Score* |
|-----------------------------|-----------|---------|------------------|
|                             | 3         | `{python} round(mape_trabajadores_1_3, 4)` | `{python} round(is_trabajadores_1_3, 4)` |
| $SARIMA(0,1,1)(1,1,0)_{12}$ | 6         | `{python} round(mape_trabajadores_1_6, 4)` | `{python} round(is_trabajadores_1_6, 4)` |
|                             | 12        | `{python} round(mape_trabajadores_1, 4)`   | `{python} round(is_trabajadores_1, 4)`   |
: Métricas de evaluación para la serie de trabajadores registrados con modelos ARIMA {#tbl-arimaTrabajadores}

El modelo ARIMA elegido para la serie de trabajadores registrados en el sector privado de enseñanza logra captar muy bien el patrón de datos y el pronóstico es muy cercano a lo valores reales.

![Pronóstico para la serie de temperaturas](../Imgs/plotnine/arima4.png)

| Modelo                      | Horizonte | MAPE    | *Interval Score* |
|-----------------------------|-----------|---------|------------------|
|                             | 6         | `{python} round(mape_temperatura_3_6, 4)` | `{python} round(is_temperatura_3_6, 4)` |
| $SARIMA(1,1,0)(1,1,0)_{24}$ | 12         | `{python} round(mape_temperatura_3_12, 4)` | `{python} round(is_temperatura_3_12, 4)` |
|                             | 24        | `{python} round(mape_temperatura_3, 4)`   | `{python} round(is_temperatura_3, 4)`   |
: Métricas de evaluación para la serie de temperaturas con modelos ARIMA. {#tbl-arimaTemperatura}

El pronóstico de la temperatura es muy bueno en las primeras 6 horas, pero empeora ligeramente en horizontes más largos, sin embargo, los intervalos de confianza incluyen casi constantemente a los valores reales y se adapta bien a los cambios estacionales.

Un problema que se encuentra es que el intervalo de confianza para las atenciones por guardia toma valores negativos, algo claramente ilógico. Esto se puede resolver ajustando modelos sobre el logaritmo de la variable, y luego transformando los valores pronosticados a la escala original. Esta solución queda propuesta para un futuro trabajo.


### 4.2.2 Modelos de aprendizaje automático

Los modelos de pronóstico basados en el aprendizaje automático no tienen supuestos que cumplir, por lo que el modelaje se vuelve mucho más sencillo y automatizable. Los únicos aspectos en los que se debe tener especial cuidado y atención es en la selección de características y parámetros del modelo.

Tanto para XGBoost como para LightGBM las características elegidas para todas las series fueron las siguientes:

- Identificación temporal

- El promedio de las 3 observaciones anteriores 

- El desvío estándar de las 3 observaciones anteriores

- El valor del primer rezago

- El valor del segundo rezago

- El valor del rezago estacional

Con el objetivo de seleccionar los mejores hiperparámetros, se elaboró una grilla con diferentes combinaciones de valores para cada parámetro. Se ajustaron modelos con cada combinación en la grilla y se evaluaron sobre un conjunto de validación.

XGBoost y Lightgbm permiten parametrizar los modelos de varias formas, los parámetros que se decidieron probar en este trabajo son los siguientes:

- Número de árboles que se contruyen paralelamente en cada iteración ($A$). Opciones: 20, 50, 100, 150.

- Profundidad máxima del árbol ($P$). Opciones: 2, 3, 4, 5.

- Número máximo de hojas del árbol ($H$). Opciones: 2, 4, 8, 16.

- Tasa de aprendizaje en el método del gradiente ($\eta$). Opciones: 0.001, 0.1, 0.2.

- Proporción de características que se usa en cada árbol ($C$). Opciones: 0.7, 1.

En las tablas @tbl-resultadosxgb y @tbl-resultadoslgbm se muestran para cada serie que combinación de parámetros, de las 384 posibles, fue la que menor MAPE produjo sobre el conjunto de validación aplicando XGBoost o LightGBM respectivamente. Además, se presenta el MAPE e *Interval Score* que devuelve el modelo entrenado con todos los datos de entrenamiento y evaluado sobre el conjunto de prueba.


| Serie                    | Hor. | $A$ | $P$ | $H$ | $\eta$  | $C$ | MAPE  | Interval Score |
|--------------------------|------|-----|-----|-----|-----|-----|-------|----------------|
|                          | 3    | `{python} resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_xgb3['mape'], 4)` | `{python} round(resultados_1_xgb3['score'], 4)` |
| Atenciones    | 6    | `{python} resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_xgb6['mape'], 4)` | `{python} round(resultados_1_xgb6['score'], 4)` |
|                          | 12   | `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_xgb['mape'], 4)` | `{python} round(resultados_1_xgb['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 3    | `{python} resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`  | `{python} resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_xgb3['mape'], 4)` | `{python} round(resultados_2_xgb3['score'], 4)` |
| Trabajadores | 6    | `{python} resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_xgb6['mape'], 4)` | `{python} round(resultados_2_xgb6['score'], 4)` |
|                          | 12   | `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_xgb['mape'], 4)` | `{python} round(resultados_2_xgb['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 6    | `{python} resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_xgb6['mape'], 4)` | `{python} round(resultados_3_xgb6['score'], 4)` |
| Temperatura              | 12   | `{python} resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]` | `{python} resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_xgb12['mape'], 4)` | `{python} round(resultados_3_xgb12['score'], 4)` |
|                          | 24   | `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_xgb['mape'], 4)` | `{python} round(resultados_3_xgb['score'], 4)` |
: Modelos XGBoost seleccionados y métricas de evaluación. {#tbl-resultadosxgb}

```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_xgb['pred'], pred_color = '#6BC78A', line_color='black', label = 'XGBoost', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/xgboost1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_xgb['pred'], pred_color = '#5299CB', line_color='black', label = 'XGBoost', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/xgboost2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_xgb['pred'], pred_color = '#E44E58', line_color='black', label = 'XGBoost', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/xgboost3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronxgb layout-nrow=3}

![](../Imgs/plotnine/xgboost1.png)

![](../Imgs/plotnine/xgboost2.png)

![](../Imgs/plotnine/xgboost3.png)

Pronósticos con XGBoost.

:::

Los pronósticos con XGBoost fueron muy buenos sobre las 3 series. Si algo se puede remarcar es que en la serie de atenciones el modelo no capta del todo el pico de atenciones en invierno. 


| Serie                    | Hor. | $A$ | $P$ | $H$ | $\eta$  | $C$ | MAPE  | Interval Score |
|--------------------------|------|-----|-----|-----|-----|-----|-------|----------------|
|                          | 3    | `{python} resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_lgbm3['mape'], 4)` | `{python} round(resultados_1_lgbm3['score'], 4)` |
| Atenciones    | 6    | `{python} resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_lgbm6['mape'], 4)` | `{python} round(resultados_1_lgbm6['score'], 4)` |
|                          | 12   | `{python} resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_lgbm['mape'], 4)` | `{python} round(resultados_1_lgbm['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 3    | `{python} resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`  | `{python} resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_lgbm3['mape'], 4)` | `{python} round(resultados_2_lgbm3['score'], 4)` |
| Trabajadores | 6    | `{python} resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_lgbm6['mape'], 4)` | `{python} round(resultados_2_lgbm6['score'], 4)` |
|                          | 12   | `{python} resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_lgbm['mape'], 4)` | `{python} round(resultados_2_lgbm['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 6    | `{python} resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_lgbm6['mape'], 4)` | `{python} round(resultados_3_lgbm6['score'], 4)` |
| Temperatura              | 12   | `{python} resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]` | `{python} resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_lgbm12['mape'], 4)` | `{python} round(resultados_3_lgbm12['score'], 4)` |
|                          | 24   | `{python} resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_lgbm['mape'], 4)` | `{python} round(resultados_3_lgbm['score'], 4)` |
: Modelos LightGBM seleccionados y métricas de evaluación. {#tbl-resultadoslgbm}

```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_lgbm['pred'], pred_color = '#6BC78A', line_color='black', label = 'LightGBM', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/lgbm1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_lgbm['pred'], pred_color = '#5299CB', line_color='black', label = 'LightGBM', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/lgbm2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_lgbm['pred'], pred_color = '#E44E58', line_color='black', label = 'LightGBM', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/lgbm3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronlgbm layout-nrow=3}

![](../Imgs/plotnine/lgbm1.png)

![](../Imgs/plotnine/lgbm2.png)

![](../Imgs/plotnine/lgbm3.png)

Pronósticos con LightGBM.

:::

Los pronósticos con LightGBM son ligeramente peores a los conseguidos con XGBoost en todos los casos. Esta diferencia es especialmente importante en la serie de temperaturas, ya que en el pronóstico a 24 horas los intervalos de confianza en las primeras 6 horas no suelen cubrir a las observaciones. Este modelo tampoco pudo identificar el pico de atenciones en guardia en los meses invernales.

Algo interesante de notar sobre los modelos a partir de las tablas @tbl-resultadosxgb y @tbl-resultadoslgbm es que XGBoost suele elegir bosques más sencillos, con menos árboles, que a su vez son menos profundos y tienen menos hojas. También se evidenció la tendencia de LightGBM a expandir los árboles por las hojas. Otro aspecto importante de mencionar es que los modelos optaron en general por elegir una tasa de aprendizaje y una proporción de características utilizadas altas, elecciones que podrían llevar a un sobreajuste. Aún así, esto no se vio evidenciado en los resultados.

### 4.2.3 Redes neuronales (LSTM)

Los modelos basados en aprendizaje profundo se encargan de definir las características más relevantes y los parámetros más adecuados de forma automática. En las redes neuronales, como es el caso de LSTM, solo se tendrá que elegir la forma del modelo. 

Para evitar el sobreajuste en redes neuronales existen numerosas alternativas. Entre estas se puede optar por frenar el entrenamiento antes de completar todas las iteraciones si es que no se ve mejora en la función de pérdida, esto se conoce como *early stopping*. Otra opción es "ignorar" una cierta proporción de neuronas en cada iteración, que es equivalente a entrenar distintas redes neuronales. Esta última técnica es denominada *dropout*.

Para cada serie se entrenaron modelos LSTM con 300 iteraciones y una tasa de aprendizaje de 0.001. Se adoptó una paciencia para el *early stopping* de 10, esto quiere decir que si la función de pérdida no muestra mejoras en 10 iteraciones seguidas se deteniene el entrenamiento. Además, se probaron las siguientes características para el modelo:

- Capas y neuronas en el modelo ($N$). Opciones: [32], [12, 24], [24, 42].

- Rezagos con los que se entrena el modelo ($R$). Opciones: 1, 12, 24.

- *Dropout* en cada capa ($D$). Opciones: 0.1, 0.3.

- Función de activación ($A$). Opciones: ReLu, Tangente Hiperbólica (tanh).


| Serie                    | Hor. | $N$ | $R$ | $D$ | $A$ | MAPE  | _Interval Score_ |
|--------------------------|------|--------|-----|-----|-----|-------|------------------|
|              | 3    | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`       | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`               | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`          | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`       | `{python} round(resultados_1_lstm3['mape'], 4)`  | `{python} round(resultados_1_lstm3['score'], 4)`  |
| Atenciones   | 6    | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm6['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`    | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm6['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm6['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_1_lstm6['mape'], 4)`  | `{python} round(resultados_1_lstm6['score'], 4)`  |
|              | 12   | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`     | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`     | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`     | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`     | `{python} round(resultados_1_lstm['mape'], 4)`   | `{python} round(resultados_1_lstm['score'], 4)`   |
|              |      |                                                                                                                                |                                                                                                                                       |                                                                                                                                     |                                                                                                                                     |                                                  |                                                   |
|              | 3    | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`   | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_2_lstm3['mape'], 4)`  | `{python} round(resultados_2_lstm3['score'], 4)`  |
| Trabajadores | 6    | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`   | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_2_lstm6['mape'], 4)`  | `{python} round(resultados_2_lstm6['score'], 4)`  |
|              | 12   | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`     | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`     | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`     | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`     | `{python} round(resultados_2_lstm['mape'], 4)`   | `{python} round(resultados_2_lstm['score'], 4)`   |
|              |      |                                                                                                                                |                                                                                                                                       |                                                                                                                                     |                                                                                                                                     |                                                  |                                                   |
|              | 6    | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`   | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_3_lstm6['mape'], 4)`  | `{python} round(resultados_3_lstm6['score'], 4)`  |
| Temperatura  | 12   | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['units'].values[0]` | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]` | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]` | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]` | `{python} round(resultados_3_lstm12['mape'], 4)` | `{python} round(resultados_3_lstm12['score'], 4)` |
|              | 24   | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`     | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`     | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`     | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`     | `{python} round(resultados_3_lstm['mape'], 4)`   | `{python} round(resultados_3_lstm['score'], 4)`   |
: Modelos LSTM seleccionados y métricas de evaluación. {#tbl-resultadoslstm}


```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_lstm['pred'], pred_color = '#6BC78A', line_color='black', label = 'LSTM', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/lstm1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_lstm['pred'], pred_color = '#5299CB', line_color='black', label = 'LSTM', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/lstm2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_lstm['pred'], pred_color = '#E44E58', line_color='black', label = 'LSTM', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/lstm3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronlstm layout-nrow=3}

![](../Imgs/plotnine/lstm1.png)

![](../Imgs/plotnine/lstm2.png)

![](../Imgs/plotnine/lstm3.png)

Pronósticos con LSTM.

:::

Estos intervalos de confianza también fueron obtenidos gracias a *conformal predictions* pero se realizaron con funciones ya integradas en la librería `scalecast`.

### 4.2.4 Modelos fundacionales (TimeGPT y Chronos)

La ventaja de los modelos fundacionales por sobre las redes neuronales convencionales radica en que el ajuste de parámetros se realiza con un preentrenamiento en grandes conjuntos de datos. Esto significa que no es necesaria ninguna intervención manual. No es necesario definir características, ajustar parámetros, ni especificar la forma del modelo. Este modo de pronóstico se conoce como *zero-shot*.

Si se deseara un ajuste más controlado sobre la serie se podría hacer uso del "ajuste fino" (*fine tuning*). El ajuste fino consiste en evaluar la función de perdida con los parámetros preestablecidos y realizar iteraciones extra de entrenamiento para ajustar el modelo específicamente al conjunto de datos dado con el objetivo de minimizar aún más el error del pronóstico. Sin embargo, luego de numerosas pruebas, esta herramienta no logró cambios significativos en el pronóstico de las series estudiadas por sobre las configuraciones base, logrando en ciertos casos resultados peores y aumentos excesivos los tiempos de procesamiento. Es por esto que no se utiliza esta característica en los resultados finales pero se deja propuesta para futuras aplicaciones.


```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_gpt['pred'], pred_color = '#6BC78A', line_color='black', label = 'TimeGPT', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/gpt1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_gpt['pred'], pred_color = '#5299CB', line_color='black', label = 'TimeGPT', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/gpt2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_gpt['pred'], pred_color = '#E44E58', line_color='black', label = 'TimeGPT', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/gpt3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-prongpt layout-nrow=3}

![](../Imgs/plotnine/gpt1.png)

![](../Imgs/plotnine/gpt2.png)

![](../Imgs/plotnine/gpt3.png)

Pronósticos con TimeGPT.

:::

| Serie                    | Hor. | MAPE  | _Interval Score_ |
|--------------------------|------|-------|------------------|
|                          |3     |`{python} round(resultados_1_gpt3['mape'], 4)`|`{python} round(resultados_1_gpt3['score'], 4)`|
|Atenciones                |6     |`{python} round(resultados_1_gpt6['mape'], 4)`|`{python} round(resultados_1_gpt6['score'], 4)`|
|                          |12    |`{python} round(resultados_1_gpt['mape'], 4)`|`{python} round(resultados_1_gpt['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_2_gpt3['mape'], 4)`|`{python} round(resultados_2_gpt3['score'], 4)`|
|Trabajadores                |6     |`{python} round(resultados_2_gpt6['mape'], 4)`|`{python} round(resultados_2_gpt6['score'], 4)`|
|                          |12    |`{python} round(resultados_2_gpt['mape'], 4)`|`{python} round(resultados_2_gpt['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_3_gpt6['mape'], 4)`|`{python} round(resultados_3_gpt6['score'], 4)`|
|Temperatura                |6     |`{python} round(resultados_3_gpt12['mape'], 4)`|`{python} round(resultados_3_gpt12['score'], 4)`|
|                          |12    |`{python} round(resultados_3_gpt['mape'], 4)`|`{python} round(resultados_3_gpt['score'], 4)`|
: Métricas de evaluación para los ajustes con TimeGPT. {#tbl-metricasTimegpt}

Los pronósticos con TimeGPT sobre las series de temperatura y atenciones fueron regulares, en ninguno de los 2 casos pudo detectar los patrones estacionales de las series. Por otro lado, si pudo pronosticar correctamente las observaciones futuras en la serie de trabajadores registrados. Los problemas de pronóstico de TimeGPT parecen estar en el corto y largo plazo, ya que en la tabla @tbl-metricasTimegpt se puede ver como para los pronósticos a medio plazo las métricas de evaluación son menores que para corto y largo plazo.

Chronos al ser ser una familia de modelos cuenta con múltiples opciones para realizar pronósticos *zero-shot*. En esta tesina se utilizó el modelo `bolt-small`, el cual cuenta con 48 millones de parámetros.

```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_chronos['pred'], pred_color = '#6BC78A', line_color='black', label = 'Chronos', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/chronos1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_chronos['pred'], pred_color = '#5299CB', line_color='black', label = 'Chronos', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/chronos2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_chronos['pred'], pred_color = '#E44E58', line_color='black', label = 'Chronos', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/chronos3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronchronos layout-nrow=3}

![](../Imgs/plotnine/chronos1.png)

![](../Imgs/plotnine/chronos2.png)

![](../Imgs/plotnine/chronos3.png)

Pronósticos con Chronos.

:::


| Serie                    | Hor. | MAPE  | _Interval Score_ |
|--------------------------|------|-------|------------------|
|                          |3     |`{python} round(resultados_1_chronos3['mape'], 4)`|`{python} round(resultados_1_chronos3['score'], 4)`|
|Atenciones                |6     |`{python} round(resultados_1_chronos6['mape'], 4)`|`{python} round(resultados_1_chronos6['score'], 4)`|
|                          |12    |`{python} round(resultados_1_chronos['mape'], 4)`|`{python} round(resultados_1_chronos['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_2_chronos3['mape'], 4)`|`{python} round(resultados_2_chronos3['score'], 4)`|
|Trabajadores                |6     |`{python} round(resultados_2_chronos6['mape'], 4)`|`{python} round(resultados_2_chronos6['score'], 4)`|
|                          |12    |`{python} round(resultados_2_chronos['mape'], 4)`|`{python} round(resultados_2_chronos['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_3_chronos6['mape'], 4)`|`{python} round(resultados_3_chronos6['score'], 4)`|
|Temperatura                |6     |`{python} round(resultados_3_chronos12['mape'], 4)`|`{python} round(resultados_3_chronos12['score'], 4)`|
|                          |12    |`{python} round(resultados_3_chronos['mape'], 4)`|`{python} round(resultados_3_chronos['score'], 4)`|
: Métricas de evaluación para los ajustes con Chronos. {#tbl-metricasChronos}

Chronos pudo detectar correctamente los cambios de temperatura en las 24 horas, pero por otro lado, hace un pésimo trabajo intentando pronosticar la serie de atenciones. En las dos series Chronos tiene un comportamiento inverso, mientras que en la serie de atenciones a medida que se incrementa el horizonte las métricas empeoran, para la serie de temperaturas mejoran. También se puede ver como el pronóstico sobre la serie de trabajadores es peor que con otros modelos.

## 4.3 Comparación de resultados y análisis final

Para las comparaciones en la primera serie se utiliza el modelo arima que mejor ajusto, este es el 2do modelo manual

**Serie 1**

```{python}
metricas_1
```

**Serie 2**

```{python}
metricas_2
```

**Serie 3**

```{python}
metricas_3
```

\newpage

# 5. Conclusiones	

A medida que se avanzó por el documento, las diferencias, las ventajas y desventajas de cada método se fueron evidenciando. Con ARIMA el ajuste fue completamente manual, tomó su requerido tiempo y aplicarlo a otras series implicaría empezar de cero. Luego, en los algoritmos de aprendizaje automático se tuvieron que definir una serie de características para ayudar al modelo a ajustarse automáticamente a los datos, si se deseara pronosticar otra serie, sería prudente cambiar las características a otras más acordes a los datos. Con LSTM simplemente se tuvo que definir la forma del modelo, y tanto con TimeGPT como con Chronos solo fue necesario brindarles los datos y su periocidad. En estos últimos 3 modelos ningún cambio es necesario para pronosticar otra serie.

En este documento se presentan 3 contribuciones claves para el campo de la estadística. En primer lugar y como objetivo principal de esta tesina, la introducción de los modelos transformadores para el pronóstico de series temporales. Además se incluyeron aportes como evaluar el desempeño de los modelos con una nueva medida del error, el *interval score*, el cúal no tiene en cuenta únicamente el pronóstico puntual sino también probabilístico. El último gran aporte es la construcción de intervalos de pronóstico por medio de *conformal predictions*, que no depende de conocer la distribución de los residuos.

\newpage

# 6. Mejoras y extenciones a la investigación

En esta tesina se buscó abordar el tema de la forma más amplia posible sin sacrificar profundidad. Sin embargo, por la amplitud del mismo se dejaron fuera muchos temas interesantes que se podrían tratar en otros trabajos.

En primer lugar, se podrían estudiar numerosas otras series con distintas características y aplicar las respectivas correcciones a aquellas series acotadas, como el caso de la cantidad de atenciones en guardia que no puede ser negativa.

En la selección de los modelos se eligió como mejor aquel que minimizara el MAPE, pero se podría haber hecho la elección en base al *Interval Score* o alguna otra medida de error. También se podría indagar sobre múltiples medidas de error probabilísticas diferentes al *Interval Score*, tales como *Scaled quantile loss*, *Weighted quantile loss* o *Implicit quantile loss*.

*Boosting* y el ensamblaje de modelos no esta limitado únicamente a los árboles de decisión, y se podría explorar como funcionan estas técnicas en otros modelos, como en redes neuronales.

Para obtener pronósticos probabilísticos en los algoritmos de aprendizaje automático se optó por EnbPI, pero queda propuesto probar otros métodos o alternativas, como *Natural Gradient Boosting* (NGBoost).

Otra expansión a la investigación se puede dar en el ajuste de hiperparámetros, y características en el caso de los algoritmos de aprendizaje automatizado. Si bien con la búsqueda de parámetros se intentó explorar múltiples opciones, por cuestiones de tiempo y exigencia computacional es imposible explorarlas todas, es por esto que aún queda un amplio campo de investigación en este aspecto. A su vez, en los modelos fundacionales también es posible indagar sobre el ajuste fino.

\newpage

# 7. Bibliografía

**Alammar, J.** (27 de junio de 2018). *The Illustrated Transformer*. Jay Alammar. [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)

**Ansari et al.** (2024). *Chronos: Learning the Language of Time Series*. Transactions on Machine Learning Research. [https://arxiv.org/abs/2403.07815](https://arxiv.org/abs/2403.07815)

**Awan, A.** (2 de septiembre de 2024). *Time Series Forecasting With TimeGPT*. Datacamp. [https://www.datacamp.com/tutorial/time-series-forecasting-with-time-gpt](https://www.datacamp.com/tutorial/time-series-forecasting-with-time-gpt)

**Bermejo, J.** (21 de mayo de 2024). *Redes neuronales*. Facultad de Ciencias Económicas y Estadística de la Universidad Nacional de Rosario.

**Chen, Y., Yao, X.** (2023). *Conformal prediction for time series*. Proceedings of Machine Learning Research. [https://arxiv.org/abs/2010.09107](https://arxiv.org/abs/2010.09107)

**Elhariri, K.** (1 de marzo de 2022). *The Transformer Model*. Medium. [https://medium.com/data-science/attention-is-all-you-need-e498378552f9](https://medium.com/data-science/attention-is-all-you-need-e498378552f9)

**GeeksforGeeks**. (s.f.). *What is LSTM – Long short term memory?*. Recuperado el 15 de julio de 2025 de [https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/](https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/)

**Gilliland, M., Sglavo, U., & Tashman, L.** (2016). *Forecast Error Measures: Critical Review and Practical Recommendations*. John Wiley & Sons Inc.

**Gneiting, T., & Raftery A. E.** (2007). *Strictly Proper Scoring Rules, Prediction, and Estimation*. Journal of the American Statistical Association, 102(477), 359–378. [https://doi.org/10.1198/016214506000001437](https://doi.org/10.1198/016214506000001437)

**Hyndman, R. J., & Athanasopoulos, G.** (2021). *Forecasting: principles and practice (3rd ed.).*
OTexts. [https://otexts.com/fpp3/](https://otexts.com/fpp3/)

**Hyndman, R.J., Athanasopoulos, G., Garza, A., Challu, C., Mergenthaler, M., & Olivares, K.G.** (2024). *Forecasting: Principles and Practice, the Pythonic Way*. OTexts. [OTexts.com/fpppy](OTexts.com/fpppy).

**IBM**. (s.f.). *Explainers*. Recuperado el 14 de marzo de 2025 de [https://www.ibm.com/think/topics](https://www.ibm.com/think/topics)

**Kamtziris, G.** (27 de febrero de 2023). *Time Series Forecasting with XGBoost and LightGBM: Predicting Energy Consumption.* Medium. [https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-460b675a9cee](https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-460b675a9cee)

**Korstanje, J.** (2021). A*dvanced Forecasting with Python*. Apress.

**Nielsen, A.** (2019). *Practical Time Series Analysis: Prediction with Statistics and Machine Learning*. O'Reilly Media.

**Nixtla**. (s.f.). *About TimeGPT*. Recuperado en diciembre de 2024 de [https://docs.nixtla.io/docs/getting-started-about_timegpt](https://docs.nixtla.io/docs/getting-started-about_timegpt)

**Parmezan, A., Souza, V., & Batista, G.** (1 de mayo de 2019). *Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model*. Information Sciences. [https://www.sciencedirect.com/science/article/abs/pii/S0020025519300945](https://www.sciencedirect.com/science/article/abs/pii/S0020025519300945)

**Prunello, M., & Marfetán, D.** (12 de mayo de 2024). *Árboles de decisión*. Facultad de Ciencias Económicas y Estadística de la Universidad Nacional de Rosario.

**Sabino Parmezan, A. R., Souza, V. M. A., & Batista, G. E. A. P. A.** (2019). *Evaluation of statistical and machine learning models for time series prediction: Identifying the state‑of‑the‑art and the best conditions for the use of each model*. Information Sciences, 484, 302–337. [https://doi.org/10.1016/j.ins.2019.01.076](https://doi.org/10.1016/j.ins.2019.01.076)

**Sanderson, G.** [3Blue1Brown]. (2024). *Attention in transformers, step-by-step | DL6 [Video]*. Youtube. [https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1204s](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1204s)

**Sanderson, G.** [3Blue1Brown]. (2024). *Transformers (how LLMs work) explained visually | DL5 [Video]*. Youtube. [https://www.youtube.com/watch?v=wjZofJX0v4M](https://www.youtube.com/watch?v=wjZofJX0v4M)

**Shastri, Y.** (26 de abril de 2024). *Attention Mechanism in LLMs: An Intuitive Explanation*. Datacamp. [https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition](https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition)

**Silberstein, E.** (7 de noviembre de 2024). *Tracing the Transformer in Diagrams*. Medium. [https://medium.com/data-science/tracing-the-transformer-in-diagrams-95dbeb68160c](https://medium.com/data-science/tracing-the-transformer-in-diagrams-95dbeb68160c)

**Valeriy, M.** (11 de agosto de 2023). *Demystifying EnbPI: Mastering Conformal Prediction Forecasting*. Medium. [https://valeman.medium.com/demystifying-enbpi-mastering-conformal-prediction-forecasting-d49e65532416](https://valeman.medium.com/demystifying-enbpi-mastering-conformal-prediction-forecasting-d49e65532416)

**Vaswani et al.** (2017). *Attention is all you need*. Google. [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)


\newpage

# 8. Anexo

## 8.1 Gráficos estacionales

```{python}
#| label: fig-atenciones_estacionalidad
#| fig-cap: Atenciones por guardia mensuales por patologías respiratorias por año
#| fig-height: 2

atenciones_trunc['mes'] = atenciones_trunc['ds'].dt.month
atenciones_trunc['Año'] = atenciones_trunc['ds'].dt.year

(
  ggplot(atenciones_trunc) +
  aes(y = "y", x = "mes", group = "Año", color = "factor(Año)") +
  geom_point(size = 0.3) +
  geom_line() +
  scale_x_continuous(breaks = range(1,13), limits = (1,12), labels=[
        "Ene", "Feb", "Mar", "Abr", "May", "Jun", "Jul", "Ago", "Sep", "Oct", "Nov", "Dic"
    ]) +
  labs(color = "Año", x = "Mes", y = "Atenciones") +
  theme(figure_size=(6, 3))
)
```

```{python}
#| label: fig-temp_estacional
#| fig-cap: Atenciones por guardia mensuales por patologías respiratorias por año
#| fig-height: 2

temperatura_trunc['Hora'] = temperatura_trunc['ds'].dt.hour
temperatura_trunc['Día'] = temperatura_trunc['ds'].dt.day

(
  ggplot(temperatura_trunc) +
  aes(y = "y", x = "Hora", group = "Día", color = "Día") +
  geom_point(size = 0.3) +
  geom_line() + 
  labs(y = "Temperatura (Cº)", color = "Día") +
  scale_x_continuous(breaks = range(0,24)) +
  theme(legend_position= "bottom", figure_size=(6, 3)
  )
)
```

## 8.2 Salidas de modelos arima {#salidas_arima}

**Atenciones modelo 1**

```{python}
print(resultados_arima['salida_arima_atenciones_1'])
```

**Atenciones modelo 2**

```{python}
print(resultados_arima['salida_arima_atenciones_2'])
```

**Atenciones modelo selección automática**

```{python}
print(resultados_arima['salida_arima_atenciones_auto'])
```

**Trabajadores modelo 1**

```{python}
print(resultados_arima['salida_arima_trabajadores_1'])
```

**Trabajadores modelo selección automática**

```{python}
print(resultados_arima['salida_arima_trabajadores_auto'])
```

**Temperatura modelo selección automática**

```{python}
print(resultados_arima['salida_arima_temperatura_auto'])
```

**Temperatura modelo 1**

```{python}
print(resultados_arima['salida_arima_temperatura_1'])
```

**Temperatura modelo 2**

```{python}
print(resultados_arima['salida_arima_temperatura_2'])
```

**Temperatura modelo 3**

```{python}
print(resultados_arima['salida_arima_temperatura_3'])
```

## 8.3 Comprobación de supuestos de modelos arima {#supuestos_arima}

**Modelo atenciones selección automática**

```{python}
resid_check(resultados_arima['resid_arima_atenciones_2'], ds = atenciones_trunc['ds'], time='%Y', name='resid_atenciones_auto')
```

::: {#fig-pronarima2 layout-nrow=2}

![](../Imgs/plotnine/resid_atenciones_auto_1.png)

![](../Imgs/plotnine/resid_atenciones_auto_2.png)

![](../Imgs/plotnine/resid_atenciones_auto_3.png)

![](../Imgs/plotnine/resid_atenciones_auto_4.png)

Comprobación de supuestos del modelo arima automático para la serie de atenciones
:::

**Modelo trabajadores 1**

```{python}
resid_check(resultados_arima['resid_arima_trabajadores_1'], ds = trabajadores_trunc['ds'], time='%Y', name='resid_trabajadores_1')
```

::: {#fig-pronarima2 layout-nrow=2}

![](../Imgs/plotnine/resid_trabajadores_1_1.png)

![](../Imgs/plotnine/resid_trabajadores_1_2.png)

![](../Imgs/plotnine/resid_trabajadores_1_3.png)

![](../Imgs/plotnine/resid_trabajadores_1_4.png)

Comprobación de supuestos del modelo arima manual para la serie de trabajadores
:::


**Modelo trabajadores selección automática**

```{python}
resid_check(resultados_arima['resid_arima_trabajadores_auto'], ds = trabajadores_trunc['ds'], time='%Y', name='resid_trabajadores_auto')
```

::: {#fig-pronarima2 layout-nrow=2}

![](../Imgs/plotnine/resid_trabajadores_auto_1.png)

![](../Imgs/plotnine/resid_trabajadores_auto_2.png)

![](../Imgs/plotnine/resid_trabajadores_auto_3.png)

![](../Imgs/plotnine/resid_trabajadores_auto_4.png)

Comprobación de supuestos del modelo arima automático para la serie de trabajadores
:::

**Modelo Temperatura 3**

```{python}
resid_check(resultados_arima['resid_arima_temperatura_3'], ds = temperatura_trunc['ds'], time='%d', name='resid_temperatura_3')
```

::: {#fig-pronarima2 layout-nrow=2}

![](../Imgs/plotnine/resid_temperatura_3_1.png)

![](../Imgs/plotnine/resid_temperatura_3_2.png)

![](../Imgs/plotnine/resid_temperatura_3_3.png)

![](../Imgs/plotnine/resid_temperatura_3_4.png)

Comprobación de supuestos del tercer modelo arima manual para la serie de temperatura
:::


# NOTAS
   
- TO DO
      Redactar conclusiones de tablas
      controlar seleccion de hiperparametros
      Redactar 
      Revisar Arima
      contar como funcionan las autocorrelaciones y parciales para la seleccion de modelos arima
      revisar transformers
      revisar medidas de evaluacion
      Eliminar la instalacion de neuralforecast
      Volver a correr toda la aplicacion y verificar resultados
      verificar luego de la aplicacion cuales son las librerias que termino usando
      Verificar, actualizar y completar las descripciones de las funciones
      Actualizar los requirements


- Guardar las versiones de librerias, software y hardwate en la que se corre el codigo

- Hacer equivalencia del documento en GitBook

nomenclatura: 
  - conjunto de observaciones $z_1, ..., z_t, ..., z_n$
  - pronostico $z_{n+1}, ..., z_{n+l}, ...   , z_{n+h}$

