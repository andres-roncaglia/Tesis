---
format: 
  pdf:
    papersize: A4
engine: python
metadata:
  quarto-python:
    python: ".venv/Scripts/python.exe"
fig-pos: H
toc: false
lang: es
echo: False
warning: False
message: False
geometry: 
  - top= 25.4mm
  - left= 25.4mm
  - right = 25.4mm
  - bottom = 25.4mm
header-includes: 
  - \usepackage{multirow}
  - \usepackage{float}
---

```{python}
# LIBRERIAS

# Para graficar
import matplotlib.pyplot as plt
import seaborn as sns

# Para mostrar los resultados
from Funciones import plot_forecast, load_env, autocorr_plot, resid_check, interval_score

# Para calcular MAPE
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

# Para la transformacion de box y cox
from scipy import stats

```


\newgeometry{margin=0mm}

\pagenumbering{gobble}
 
\thispagestyle{empty}

\begin{figure}[htbp]
    \noindent
    \includegraphics[width=\paperwidth, height=\paperheight]{../Imgs/Portada_tesina.png}
\end{figure}


\newpage

\newpage


\restoregeometry


\section*{Agradecimientos}

\newpage

\section*{Resumen}
Palabras clave: series temporales, predicción, ARIMA, aprendizaje automático, redes neuronales, transformers, TimeGPT. 

\newpage
\renewcommand{\contentsname}{Índice}
\tableofcontents
\newpage
\pagenumbering{arabic}

# 1. Introducción


La predicción de valores futuros en series de tiempo es una herramienta clave en múltiples ámbitos, tales como la economía, el comercio, la salud, la energía y el medio ambiente. En estos contextos, anticipar el comportamiento de una variable permite mejorar la planificación, asignar recursos de forma más eficiente y reducir la incertidumbre.

Actualmente, la ciencia de datos se encuentra en una etapa de constante expansión e innovación, impulsada por la gran cantidad de datos generados diariamente, por lo que en un contexto creciente de complejidad y exigencia temporal, resulta conveniente contar con herramientas que faciliten y acorten los tiempos de trabajo. Si bien los métodos más conocidos para trabajar series de tiempo son precisos, los modelos tradicionales como ARIMA son difíciles de automatizar y requieren de amplios conocimientos para encontrar un buen ajuste, mientras que los algoritmos de aprendizaje automatizado que se utilizan actualmente pueden demandar largos tiempos de entrenamiento y un gran coste computacional. Frente a estas limitaciones, han surgido recientemente modelos capaces de seleccionar de forma automática el mejor ajuste para una serie temporal dada, sin requerir entrenamiento previo ni conocimientos especializados en análisis de series de tiempo. Estos son los denominados modelos fundacionales preentrenados, tales como TimeGPT o Chronos.

Sin embargo, aún persisten interrogantes sobre el desempeño de estos nuevos modelos y la falta de acceso al código fuente de algunos de estos limita la posibilidad de auditar sus resultados o replicar su implementación. Por ello, esta tesina propone realizar una comparación sistemática de modelos de pronóstico para series de tiempo, abordando tres enfoques metodológicos: modelos estadísticos tradicionales, algoritmos de *machine learning* y modelos de aprendizaje profundo. El objetivo es evaluar su desempeño en distintos contextos, utilizando métricas como el porcentaje del error absoluto medio (MAPE) y el *Interval Score*, con el fin de analizar ventajas, limitaciones y potenciales usos de cada uno.

Este análisis busca aportar una mirada crítica e informada sobre el uso de nuevas tecnologías en la predicción de series de tiempo, contribuyendo a la toma de decisiones metodológicas más sólidas desde una perspectiva estadística.

\newpage

# 2. Objetivos

## 2.1 Objetivo general

El objetivo de esta tesina es, en primer lugar, comparar la precisión, eficiencia y facilidad de pronosticar series de tiempo con distintos modelos, incluyendo enfoques estadísticos clásicos, algoritmos de *machine learning* y modelos de *deep learning*, analizando al mismo tiempo sus ventajas, limitaciones y condiciones de uso más apropiadas.

## 2.2 Objetivos específicos

- Implementar modelos clásicos de series de tiempo, como ARIMA y SARIMA, explicando y garantizando el cumplimiento de los fundamentos teóricos y supuestos que los sostienen.

- Aplicar modelos de aprendizaje automático supervisado, como XGBoost y LightGBM, explorando distintas configuraciones para garantizar el mejor ajuste.

- Desarrollar modelos de aprendizaje profundo, en particular redes LSTM, dando introducción a las redes neuronales y modelos de pronóstico más complejos.

- Realizar pronósticos con modelos fundacionales(TimeGPT,Chronos) y comprender su funcionamiento.

- Definir y aplicar métricas de evaluación (MAPE, *Interval Score*) para comparar el rendimiento de todos los modelos bajo un mismo conjunto de datos.

- Reflexionar valorativamente sobre los criterios de selección de modelos en función del contexto de aplicación, la complejidad computacional y la interpretabilidad de los resultados.

\newpage

# 3. Metodología {#metodologia}

El enfoque metodológico adoptado en esta tesina consiste en comparar el desempeño de distintos modelos de pronóstico aplicados a series temporales. Para ello, se seleccionan modelos representativos de tres enfoques principales: modelos estadísticos tradicionales, algoritmos de aprendizaje automático (*machine learning*) y modelos de aprendizaje profundo (*deep learning*).

El análisis se estructura en tres componentes fundamentales: una descripción conceptual de los modelos, su implementación práctica sobre series con diferentes características, y una evaluación cuantitativa comparativa a través de métricas de error.

## 3.1 Conceptos básicos de series de tiempo

Se denomina serie de tiempo a un conjunto de observaciones $\{z_1, z_2, ..., z_t, ..., z_n\}$ cuantitativas ordenadas en el tiempo, usualmente de forma equidistante, sobre una variable de interés. El análisis de series de tiempo tiene como objetivo sintetizar y extraer información estadística relevante, tanto para interpretar el comportamiento histórico de la variable como para generar pronósticos $\{ z_{n+1}, ...,z_{n+l}, ...., z_{n+h} \}$.

Dado que las series temporales pueden exhibir diversos patrones subyacentes, resulta útil descomponerlas en componentes separadas, cada una de las cuales representa una característica estructural específica del comportamiento de la serie.

- Estacionalidad: corresponde a las fluctuaciones periódicas que se repiten a intervalos regulares de tiempo. Un ejemplo típico es la temperatura, que tiende a disminuir en invierno y aumentar en verano, repitiendo este patrón anualmente.

- Tendencia (o tendencia-ciclo): refleja la evolución a largo plazo de la media de la serie, asociada a procesos de crecimiento o decrecimiento sostenido. Por ejemplo, la población mundial exhibe una tendencia creciente a lo largo del tiempo.

- Residuos: representa las variaciones no sistemáticas que no pueden ser explicadas por la tendencia ni la estacionalidad. Estas fluctuaciones, que suelen deberse a eventos impredecibles o factores exógenos, se asumen como aleatorias.

## 3.2 Modelos estadísticos tradicionales

Son llamados modelos tradicionales a aquellos que surgen antes del auge del *machine learning* y los modelos de aprendizaje profundo. Son caracterizados por sus fuertes fundamentos estadísticos y su capacidad en capturar dependencias temporales en los datos.

### 3.2.1 SARIMA

Se dice que una serie es débilmente estacionaria si la media y la variancia se mantienen constantes en el tiempo y la correlación entre distintas observaciones solo depende de la distancia en el tiempo entre estas. Por comodidad, cuando se mencione estacionariedad se estará haciendo referencia al cumplimiento de estas propiedades. 

Se denomina función de autocorrelación a la función de los rezagos, entendiendo por rezago a la distancia ordinal entre dos observaciones, que grafica la autocorrelación entre pares de observaciones. Es decir que para cada valor $k$ se tiene la correlación entre todos los pares de observaciones a $k$ observaciones de distancia. En su lugar, la función de autocorrelación parcial calcula la correlación condicional de los pares de observaciones, removiendo la dependencia lineal de estas observaciones con las que se encuentran entre estas.

Los modelos $ARIMA$ (*AutoRegresive Integrated Moving Average*) son unos de los modelos de pronostico tradicionales mejor establecidos. Son una generalización de los modelos autoregresivos (AR), que suponen que las observaciones futuras son función de las observaciones pasadas, y los modelos promedio móvil (MA), que pronostican las observaciones como funciones de los errores de observaciones pasadas. Además, estos modelos pueden adaptarse a series no estacionarias mediante la aplicación de diferenciaciones de orden $d$, las cuales implican restar a cada observación el valor registrado $d$ periodos anteriores.

Formalmente un modelo $ARIMA(p,d,q)$ se define como:

$$
\psi_p(B)(1-B)^dz_t = \theta_0 + \theta_q(B)\alpha_t
$${#eq-1}

Donde $z_t$ es la observación $t$-ésima, $\psi_p(B)$ y $\theta_q(B)$ son funciones de los rezagos ($B$), correspondientes a la parte autoregresiva y promedio móvil respectivamente, $d$ es el grado de diferenciación y $\alpha_t$ es el error de la $t$-ésima observación.

Se debe tener en cuenta estos aspectos importantes:

- Se dice que una serie es invertible si se puede escribir cada observación como una función de las observaciones pasadas más un error aleatorio. Por definición, todo modelo AR es invertible.

- Por definición, todo modelo MA es estacionario.

- $\psi_p(B) = 1 - \psi_1 B - \psi_2 B_2 - ... - \psi_p B^p$ es el polinomio característico de la componente AR y $\theta_q(B) = 1 - \theta_1 B - \theta_2 B^2 - ... - \theta_p B^q$ de la componente MA. Si las raíces de los polinomios característicos caen fuera del círculo unitario, entonces un proceso AR se puede escribir de forma MA y es estacionario, y a su vez un proceso MA se puede escribir de forma AR y es invertible.

- Un proceso $ARIMA$ es estacionario e invertible si su componente AR y MA lo son respectivamente.

Sin embargo este tipo de modelos no tienen en cuenta la posible estacionalidad que puede tener una serie, es por esto que se introducen los modelos $SARIMA(p,d,q)(P,D,Q)_s$ que agregan componentes AR, MA y diferenciaciones a la parte estacional de la serie con período $s$.

Un buen modelo $SARIMA$ debe cumplir las siguientes propiedades:

- Sus residuos se comportan como ruido blanco, es decir, están incorrelacionados y siguen una distribución normal, con media y variancia constantes.

- Es admisible, es decir que es invertible y estacionario.

- Es parsimonioso, en el sentido de que sus parámetros son significativos.

- Es estable en los parámetros, que se cumple cuando las correlaciones entre los parámetros no son altas.

## 3.3 Algoritmos de aprendizaje automático

El aprendizaje automático (machine learning) es una rama de la inteligencia artificial que permite a las computadoras aprender de los datos y realizar tareas de forma autónoma. Aunque los métodos presentados no fueron diseñados específicamente para datos temporales, han demostrado ser útiles en múltiples contextos mediante diversas pruebas empíricas.

Los métodos de *machine learning*, a diferencia de los modelos tradicionales, se enfocan principalmente en identificar los patrones que describen el comportamiento del proceso que sean relevantes para pronosticar la variable de interés, y no se componen de reglas ni supuestos que tengan que seguir. Para la identificación de patrones, estos modelos requieren la generación de características. 

### 3.3.1 Introducción a árboles de decisión y ensamblado

Los árboles de decisión pueden ser explicados sencillamente como un conjunto extenso de estructuras condicionales *if-else*. El modelo pronosticará un cierto valor $x$ si una cierta condición es verdadera, u otro valor $y$ si es falsa. Es importante ver que no hay una tendencia lineal en este tipo de lógica, por lo que los árboles de decisión pueden ajustar tendencias no lineales. El resultado que se obtiene al aplicar esta técnica puede resumirse gráficamente como un camino que toma diferentes bifurcaciones (o un tronco con diferentes ramas), y de esta característica surge su nombre.

Un árbol puede tener distinta cantidad de divisiones en un mismo nivel, llamadas hojas, y profundidad, las cuales determinan en qué medida el modelo se ajusta a los datos con los que se entrena. Lógicamente árboles más profundos y con más hojas suelen generar sobreajuste, es decir, un modelo que se adapta demasiado a los datos de entrenamiento y generaliza mal.

![Ejemplo de árbol de decisión](../Imgs/arbol_decision.png)


Los métodos de ensamblaje combinan la predicción de varios estimadores base con el objetivo de mejorar la robustez de la predicción. Existen numerosos métodos de ensamblaje entre los cúales se encuentran los bosques de decisión y los árboles potenciados por gradiente (del inglés *Gradient-boosted trees*). 

*Boosting* es un proceso iterativo, que consiste en la construcción de árboles de forma secuencial donde cada nuevo árbol busca predecir los residuos de los árboles anteriores. Es así entonces que el primer árbol buscará predecir los valores futuros de la serie, mientras que el segundo intentará predecir los valores reales menos los pronosticados por el primer árbol, el tercero tratará de inferir la diferencia entre los valores reales y el valor pronosticado del primer árbol menos los errores del segundo, y así sucesivamente. En cada iteración se pesan los puntos y se corrigen aquellos que tengan un mayor error por medio del descenso del gradiente. 

![Proceso de ensamblado](../Imgs/Ensamblaje.png)

La dirección de máximo crecimiento para una función está determinada por su gradiente, y del mismo modo, la dirección contraria a este gradiente es la dirección de máximo decrecimiento. De este modo, el descenso del gradiente busca encontrar los valores más bajos para una función de pérdida. El algoritmo de descenso de gradiente propone calcular el gradiente de la función de costo bajo el valor actual de parámetros, para luego modificarlo moviéndose en la dirección de mayor descenso. Este resultado se basa en derivadas, tasas de cambio instantáneo, por lo que conviene desplazarse una magnitud pequeña $\eta$, llamada “tasa de aprendizaje”. Es un algoritmo iterativo, en el que en cada paso la regla de actualización consiste en calcular la derivada de la función de costo respecto a cada parámetro y desplazarse una cierta magnitud $\eta$ en la dirección contraria. Esto se repite un cierto número de veces hasta alcanzar la convergencia.

![Ejemplo del descenso del gradiente en una función de pérdida](../Imgs/desc_gradiente.png)

Sin embargo, los modelos no se construyen infinitamente, sino que se busca minimizar una función de pérdida que incluye una penalización por la complejidad del modelo, limitando así la cantidad de árboles que se producen. Existen múltiples métodos de ensamblaje (XGBoost, LightGBM, CatBoost, AdaBoost, entre otros) que se diferencian en la forma en la que se construyen los árboles. En esta tesina se usarán los algoritmos *eXtreme Gradient Boosting* (XGBoost) y *Light Gradient-Boosting Machine* (LightGBM).

### 3.3.2 Diferencias entre XGBoost y LightGBM

Las diferencias entre XGBoost y LightGBM radican en la forma en que cada uno identifica las mejores divisiones dentro de los árboles y de que forma los hacen crecer.

Mientras que XGBoost usa un método en el que se construyen histogramas para cada una de las características generadas para elegir la mejor división por característica, LightGBM usa un método más eficiente llamado *Gradient-Based One-Side Sample* (GOSS). GOSS calcula los gradientes para cada punto y lo usa para filtrar afuera aquellos puntos que tengan un bajo gradiente, ya que esto significaría que estos están mejor pronosticados que el resto y no es necesario enfocarse tanto en ellos. Además, LightGBM utiliza un procedimiento que acelera el ajuste cuando se tienen muchas características correlacionadas de las cuales elegir. 

A la hora de hacer crecer los árboles, XGBoost lo hace nivel a nivel, es decir que primero se crean todas las divisiones de un nivel, y luego se pasa al siguiente, priorizando que el árbol sea simétrico y tenga la misma profundidad en todas sus ramas. LigthGBM, en cambio, se expande a partir de la hoja que más reduce el error, mejorando la precisión y eficiencia en series largas, pero arriesgándose a posibles sobreajustes si no se limita correctamente la profundidad de los árboles.

### 3.3.3 Intervalos de confianza en algoritomos de aprendizaje automático

No depender del cumplimiento de los supuestos distribucionales de los errores es una gran ventaja, pero conlleva la consecuencia de no poder obtener pronósticos probabilísticos. Para obtener intervalos de confianza que acompañen los pronósticos puntuales existen varias alternativas. 

La regresión cuantil con *boosting* consiste en forzar al modelo a pronosticar los cuantiles deseados en lugar de la media. Sin embargo, esta opción no es fácilmente aplicable a XGBoost.

Una alternativa que recientemente gana popularidad es *conformal predictions*, que es una familia de métodos no paramétricos para construir intervalos de predicción. En general, esta familia de métodos requieren el cumplimiento del supuesto de intercambiabilidad, es decir, que las observaciones no deben ser dependientes del tiempo, algo que no se cumple en series de tiempo. Sin embargo, *Ensemble Batch Prediction Intervals* (EnbPI) es un método de conformal prediction diseñado específicamente para series de tiempo que no requiere este supuesto.

Para aplicar EnmPI primero se debe elegir un estimador por ensamblado y formar $B$ muestras con *Bootstrap* por bloques. Esto último consiste en dividir el conjunto de datos de entrenamiento en bloques y formar un nuevo conjunto remuestreando sobre estos mismos bloques para mantener la correlación de los datos. Se ajustan $B$ modelos, uno para cada muestra bootstrap. Para cada punto en el conjunto de entrenamiento se calcula el residuo usando únicamente aquellos estimadores que no contenían al punto en cuestión. Se generan los pronósticos puntuales usando la media de los pronósticos de los $B$ modelos. Los intervalos de confianza se consiguen sumando a los pronósticos puntuales los cuantiles de la distribución de los residuos.

$$
IC_{Z_{n+l}; 1-\alpha}= Z_{n}(l) \pm Q_{1-\alpha}(e)
$${#eq-2}

Este último método será el que se utilizará para calcular los intervalos de confianza en los algoritmos de aprendizaje automático. 

## 3.4 Modelos de aprendizaje profundo

El *deep learning* (aprendizaje profundo) es una rama del *machine learning* que tiene como base un conjunto de algoritmos que intentan modelar niveles altos de abstracción en los datos usando múltiples capas de procesamiento, con complejas estructuras o compuestas de varias transformaciones no lineales. 

Entre estos algoritmos se encuentran las redes neuronales, que imitan el funcionamiento del cerebro humano usando procesos que simulan la forma biológica en la que trabajan las neuronas para identificar fenómenos, evaluar opciones y llegar a conclusiones. 

### 3.4.1 Introducción a redes neuronales

Una red neuronal esta compuesta en grandes rasgos de 3 capas: entrada, oculta y salida. Dentro de cada capa se pueden encontrar neuronas y conexiones entre estas, donde cada neurona representa una variable y cada conexión un peso, y es por esto que a estas conexiones las llamaremos así en adelante. La suma de los pesos y las neuronas que no formen parte de la capa de entrada dan el total de parámetros que tiene que ajustar el modelo.

![Red neuronal completamente conectada](../Imgs/red_conectada.png){#fig-redneuro}

En la capa de entrada se introducen las variables explicativas, y luego cada neurona fuera de esta capa es una función de las neuronas anteriores conectadas a la misma. Estas funciones son llamadas funciones de activación.

![Red neuronal completamente conectada](../Imgs/conexiones_redneuro.png){#fig-conexiones}

Los parámetros se estiman buscando minimizar una función de costo, esto se logra con el descenso del gradiente y por medio de retropropagación. La retropropagación consiste en realizar una estimación inicial de la variable respuesta con los valores iniciales de la red neuronal, que pueden estar dados por ejemplo por una distribución normal, y de manera inversa a la dirección de la red neuronal calcular derivadas para encontrar la dirección de máximo decrecimiento de la función de costo para cada parámetro en la red neuronal. 

Existen distintos tipos de redes neuronales según la forma en la que se conectan las neuronas. En esta tesina son de interés especialmente las *Convolutional Neural Networks* (CNN) y las *Recurrent Neural Networks* (RNN), en español, redes neuronales convolucionales y recurrentes respectivamente. Las primeras son útiles para el reconocimiento de patrones en los datos, mientras que las últimas son especialmente buenas en la predicción de datos secuenciales.


### 3.4.2 *Long Short Term Memory (LSTM)*

Lo que caracterizan a las redes neuronales recurrentes son los bucles de retroalimentación que se presentan en la @fig-rnn. Mientras que cada neurona de entrada en una red neuronal completamente conectada es independiente, en las redes neuronales recurrentes se relacionan entre ellas y se retroalimentan.

![Ejemplo de RNN](../Imgs/red_recurrente.png){#fig-rnn}


Un problema frecuente en las RNN es su dificultad para capturar dependencias de largo plazo. Esto puede tener 2 causas, el desvanecimiento o la explosión del gradiente. El desvanecimiento del gradiente ocurre cuando, iteración tras iteración, el gradiente se aproxima a cero y se estabiliza, evitando que la red siga aprendiendo. Por el contrario, cuando el gradiente crece exponencialmente se habla de una explosión, esto lleva a inestabilidades en el aprendizaje, haciendo que las actualizaciones de los parámetros sean erráticas e impredecibles.

Las redes neuronales con memoria a corto y largo plazo (LSTM) son un tipo de RNN que solucionan este problema mediante un algoritmo logístico de 3 puertas, y usando una neurona que guarda la información histórica necesaria para la red.

![Estructura *Long Short Term Memory*](../Imgs/lstm.png)

**Puerta de guardado**

La puerta de guardado se encarga de decidir si mantener o descartar la información actualmente guardada en la neurona de memoria de la red neuronal. Esta puerta recibe la entrada y el estado de la RNN, y las pasa como argumentos de una función sigmoide. 

$$
\sigma(x) =  \frac{1}{1+e^{-x}}
$${#eq-3}

$$
K_t = \sigma(W_k \times [S_{t-1}, x_t] + B_k)
$${#eq-4}

Si $K_t$ es igual a 1, significa que la información guardada debe ser mantenida perfectamente. Si $K_t$ fuera igual a 0, la información guardada debe ser descartada completamente.

Sean $S_{t-1}$ el estado actual de la RNN, $x_t$ la entrada actual, y $W_t$ y $B_t$ los pesos y sesgos de la puerta de guardado respectivamente:

$$
Old_t = K_t \times C_{t-1}
$${#eq-5}

Donde $C_{t-1}$ es la información guardada actualmente y $Old_t$ lo que se mantendrá para la próxima iteración de la red.

**Puerta de entrada**

La puerta de entrada controla que información añadir a la neurona de memoria. Sean $W_i$ y $B_i$ los pesos y sesgos de la puerta de guardado:

$$
I_t = \sigma(W_i\times[S_{t-1},x_t]+B_i)
$${#eq-6}

$$
New_t = I_t \times N_t
$${#eq-7}

Donde $N_t$ es el nuevo valor propuesto por la red neuronal y $New_t$ es la información que se va a agregar a la neurona de memoria. Luego, el nuevo valor de la neurona de memoria es:

$C_t = Old_t + New_t$

**Puerta de salida**

La puerta de salida se encarga de extraer la información más importante del estado actual de la neurona para usar como salida. Sean $W_o$ y $B_o$ los pesos y sesgos de la puerta de salida y $tanh(x)$ la función tangente:

$$
O_t = \sigma(W_o \times [S_{t-1},x_t] + B_o)
$${#eq-8}

$$
S_t = O_t \times tanh(C_t)
$${#eq-9}

Donde $S_t$ es el nuevo estado de la red neuronal.

Las 3 puertas son logísticas para que sea sencillo aplicar la retropropagación. Este sistema de puertas evita los problemas de desvanecimiento y explotación del gradiente, y evita que se acumulen muchos estados por largos períodos de tiempo eligiendo que información es relevante guardar.

### 3.4.3 Modelos transformadores

Otro tipo de modelo de aprendizaje profundo son los *transformer models* (modelos transformadores), los cúales son significativamente más eficientes al entrenar y realizar inferencias que las RNNs; gracias al uso de mecanismos de atención, presentados en la publicación '[*Attention is all you need*](https://arxiv.org/pdf/1706.03762)' de Google. La autoatención captura dependencias y relaciones en la secuencias de valores que se alimentan al modelo, logrando poner en contexto a cada observación. 

Los modelos transformadores fueron creados originalmente con el propósito de generar texto. Sin embargo, tanto TimeGPT como Chronos explotan esta tecnología para el pronóstico de series de tiempo. Ambos modelos son preentrenados, lo cúal significa que la optimización de parámetros y pesos fue realizada antes de usarse el modelo. Esto se logra entrenando y generalizando el modelo en un conjunto de datos extenso, por lo general de fuentes públicas. El preentrenamiento permite que el modelo adquiera conocimientos generales sobre la estructura y los patrones de los datos, los cuales luego pueden ser reutilizados en tareas concretas mediante técnicas como *fine-tuning* (ajuste fino). Los modelos preentrenados constituyen una gran innovación, lo que mejora la accesibilidad, precisión, eficiencia computacional y velocidad del pronóstico.

Dado que los modelos de lenguaje de texto utilizan diccionarios de *tokens*, que son segmentos de caracteres representados vectorialmente según ciertos parámetros, es necesario tokenizar los valores de la serie temporal. El diccionario de *tokens* con el que operan los modelos de lenguaje no es infinito, por lo tanto es necesario proyectar las observaciones a un set finito de *tokens*. Para cumplir esto, Chronos escala y discretiza las observaciones. TimeGTP por su parte usa las mismas observaciones como *tokens*, esto dado que, si bien su arquitectura es la de un modelo transformador, esta no está basada en ningún modelo de lenguaje existente, y en cambio trabaja con un modelo especializado en series de tiempo entrenado para minimizar el error de pronóstico.

Para el escalado se aplica a las observaciones una transformación del tipo $f(x_i) = (x_i-m)/s$. Existen variadas técnicas de escalado eligiendo apropiadamente $m$ y $s$, pero se opta por elegir $m=0$ y $s = \frac{1}{C}\sum^C_{i=i}|x_i|$ debido a que preserva los valores iguales a cero, los cúales pueden ser importante de destacar en numerosas aplicaciones.

Sin embargo estos valores siguen siendo números reales y no pueden ser procesados directamente por un modelo de lenguaje. Es por esto que se discretizan las observaciones. Se seleccionan $B$ centros de intervalos en la recta real, $c_1, c_2, ..., c_B$, y $B-1$ extremos $b_i$ que los separen, $c_i < b_i < c_{i+1}$ para $i \in \{1,...,B-1 \}$. Las funciones de discretización $q:\Re \rightarrow \{1,2,...,B\}$, y de descuantificación $d: \{1,2,...,B\} \rightarrow \Re$ se definen como:

$$
q(x)= \left \{ \begin{matrix} 1 \ \ \ \ \ \ \ \ \text{si} \ \ -\infty \leq x < b_1 \hfill \\ 
2 \ \ \ \ \ \ \ \ \text{si} \ \ b_1 \leq x < b_2 \hfill \\
 \ \vdots \hfill \\
B \ \ \ \ \ \ \ \ \text{si} \ \ b_{B-1} \leq x < \infty \hfill \end{matrix} \right. \hspace{2, cm} \text{y} \hspace{2, cm} d(j) = c_j
$${#eq-10}

Una vez se hayan transformado las observaciones para poder ser leídas por el modelo, el funcionamiento de un transformador es el siguiente: 

![Diagrama de la estructura del modelo transformador de TimeGPT](../Imgs/modelo_timegpt.png){#fig-modelo}

*Codificador*

1. Representación vectorial: Cada *token* es transformado en un vector (vector de entrada) con muchas dimensiones ($\vec E$). Las dimensiones corresponden a diferentes características que el modelo definió en el preentrenado con una numerosa cantidad de parámetros.

2. Codificación posicional: Un set de valores adicionales o vectores son añadidos a los vectores de entrada antes de alimentarlos al modelo ($\vec E \Leftarrow \vec E + \vec P$). Estas codificaciones posicionales tienen patrones específicos que agregan la información posicional del token.

3. Atención multi-cabezal (del inglés *Multi-Head Attention*): La autoatención opera en múltiples 'cabezales de atención' para capturar los diferentes tipos de relaciones entre tokens. Una cabeza de atención verifica el contexto en el que se presenta el token, y manipula los valores del vector que lo representa para añadir esta información contextual.

La verificación del contexto funciona gracias a una matriz denominada *Query* ($W_Q$) que examina ciertas características definidas con aterioridad en el preentrenado. El vector de entrada ($\vec E$) es multiplicado por esta matriz, resultando en un vector de consultas ($\vec Q$) para cada token. Una matriz de claves ($W_K$) que comprueba las relaciones con las características en la matriz de consultas, y tiene las mismas dimensiones que esta, es también multiplicada por $\vec E$ generando así el vector de claves ($\vec K$). Luego, se forma una nueva matriz a partir de los productos cruzados entre los vectores $\vec K$ y $\vec Q$ de cada token, se divide por la raíz de la dimensión de los vectores^[Es útil para mantener estabilidad numérica, la cual describe cómo los errores en los datos de entrada se propagan a través del algoritmo. En un método estable, los errores debidos a las aproximaciones se atenúan a medida que la computación procede.] ($\sqrt{d_k}$) y se normaliza con *softmax*^[ $softmax(x) = \frac{e^{x_i/t}}{\sum_j e^{x_j/t}}$] por columna^[Aplicar *softmax* hace que cada columna se comporte como una distribución de probabilidad.] ($\vec S$), valores más altos indican que un token (de las columnas) esta siendo influenciado por el comportamiento de otro token (de las filas).

![(before applying softmax) for example: Fluffy and Blue contribute to creature](../Imgs/attention_matrix.png)

Ahora que se sabe que tokens son relevantes para otros tokens, es necesario saber como son afectados. Para esto existe otra matriz de valores ($W_V$) que es multiplicada por cada vector de entrada resultando en los vectores de valor ($\vec V$) que son multiplicados a cada columna. La suma por filas devuelven el vector ($\Delta \vec E$) que debe ser sumado al vector de entrada original de cada token.

![](../Imgs/attention_matrix_softmax.png)

$$
\Delta \vec E_j = \sum_i S_j \cdot \vec V_i
$${#eq-11}

Con múltiples 'cabezas', cada una con sus propias matrices $W_K$, $W_Q$ y $W_V$, se generan varios $\Delta \vec E$ que se suman y se añaden al vector de entrada original. 

$$
\vec E \Leftarrow \vec E+ \sum_h \Delta \vec E_h
$${#eq-12}

De forma resumida

$$
\text{Atención}(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$${#eq-13}

Es importante notar que todas las matrices $Q$, $K$ y $V$ son preentrenadas.


4. Sumar y normalizar (*Add and norm*): En lugar de simplemente pasar los datos por las capas, las conexiones residuales se añaden sobre el vector de entrada en la salida de cada capa. Esto es lo que se hizo cuando se sumaron los cambios al vector de entrada, en lugar de directamente modificar el vector.

Dado que las redes neuronales profundas sufren de inestabilidad en los pesos al actualizarlos, una normalización al vector estabiliza el entrenamiento y mejora la convergencia. 

5. Red neuronal convolucional (CNN): A diferencia de otros modelos transformadores tradicionales, TimeGPT incorpora *CNN*s para descubrir dependencias locales y patrones de corto plazo, tratando de minimizar una función de costo (MAPE, MAE, RMSE, etc.).

*Decodificador*

1. *Output embedding* (desplazado hacia la derecha): La entrada del decodificador son los tokens desplazados hacia la derecha.

2. Codificación posicional

3. *Masked multi-head attention* (Atención multicabezal enmascarada): Ahora que las predicciones deben hacerce únicamente con los valores previos a cada token, en el proceso de 'atención' y antes de aplicar la transformación softmax se debe reemplazar todos los valores debajo de la diagonal principal de la matriz $QK$ por $-\infty$ para prevenir que los tokens sean influenciados por tokens anteriores.

4. *Multi-head attention*: En este caso, se usan las matrices de claves y valores que da como salida el codificador y la matriz de consultas es la salida de la capa de atención multicabezal enmascarada.

5. Conexión lineal: Es una capa completamente conectada que traduce las representaciones de atributos aprendidas en predicciones relevantes.


### 3.4.4 Diferencias entre TimeGPT y Chronos

TimeGPT es un modelo transformer preentrenado (de aquí las siglas GPT, *generative pre-trained transformer*) para el pronóstico de series de tiempo que puede producir predicciones en diversas áreas y aplicaciones con gran precisión y sin entrenamiento adicional. El mismo fue desarrollado por Nixtla y tuvo su primera beta privada en Agosto de 2023, volviendose accesible a todo público desde el 18 de julio de 2024.

Chronos es una familia de modelos transformadores preentrenados para series de tiempo basados en arquitecturas de modelos de lenguaje, el cúal fue desarrollado por Amazon y lanzado en marzo de 2024.

La primera diferencia entre ambos modelos es la accesibilidad al código fuente, mientras que Chronos es de código abierto, el modelo desarrollado por Nixtla no lo es.

Tal vez la diferencia más importante es como operan los modelos. Se mencionó anteriormente que TimeGPT utiliza la arquitectura transformer para diseñar un modelo que pueda trabajar directamente con series de tiempo. Chronos en cambio, aprovecha los modelos de lenguaje existentes para aplicarlos a datos temporales. Esto conlleva a otras diferencias clave, como que Chronos debe transformar los datos antes de poder procesarlos, y por trabajar con datos discretos, está entrenado para minimizar la entropía cruzada entre las distribuciones de las categorías reales contra las predichas.

La función de pérdida utilizada por Chronos está dada por:
$$
\ell(\theta) = \sum^{h+1}_{l=1} \sum^{|\nu_{ts}|}_{i=1} \mathbfcal{1}_{z_{n+l+1}=i} log \ \mathcal{p}_\theta(z_{n+l+1}=i| z_{1:n+l})
$${#eq-14}

Donde $|\nu_{ts}|$ es el tamaño del diccionario de tokens, el cúal depende del número de intervalos creados. $z_{n+h+1}$ es la serie transformada en *tokens* de la cúal las primeras $n$ observaciones se usarán como entrenamiento para pronosticar las siguientes $h$, y se agrega al final un token $\texttt{EOS}$ que se utiliza comunmente en los modelos de lenguaje para denotar el final de la secuencia. $\mathcal{p}_\theta$ es la probabilidad estimada por el modelo bajo la parametrización $\theta$.

Es importante notar que no es una función que detecta distancias, por lo que se espera que el modelo asocie a los intervalos cercanos gracias a la información en el conjunto de entrenamiento. Es decir que Chronos aplica regresión por clasificación.

Otra diferencia entre TimeGPT y Chronos es el tipo de red neuronal que utilizan para detectar patrones en los datos, mientras que el primero hace uso de las CNN, el segundo aplica *Feed-Forward Networks*. Luego de la conexión lineal, Chronos necesita volver a aplicar softmax para obtener las probabilidades del pronóstico, procedimiento que no es necesario por parte de TimeGPT.   

## 3.5 Métricas de evaluación


Para comparar el rendimiento de los modelos se utilizan métricas cuantitativas. Para los pronósticos puntuales se usará el porcentaje del error absoluto medio (MAPE), mientras que para los pronósticos probabilísticos se aplicará el *Interval Score*, propuesto por Gneiting y Raftery (2007), que penaliza tanto la amplitud de los intervalos como la falta de cobertura. Estas comparaciones permiten evaluar la precisión, la robustez y la eficiencia de cada enfoque.

Sea $e_l = Z_{n+l} - \hat Z_n(l)$ el error de la l-ésima predicción, donde $\hat Z_n(l)$ representa el pronóstico $l$ pasos hacia adelante, algunas medidas del error para pronósticos $h$ pasos hacia adelante son:

- Error Cuadrático Medio (*Mean Square Error, MSE*):

$$
MSE = \frac{1}{h}\sum^h_{l=1}e_l^2
$${#eq-15}

- Error absoluto medio (*Mean Absolute Error, MAE*):

$$
MAE = \frac{1}{h}\sum^h_{l=1}|e_l|
$${#eq-16}

- Porcentaje del error absoluto medio (*Mean Absolute Percentage Error, MAPE*)

$$
MAPE = (\frac{1}{h}\sum^h_{l=1}|\frac{e_l}{Z_{n+l}}|)\cdot 100 \%
$${#eq-17}

El problema de estos errores es que solo tienen en cuenta la estimación puntual, y por lo general es buena idea trabajar con pronósticos probabilísticos para cuantificar la incertidumbre de los valores futuros de la variable. Gneiting y Raftery (2007, JASA) propusieron en [*Strictly Proper Scoring Rules, Prediction, and Estimation*](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf) una nueva medida del error que tiene en cuenta los intervalos probabilísticos de la estimación, llamándola *Interval Score*:

$$
S = \frac{1}{h}\sum_{l=1}^h (W_l + O_l + U_l) 
$${#eq-18}

Donde:

$$
W_l = IS_l - II_l \hspace{1, cm}
$${#eq-19}

$$
O_l = \left \{ \begin{matrix} \frac{2}{\alpha}(Z_n(l) - Z_{n+l}) \hspace{10,mm} \text{si } Z_n(l) > Z_{n+l} \\ 0 \hspace{1,cm} \text{en otro caso} \hfill \end{matrix} \right. \hspace{1, cm} U_l = \left \{ \begin{matrix} \frac{2}{\alpha}(Z_{n+l} - Z_n(l)) \hspace{10,mm} \text{si } Z_n(l) < Z_{n+l} \\ 0 \hspace{1,cm} \text{en otro caso} \hfill \end{matrix} \right.
$${#eq-20}

Siendo $IS_l$ e $II_l$ los extremos superior e inferior del intervalo del l-ésimo pronóstico respectivamente. Es fácil darse cuenta que $W$ es una penalización por el ancho del intervalo, y que $O$ y $U$ son penalizaciones por sobre y subestimación respectivamente.

Se considera mejor al modelo que minimiza estas métricas.

## 3.6 Técnicas para la estimación de parámetros

La correcta elección de los parámetros del modelo constituye una de las tareas más importantes para lograr un buen ajuste de los datos. No resulta conveniente utilizar todos los datos disponibles para este fin, ya que esto puede conducir a un sobreajuste (*overfitting*). Se habla de sobreajuste cuando el modelo ase adapta excesivamente a los datos de entrenamiento, lo que perjudica su capacidad para generalizar sobre datos nuevos. Para evitar este problema se aplican técnicas específicas de validación que permiten seleccionar los parámetros sin comprometer la capacidad predictiva del modelo.

La validación *holdout* consiste en reservar una parte del conjunto de entrenamiento como validación, e ir probando las métricas de evaluación de las distintas configuraciones de parámetros del modelo, ajustado sobre el resto de los datos de entrenamiento, sobre este nuevo conjunto. Luego, para ajustar el modelo con la mejor combinación de parámetros, se utilizarán tanto los datos de entrenamiento como de validación. Por la ordinalidad de los datos, el conjunto de validación tendrá que ser más reciente que el conjunto de entrenamiento. 

$$
\text{Conjunto de entrenamiento total : } \{\underbrace{z_1, ...,  z_c}_{\text{Entrenamiento}}, \underbrace{z_{c+1}, ..., z_n}_{\text{Validación}} \}
$${#eq-21}

Si alguna serie presentara estacionalidad, se priorizará que el conjunto de validación tenga el largo del ciclo, para poder evaluar el ajuste del modelo en todo el ciclo.

Para ARIMA se usará el método de Box-Jenkins. En este método se compararán aquellos modelos que cumplan con los supuestos, y se elegirá como mejor combinación de parámetros aquella que minimize el AIC, medida de ajuste que penaliza por la cantidad de parámetros.

\newpage

# 4. Aplicación

La aplicación empírica de esta tesina tuvo como objetivo implementar, ajustar y comparar los modelos presentados en la [sección 3](#metodologia), utilizando un conjunto de series temporales seleccionadas. Para ello, se empleó el lenguaje de programación Python, junto con diversas librerías de código abierto.

Se trabajó con series temporales reales, obtenidas de bases de datos públicas y confiables. Cada serie fue modelada desde tres enfoques metodológicos distintos: modelos estadísticos tradicionales, algoritmos de aprendizaje automático y modelos de aprendizaje profundo. Para cada uno de ellos, se exploraron distintas configuraciones de parámetros, explicando su significado y función en el ajuste.


  - Los modelos estadísticos clásicos, como ARIMA y SARIMA, serán ajustados utilizando la librería `pmdarima`. La selección de parámetros se se realizó manualmente y mediante procedimientos automáticos, tomando como criterio principal el Criterio de Información de Akaike (AIC).

  - Para el enfoque de aprendizaje automático, se emplearon algoritmos de boosting, específicamente XGBoost y LightGBM, implementados con las librerías `xgboost` y `lightgbm` respectivamente.

  - En el caso de los modelos de aprendizaje profundo, se entrenaron redes LSTM utilizando la librería `neuralforecast`.

  - Finalmente, se exploraron dos modelos fundacionales preentrenados: TimeGPT, accedido a través de la API de Nixtla mediante la librería `nixtla`, y Chronos, una familia de modelos preentrenados desarrollada por *Amazon Web Services* cuya implementación se llevó a cabo con la librería `autogluon`.

Cada modelo fue evaluado en función de su desempeño predictivo, utilizando métricas como el error absoluto porcentual medio (MAPE) y el *Interval Score*. Además, se consideraron aspectos adicionales como el tiempo de cómputo, la facilidad de implementación y la interpretabilidad de los resultados. Los resultados fueron presentados en tablas comparativas y visualizaciones gráficas, acompañadas de un análisis crítico.

## 4.1 Series a utilizar

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# TEMP

atenciones_guardia = pd.read_excel(io='../Datos/Atenciones de guardia en el HNVV por patologías respiratorias (vigiladas por epidemiología).xlsx' )

# Aseguro que la columna fecha tenga el formato adecuado
atenciones_guardia['fec'] = pd.to_datetime(atenciones_guardia['fec'], format='%Y-%m-%d')

# Filtro las columnas importantes y las renombro
atenciones_guardia = atenciones_guardia[['fec', 'frec']]
atenciones_guardia.columns = ['ds','y']

# --------------------------------------------------------------------
# Cargamos los datos
trabajadores = pd.read_excel(io='../Datos/trabajoregistrado_2502_estadisticas.xlsx', sheet_name= 'A.2.1', thousands='.', decimal=',', header=1, usecols='A,M', skipfooter=5, skiprows=84)

# Renombramos las columnas
trabajadores.columns = ['ds', 'y']

# Asignamos formato fecha
meses = {
    'ene': '01', 'feb': '02', 'mar': '03', 'abr': '04', 
    'may': '05', 'jun': '06', 'jul': '07', 'ago': '08', 
    'sep': '09', 'oct': '10', 'nov': '11', 'dic': '12'
}

trabajadores['ds'] = trabajadores['ds'].str.replace('*','')
trabajadores['ds'] = trabajadores['ds'].apply(
    lambda x: '01-' + x.replace(x.split('-')[0], meses.get(x.split('-')[0].lower(), '')).replace(x.split('-')[1], '20' + x.split('-')[1])
)

trabajadores['ds'] = pd.to_datetime(trabajadores['ds'], format='%d-%m-%Y')

# Eliminamos los datos del 2025 para tener solo años completos
trabajadores = trabajadores[trabajadores['ds'].dt.year != 2025]

# --------------------------------------------------------------------

import glob

# Cargamos todos los archivos txt
ruta = glob.glob('../Datos/Datos meteorologicos/*.txt')
tiempo_region = pd.concat([pd.read_fwf(f, skiprows=[1], dtype={'FECHA' : str, 'HORA': str} , encoding='cp1252') for f in ruta], ignore_index=True)

# Filtramos los datos de rosario
tiempo_rosario = tiempo_region[tiempo_region['NOMBRE'] == 'ROSARIO AERO']

# Creamos la columna fecha y hora
tiempo_rosario['HORA'] = tiempo_rosario['HORA'].apply(lambda x: '0' + x if len(x) != 2 else x)
tiempo_rosario.loc[:,'ds'] = tiempo_rosario['FECHA'].apply(lambda x: x[0:2] + '-' + x[2:4] + '-' + x[4:len(x)])
tiempo_rosario['ds'] = pd.to_datetime(tiempo_rosario['ds'] + ' ' + tiempo_rosario['HORA'], format='%d-%m-%Y %H')

# Nos quedamos con las columnas utiles y renombramos la respuesta
tiempo_rosario = tiempo_rosario[['ds', 'TEMP', 'HUM', 'PNM']]
tiempo_rosario.columns = ['ds', 'y', 'HUM', 'PNM'] # % de Humedad y Presion a nivel del mar en hectopascales 

```

```{python}
# Creamos datasets con la info que conocemos de las series
atenciones_trunc = atenciones_guardia.head(len(atenciones_guardia)-12).copy()
trabajadores_trunc = trabajadores.head(len(trabajadores)-12).copy()
temperatura_trunc = tiempo_rosario.head(len(tiempo_rosario)-24).copy()
```

La primera serie analizada correspondió al número mensual de atenciones de guardia por patologías respiratorias (Códigos CIE10: J09–J18, J21, J22 y J44) en el Hospital de Niños Víctor J. Vilela de la ciudad de Rosario. Esta información fue provista por la Dirección General de Estadística de la Municipalidad de Rosario. La serie mostró una marcada estacionalidad, con picos en los meses de invierno, y una fuerte caída en 2020, atribuida a las medidas sanitarias adoptadas durante la pandemia de COVID-19.

```{python}
# Grafico serie Atenciones

plt.plot(atenciones_trunc['ds'], atenciones_trunc['y'], color = "#292F36")
plt.xlabel('Fecha')
plt.ylabel('Atenciones')

```

Esta serie no presenta una tendencia clara pero si estacionalidad, ya que se observan picos en los meses de invierno. Este patrón es más dicernible con el gráfico @fig-atenciones_estacionalidad del anexo. La abrupta caída en el número de atenciones del año 2020 se atribuye al estado de pandemia causado por el Covid-19. El objetivo será pronosticar las atenciones en guardia del año 2024.

Una serie que si presenta tendencia y estacionalidad al mismo tiempo es la cantidad de personas con empleo asalariado en el área de enseñanza, registradas en el sector privado. La cantidad de empleados aumenta casi constantemente año tras año, presentadose descensos drásticos en los meses de diciembre y enero. En esta serie también se evidencian las consecuencias de la pandemia. Se buscará pronosticar los trabajadores registrados en el área de enseñanza del año 2024.

```{python}
# Grafico serie trabajadores

plt.plot(trabajadores_trunc['ds'], trabajadores_trunc['y'], color = "#4D533C")
plt.xlabel('Fecha')
plt.ylabel('Trabajadores (miles)')
```

Por último, se analizarán las temperaturas por hora a lo largo de los días del mes de marzo 2025, estudiando sus comportamientos en relación a la humedad relativa y a la presión atmosférica estándar.  

```{python}
# Grafico serie temperatura

plt.subplot(3,1,1)

plt.plot(temperatura_trunc['ds'], temperatura_trunc['y'], color = "#520004")

plt.xlabel('')
plt.ylabel('Temperatura (ºC)')
plt.tick_params(labelbottom=False)

plt.subplot(3,1,2)

plt.plot(temperatura_trunc['ds'], temperatura_trunc['HUM'], color = "#520004")
plt.xlabel('')
plt.ylabel('Humedad (%)')
plt.tick_params(labelbottom=False)

plt.subplot(3,1,3)

plt.plot(temperatura_trunc['ds'], temperatura_trunc['PNM'], color = "#520004")
plt.xlabel('Fecha')
plt.ylabel('Presión atmosférica (hPa)')

```

Gracias al gráfico @fig-temp_estacional del anexo, se puede observar el patrón estacional diario que tiene la temperatura, se mantiene constante entre la noche y la mañana, pero a la tarde sube pronunciadamente. Se estimarán las temperaturas horarias del 21 y 22 de mayo de 2025.

## 4.2 Modelado


```{python}

# Primero cargamos los ambientes donde tenemos todos los resultados

globals().update(load_env('../Codigo/Ambiente/Amb_Aplicacion.pkl'))
globals().update(load_env('../Codigo/Ambiente/Amb_Aplicacion_chronos.pkl'))

# Guardamos los resultados de Chronos en las metricas
metricas_1.loc[len(metricas_1)] = ['Chronos', resultados_1_chronos['mape'], resultados_1_chronos['score'], resultados_1_chronos['tiempo']]
metricas_2.loc[len(metricas_2)] = ['Chronos', resultados_2_chronos['mape'], resultados_2_chronos['score'], resultados_2_chronos['tiempo']]
metricas_3.loc[len(metricas_3)] = ['Chronos', resultados_3_chronos['mape'], resultados_3_chronos['score'], resultados_3_chronos['tiempo']]
```

### 4.2.1 ARIMA

Por la complejidad agregada de pronosticar usando ARIMA en series con periodicidad diaria y variables exógenas, la serie de temperatura se modelizará únicamente con el método automático que se menciona más adelante.

Antes de proponer modelos, se debe hacer un breve análisis exploratorio de las series. En primer lugar se debe observar que la variancia en cada período es la misma. De lo contrario se debería transformar la variable que se desea pronosticar.

```{python}
# Boxplots periodicos - Variancia de las series

plt.subplot(3,1,1)
sns.boxplot(x = atenciones_trunc["ds"].dt.year, y = atenciones_trunc["y"], color = "#6BC78A")
plt.ylabel("Atenciones")

plt.subplot(3,1,2)
sns.boxplot(x = trabajadores_trunc["ds"].dt.year, y = trabajadores_trunc["y"], color = "#5299CB")
plt.ylabel("Trabajadores (miles)")

# plt.subplot(3,1,3)
# sns.boxplot(x = temperatura_trunc["ds"].dt.day, y = temperatura_trunc["y"], color = "#E23C47")
# plt.ylabel("Temperatura (Cº)")
```

```{python}
# Transformacion de box y cox
atenciones_ajust, atenciones_lambda = stats.boxcox(atenciones_trunc["y"])
temperatura_ajust, temperatura_lambda = stats.boxcox(temperatura_trunc["y"])

temp = atenciones_ajust
atenciones_ajust = atenciones_trunc.copy()
atenciones_ajust['y'] = temp
```

<!-- Es probable que las series de atenciones y de temperatura requieran de una transformación. Calculando $\lambda$ para la transformación de Box y Cox se concluye que las atenciones en guardia por patologías respiratorias necesitan ser transformadas aplicando la función exponencial ($\lambda$ = `python round(atenciones_lambda, 2)`). Por otro lado, no es necesario transformar la serie de temperaturas, ya que $\lambda$ resulta muy cercano a 1. -->

Dadas las diferencias en la variabilidad anual de las atenciones por año, se calculó $\lambda$ para la transformación de Box y Cox concluyendo que las atenciones en guardia por patologías respiratorias necesitan ser transformadas aplicando la función exponencial ($\lambda$ = `python round(atenciones_lambda, 2)`).

El primer paso de la modelización es estudiar las estacionalidades. Se mencionó anteriormente que las 2 series tienen estacionalidad, es por esto que se diferencian y se grafican, buscando que no sea visible ningún patrón estacional.

```{python}
# Series diferenciadas estacionalmente
atenciones_ajust["y_sin_est"] = atenciones_ajust['y'].diff(12)
trabajadores_trunc["y_sin_est"] = trabajadores_trunc['y'].diff(12)
# temperatura_trunc["y_sin_est"] = temperatura_trunc['y'].diff(24).diff(24)


plt.subplot(3,1,1)
sns.lineplot(x = atenciones_trunc["ds"], y = atenciones_ajust["y_sin_est"], color = "#6BC78A")
plt.ylabel("Atenciones")

plt.subplot(3,1,2)
sns.lineplot(x = trabajadores_trunc["ds"], y = trabajadores_trunc["y_sin_est"], color = "#5299CB")
plt.ylabel("Trabajadores (miles)")

# plt.subplot(3,1,3)
# sns.lineplot(x = temperatura_trunc["ds"], y = temperatura_trunc["y_sin_est"], color = "#E23C47")
# plt.ylabel("Temperatura (Cº)")
```

Una vez diferenciadas estacionalmente, es importante que no exista una tendencia clara, por lo que se estandarizan estacionariamente.

```{python}
# Series diferenciadas estacionariamente
atenciones_ajust["y_diff"] = atenciones_ajust['y_sin_est'].diff(1)
trabajadores_trunc["y_diff"] = trabajadores_trunc['y_sin_est'].diff(1)
# temperatura_trunc["y_diff"] = temperatura_trunc['y_sin_est'].diff(1)


plt.subplot(3,1,1)
sns.lineplot(x = atenciones_ajust["ds"], y = atenciones_ajust["y_diff"], color = "#6BC78A")
plt.ylabel("Atenciones")

plt.subplot(3,1,2)
sns.lineplot(x = trabajadores_trunc["ds"], y = trabajadores_trunc["y_diff"], color = "#5299CB")
plt.ylabel("Trabajadores (miles)")

# plt.subplot(3,1,3)
# sns.lineplot(x = temperatura_trunc["ds"], y = temperatura_trunc["y_diff"], color = "#E23C47")
# plt.ylabel("Temperatura (Cº)")
```

Para descubrir las componentes AR y MA que afectan a las series se grafican las autocorrelaciones y autocorrelaciones parciales de cada una.

```{python}
plt.subplot(2,1,1)
autocorr_plot(atenciones_ajust["y_diff"], lags=25)
plt.subplot(2,1,2)
autocorr_plot(atenciones_ajust["y_diff"], lags=25, atype='pacf')
```

En la serie de atenciones se puede ver que tanto en las autocorrelaciones como en las autocorrelaciones parciales, hay valores significativos en el quinto y doceavo rezago, lo que podrían significar que existen compotentes MA y AR en la parte estacional o estacionaria de la serie.

Se proponen entonces los modelos $SARIMA(0,1,1)(0,1,0)_{12}$ y $SARIMA(0,1,0)(0,1,1)_{12}$.

```{python}
plt.subplot(2,1,1)
autocorr_plot(trabajadores_trunc["y_diff"], lags=25)
plt.subplot(2,1,2)
autocorr_plot(trabajadores_trunc["y_diff"], lags=25, atype='pacf')
```

En las autocorrelaciones de la serie de trabajadores resultan significativos el primer y doceavo rezago, mientras que en las autocorrelaciones parciales se destacan los rezagos 1, 12, 13 y 24. Se puede suponer que el rezago 13 no es en realidad significativo y que la serie presenta una componente MA en la parte estacionaria y una AR en la estacional, formando el modelo $SARIMA(0,1,1)(1,1,0)_{12}$.

```{python}
# plt.subplot(2,1,1)
# autocorr_plot(temperatura_trunc["y_diff"], lags=48)
# plt.subplot(2,1,2)
# autocorr_plot(temperatura_trunc["y_diff"], lags=48, atype='pacf')
```

<!-- Por último, se puede concluir que la serie de temperatura presenta componentes AR en la parte estacionaria y estacional, $SARIMA(1,1,1)(1,2,0)_{12}$. -->

Además, por medio de la función `auto_arima` de la librería `pmdarima` se propusieron modelos automáticos. Se elegirá como mejor modelo aquel que minimice el Criterio de Información de Akaike (AIC) y cumpla con las propiedades del modelo ARIMA.

\begin{table}[H]
\centering
\caption{Cumplimiento de las condiciones de estacionariedad e invertibilidad de los modelos ajustados.}
\begin{tabular}{|clclcl|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Modelo}}     & \multicolumn{1}{c|}{\multirow{2}{*}{AIC}} & \multicolumn{2}{c|}{Parte regular}                                         & \multicolumn{2}{c|}{Parte estacional}                 \\ \cline{3-6} 
\multicolumn{1}{|c|}{}                            & \multicolumn{1}{c|}{}                     & \multicolumn{1}{c|}{Estacionariedad} & \multicolumn{1}{c|}{Invertibilidad} & \multicolumn{1}{c|}{Estacionariedad} & Invertibilidad \\ \hline
\multicolumn{6}{|c|}{ATENCIONES EN GUARDIA}                                                                                                                                                                                        \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,1)(0,1,0)_{12}$} & \multicolumn{1}{l|}{857.6}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & Si             \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,0)(0,1,1)_{12}$} & \multicolumn{1}{l|}{852.1}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & Si             \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,0)(1,0,0)_{12}$} & \multicolumn{1}{l|}{1013.5}               & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & Si             \\ \hline
\multicolumn{6}{|c|}{TRABAJADORES}                                                                                                                                                                                                 \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,1)(1,1,0)_{12}$} & \multicolumn{1}{l|}{359.9}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & Si             \\ \hline
\multicolumn{1}{|c|}{$SARIMA(2,0,0)(2,1,0)_{12}$} & \multicolumn{1}{l|}{356.7}                & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{Si}              & Si             \\ \hline
\multicolumn{6}{|c|}{TEMPERATURA}                                                                                                                                                                                                  \\ \hline
\multicolumn{1}{|c|}{$SARIMAX(1,1,1)(2,0,1)_{24}$} & \multicolumn{1}{l|}{1067.1}               & \multicolumn{1}{c|}{Si}              & \multicolumn{1}{c|}{Si}             & \multicolumn{1}{c|}{No}              & Si             \\ \hline
\end{tabular}
\end{table}

Para la serie de atenciones se seguirá trabajando con el segundo modelo propuesto y con el seleccionado de manera automática, ya que el parámetro MA del primer modelo prupuesto no es significativo. Por otro lado, los dos modelos probados para la serie de trabajadores seguirán siendo utilizados. El modelo automático para la serie de temperaturas no goza de estacionariedad, por lo que se proponen los modelos $SARIMAX(1,1,1)(1,0,1)_{24}$, $SARIMAX(1,1,0)(2,0,1)_{24}$ y $SARIMAX(1,1,0)(1,1,0)_{24}$, de los cúales únicamente el último cumple con todas las propiedades buscadas, con un AIC de 1119.3.

Las salidas de los modelos se pueden ver en el [anexo](#salidas_arima) para comprobar los resultados.

Lo siguiente es comprobar que los residuos estandarizados se comporten como ruido blanco.

```{python}
#| fig-cap: "Comprobación de supuestos del 2º modelo manual para las atenciones en guardia"

resid_check(resultados_arima['resid_arima_atenciones_2'], ds = atenciones_trunc['ds'])
```

El histograma muestra la distribución de los residuos, pudiendo ver y comprobar con el test de Kolmogorov-Smirnov que es normal. La serie de los residuos muestra como estos no tienen un patrón particular ni variabilidad dependiente del tiempo, solo 2 *outlayers* fáciles de ignorar. Por último, los gráficos inferiores muestran las autocorrelaciones y autocorrelaciones parciales, esperando que no haya ninguna significativa. Por medio del test de Ljung-Box se comprueba que ninguna lo es y se concluye que los residuos no están correlacionados. Por lo tanto todos los supuestos se cumplen para este modelo. 

Con el propósito de no poblar de gráficos el documento, el resto de los gráficos para la comprobación de supuestos de los demás modelos se encuentran en el [anexo](#supuestos_arima). En estos se destaca que todos los modelos, con excepción del seleccionado automáticamente para los trabajadores, superaron la comprobación de supuestos. El problema de este modelo es la existencia de correlación en los residuos, supuesto que es sumamente importante al ajustar modelos ARIMA. Si bien los residuos del modelo seleccionado manualmente para los trabajadores no cumple con normalidad, es sencillo ver que esto se debe a unos poco valores extremos, y que sin estos es muy probable que se sí se distribuyan normalmente. De lo contrario, en la próxima sección se detallarán posibles soluciones.

```{python}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Line Plot 1"
#|   - "Line Plot 2"

plot_forecast(data = atenciones_guardia, forecast = resultados_arima['pred_atenciones_1'], pred_color = 'green', label = 'ARIMA', long = 48)

plot_forecast(data = trabajadores, forecast = resultados_arima['pred_trabajadores_1'], pred_color = 'green', label = 'ARIMA', long = 48)

```

```{python}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Line Plot 1"
#|   - "Line Plot 2"

plot_forecast(data = atenciones_guardia, forecast = resultados_arima['pred_atenciones_2'], pred_color = 'grey', line_color='#6BC78A', label = 'IC 80%', long = 48)

plot_forecast(data = atenciones_guardia, forecast = resultados_1_arima['pred'], pred_color = 'grey', line_color='#6BC78A', label = 'IC 80%', long = 48)

```

```{python}
#| layout-ncol: 2
#| fig-cap: 
#|   - "Line Plot 1"
#|   - "Line Plot 2"

plot_forecast(data = trabajadores, forecast = resultados_arima['pred_trabajadores_1'], pred_color = 'grey', line_color='#5299CB', label = 'IC 80%', long = 48)

plot_forecast(data = tiempo_rosario[['ds','y']], forecast = resultados_arima['pred_temperatura_3'], pred_color = 'grey', line_color='#E23C47', label = 'IC 80%', long = 48)

```

Uno de los problemas de pronosticar con ARIMA es que para trabajar con variables exógenas se debe conocer los valores futuros de estas. Para este caso se asume que los valores de humedad y presión atmosférica son conocidos para el día 21, sin embargo, para el resto de aplicaciones esto no será así y se considera esto como una desventaja de este modelo de pronóstico.

Otro problema que se encuentra es que el intervalo de confianza para las atenciones por guardia toma valores negativos, algo claramente ilógico. Esto se puede resolver ajustando modelos sobre el logaritmo de la variable, y luego transformando los valores pronosticados a la escala original. Esta solución queda propuesta para un futuro trabajo y se usan los resultados como exposición de los problemas para pronosticar usando la familia de modelos ARIMA.

```{python}
# CALCULO DE MAPES
mape_atenciones_2 = mean_absolute_percentage_error(y_true=atenciones_guardia.tail(12)['y'], y_pred=resultados_arima['pred_atenciones_2']['pred'])

mape_trabajadores_1 = mean_absolute_percentage_error(y_true=trabajadores.tail(12)['y'], y_pred=resultados_arima['pred_trabajadores_1']['pred'])

mape_temperatura_3 = mean_absolute_percentage_error(y_true=tiempo_rosario.tail(24)['y'], y_pred=resultados_arima['pred_temperatura_3']['pred'])


# CALCULO DE INTERVAL SCORES
is_atenciones_2 = interval_score(obs=atenciones_guardia.tail(12)['y'], lower = resultados_arima['pred_atenciones_2']['lower'], upper = resultados_arima['pred_atenciones_2']['upper'], alpha = 0.2)

is_trabajadores_1 = interval_score(obs=trabajadores.tail(12)['y'], lower = resultados_arima['pred_trabajadores_1']['lower'], upper = resultados_arima['pred_trabajadores_1']['upper'], alpha = 0.2)

is_temperatura_3 = interval_score(obs=tiempo_rosario.tail(24)['y'], lower = resultados_arima['pred_temperatura_3']['lower'], upper = resultados_arima['pred_temperatura_3']['upper'], alpha = 0.2)
```

\begin{table}[H]
\centering
\caption{Métricas de evaluación de los modelos ARIMA.}
\begin{tabular}{|clc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Modelo}}     & \multicolumn{1}{c|}{\multirow{2}{*}{MAPE}} & \multirow{2}{*}{\textit{Interval Score}} \\
\multicolumn{1}{|c|}{}                            & \multicolumn{1}{c|}{}                      &                                   \\ \hline
\multicolumn{3}{|c|}{Atenciones en guardia}                                                                                        \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,0)(0,1,1)_{12}$} & \multicolumn{1}{l|}{`{python} round(mape_atenciones_2, 4)`}                  & `{python} round(is_atenciones_2, 4)`                                \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,0)(1,0,0)_{12}$} & \multicolumn{1}{l|}{`{python} round(resultados_1_arima['mape'], 4)`}                  & `{python} round(resultados_1_arima['score'], 4)`                                \\ \hline
\multicolumn{3}{|c|}{Trabajadores registrados}                                                                                                 \\ \hline
\multicolumn{1}{|c|}{$SARIMA(0,1,1)(1,1,0)_{12}$} & \multicolumn{1}{l|}{`{python} round(mape_trabajadores_1, 4)`}                  & `{python} round(is_trabajadores_1, 4)`                                \\ \hline
\multicolumn{3}{|c|}{Temperatura}                                                                                                  \\ \hline
\multicolumn{1}{|c|}{$SARIMA(1,1,0)(1,1,0)_{24}$} & \multicolumn{1}{l|}{`{python} round(mape_temperatura_3, 4)`}                  & `{python} round(is_temperatura_3, 4)`                                \\ \hline
\end{tabular}
\end{table}


### 4.2.2 XGBoost

Los modelos de pronóstico basados en el aprendizaje automático no tienen supuestos que cumplir, por lo que el modelaje se vuelve mucho más sencillo y automatizable. Los únicos aspectos en los que se debe tener especial cuidado y atención es en la selección de características y parámetros del modelo.

Tanto para XGBoost como para LightGBM las características elegidas para todas las series fueron las siguientes:

- Identificación temporal

- El promedio de las 3 observaciones anteriores 

- El desvío estándar de las 3 observaciones anteriores

- El valor del primer rezago

- El valor del segundo rezago

- El valor del rezago estacional

Con el objetivo de seleccionar los mejores hiperparámetros, se elaboró una grilla con diferentes combinaciones de valores para cada parámetro. Se ajustaron modelos cumpliendo cada combinación en la grilla sobre un conjunto de validación.

XGBoost permite parametrizar los modelos de varias formas, los que se eligieron ajustar en este trabajo son los siguientes:

- Número de árboles que se contruyen paralelamente en cada iteración ($A$). Opciones: 20, 50, 100, 150.

- Profundidad máxima del árbol ($P$). Opciones: 2, 3, 4, 5.

- Número máximo de hojas del árbol ($H$). Opciones: 2, 4, 8, 16.

- Tasa de aprendizaje en el método del gradiente ($\eta$). Opciones: 0.1, 0.2, 0.3.

En la tabla @tbl-resultadosxgb se muestra para cada serie que combinación de parámetros, de las 192 posibles, fue la que menor MAPE produjo sobre el conjunto de validación. Además, se presenta el MAPE e *Interval Score* que devuelve el modelo entrenado con todos los datos de entrenamiento y evaluado sobre el conjunto de prueba.

\begin{table}[]
\centering
\caption{Modelos XGBoost seleccionados y métricas de evaluación.}
\label{tbl-resultadosxgb}
\begin{tabular}{|c|l|l|l|l|c|c|}
\hline
\multirow{2}{*}{Serie}   & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$A$\end{tabular}}}            & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$P$\end{tabular}}}            & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$H$\end{tabular}}}         & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$\eta$\end{tabular}}}              & \multirow{2}{*}{MAPE}                         & \multirow{2}{*}{\textit{Interval Score}}              \\
                         & \multicolumn{1}{c|}{}                                                                                                    & \multicolumn{1}{c|}{}                                                                                                 & \multicolumn{1}{c|}{}                                                                                                  & \multicolumn{1}{c|}{}                                                                                                     &                                               &                                                \\ \hline
Atenciones en guardia    & `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]` & `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` & `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` & `{python} float(resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` & `{python} round(resultados_1_xgb['mape'], 4)` & `{python} round(resultados_1_xgb['score'], 4)` \\ \hline
Trabajadores registrados & `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]` & `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` & `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` & `{python} float(resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` & `{python} round(resultados_2_xgb['mape'], 4)` & `{python} round(resultados_2_xgb['score'], 4)` \\ \hline
Temperatura              & `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]` & `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` & `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` & `{python} float(resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` & `{python} round(resultados_3_xgb['mape'], 4)` & `{python} round(resultados_3_xgb['score'], 4)` \\ \hline
\end{tabular}
\end{table}


```{python}
plt.figure(figsize=(9, 5))
plt.subplot(3,1,1)
plot_forecast(data = atenciones_guardia, forecast = resultados_1_xgb['pred'], pred_color = 'green', label = 'XGBoost', long = 24)

plt.subplot(3,1,2)
plot_forecast(data = trabajadores, forecast = resultados_2_xgb['pred'], pred_color = 'green', label = 'XGBoost', long = 24)

plt.subplot(3,1,3)
plot_forecast(data = tiempo_rosario, forecast = resultados_3_xgb['pred'], pred_color = 'green', label = 'XGBoost', long = 36)

```

### 4.2.3 LightGBM

```{python}
plt.figure(figsize=(9, 5))
plt.subplot(3,1,1)
plot_forecast(data = atenciones_guardia, forecast = resultados_1_lgbm['pred'], pred_color = 'green', label = 'LigthGBM', long = 24)

plt.subplot(3,1,2)
plot_forecast(data = trabajadores, forecast = resultados_2_lgbm['pred'], pred_color = 'green', label = 'LigthGBM', long = 24)

plt.subplot(3,1,3)
plot_forecast(data = tiempo_rosario, forecast = resultados_3_lgbm['pred'], pred_color = 'green', label = 'LigthGBM', long = 36)
```


### 4.2.4 LSTM

```{python}
plt.figure(figsize=(9, 5))
plt.subplot(3,1,1)
plot_forecast(data = atenciones_guardia, forecast = resultados_1_lstm['pred'], pred_color = 'violet', label = 'LSTM', long = 24)

plt.subplot(3,1,2)
plot_forecast(data = trabajadores, forecast = resultados_2_lstm['pred'], pred_color = 'violet', label = 'LSTM', long = 24)

plt.subplot(3,1,3)
plot_forecast(data = tiempo_rosario, forecast = resultados_3_lstm['pred'], pred_color = 'violet', label = 'LSTM', long = 36)

```

### 4.2.5 Chronos

```{python}
plt.figure(figsize=(9, 5))
plt.subplot(3,1,1)
plot_forecast(data = atenciones_guardia, forecast = resultados_1_chronos['pred'], pred_color = 'violet', label = 'Chronos', long = 24)

plt.subplot(3,1,2)
plot_forecast(data = trabajadores, forecast = resultados_2_chronos['pred'], pred_color = 'violet', label = 'Chronos', long = 24)

plt.subplot(3,1,3)
plot_forecast(data = tiempo_rosario, forecast = resultados_3_chronos['pred'], pred_color = 'violet', label = 'Chronos', long = 36)
```


### 4.2.6 TimeGPT

## 4.3 Comparaciones

**Serie 1**

```{python}
metricas_1
```

**Serie 2**

```{python}
metricas_2
```

**Serie 3**

```{python}
metricas_3
```

\newpage

# 5. Conclusiones	

<!-- -  Resume las principales conclusiones del estudio.
-  Destaca las contribuciones del trabajo y su relevancia para el campo de la estadística y el análisis de datos.
- Fortalezas y limitaciones de TimeGPT en comparación con otros modelos.
- Proporciona recomendaciones finales y reflexiones sobre posibles direcciones futuras de investigación. -->

Problemas arima: consume mucho tiempo, necesita muchos conocimientos teoricos para problemas especificos (variables de conteo, evitar negativos, variancia no constante), puede que se trabaje hasta llegar a un modelo que luego no cumpla los supuestos, no es util cuando se tienen variables exogenas, porque solo permite variables ya conocidas

En este documento se presentan 3 contribuciones claves para el campo de la estadística. En primer lugar y como objetivo principal de esta tesina, la introducción de los modelos transformadores para el pronóstico de series temporales. Además se incluyeron aportes como evaluar el desempeño de los modelos con una nueva medida del error, el *interval score*, el cúal no tiene en cuenta únicamente el pronóstico puntual sino también probabilístico. El último gran aporte es la construcción de intervalos de pronóstico por medio de *conformal predictions*, que no depende de conocer la distribución de los residuos.

\newpage

# 6. Bibliografía

**Ansari et al.** (2024). *Chronos: Learning the Language of Time Series*. Transactions on Machine Learning Research. [https://arxiv.org/abs/2403.07815](https://arxiv.org/abs/2403.07815)

**Awan, A.** (2 de septiembre de 2024). *Time Series Forecasting With TimeGPT*. Datacamp. [https://www.datacamp.com/tutorial/time-series-forecasting-with-time-gpt](https://www.datacamp.com/tutorial/time-series-forecasting-with-time-gpt)

**Bermejo, J.** (21 de mayo de 2024). *Redes neuronales*. Facultad de Ciencias Económicas y Estadística de la Universidad Nacional de Rosario.

**Elhariri, K.** (1 de marzo de 2022). *The Transformer Model*. Medium. [https://medium.com/data-science/attention-is-all-you-need-e498378552f9](https://medium.com/data-science/attention-is-all-you-need-e498378552f9)

**Gilliland, M., Sglavo, U., & Tashman, L.** (2016). *Forecast Error Measures: Critical Review and Practical Recommendations*. John Wiley & Sons Inc.

**Gneiting, T., & Raftery A. E.** (2007). *Strictly Proper Scoring Rules, Prediction, and Estimation*. Journal of the American Statistical Association, 102(477), 359–378. [https://doi.org/10.1198/016214506000001437](https://doi.org/10.1198/016214506000001437)

**Hyndman, R. J., & Athanasopoulos, G.** (2021). *Forecasting: principles and practice (3rd ed.).*
OTexts. [https://otexts.com/fpp3/](https://otexts.com/fpp3/)

**IBM**. (s.f.). *Explainers*. Recuperado el 14 de marzo de 2025 en [https://www.ibm.com/think/topics](https://www.ibm.com/think/topics)

**Kamtziris, G.** (27 de febrero de 2023). *Time Series Forecasting with XGBoost and LightGBM: Predicting Energy Consumption.* Medium. [https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-460b675a9cee](https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-460b675a9cee)

**Korstanje, J.** (2021). A*dvanced Forecasting with Python*. Apress.

**Nielsen, A.** (2019). *Practical Time Series Analysis: Prediction with Statistics and Machine Learning*. O'Reilly Media.

**Nixtla**. (s.f.-a). *About TimeGPT*. Recuperado en diciembre de 2024 de [https://docs.nixtla.io/docs/getting-started-about_timegpt](https://docs.nixtla.io/docs/getting-started-about_timegpt)

**Nixtla**. (s.f.-b). *LSTM*. Recuperado el 9 de abril de 2025 en [https://nixtlaverse.nixtla.io/neuralforecast/models.lstm.html#lstm](https://nixtlaverse.nixtla.io/neuralforecast/models.lstm.html#lstm)

**Parmezan, A., Souza, V., & Batista, G.** (1 de mayo de 2019). *Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model*. Information Sciences. [https://www.sciencedirect.com/science/article/abs/pii/S0020025519300945](https://www.sciencedirect.com/science/article/abs/pii/S0020025519300945)

**Prunello, M., & Marfetán, D.** (12 de mayo de 2024). *Árboles de decisión*. Facultad de Ciencias Económicas y Estadística de la Universidad Nacional de Rosario.

**Sanderson, G.** [3Blue1Brown]. (2024). *Attention in transformers, step-by-step | DL6 [Video]*. Youtube. [https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1204s](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1204s)

**Sanderson, G.** [3Blue1Brown]. (2024). *Transformers (how LLMs work) explained visually | DL5 [Video]*. Youtube. [https://www.youtube.com/watch?v=wjZofJX0v4M](https://www.youtube.com/watch?v=wjZofJX0v4M)

**Shastri, Y.** (26 de abril de 2024). *Attention Mechanism in LLMs: An Intuitive Explanation*. Datacamp. [https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition](https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition)

**Silberstein, E.** (7 de noviembre de 2024). *Tracing the Transformer in Diagrams*. Medium. [https://medium.com/data-science/tracing-the-transformer-in-diagrams-95dbeb68160c](https://medium.com/data-science/tracing-the-transformer-in-diagrams-95dbeb68160c)

**Vaswani et al.** (2017). *Attention is all you need*. Google. [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)


\newpage

# 7. Anexo

## 7.1 Gráficos estacionales

```{python}
#| label: fig-atenciones_estacionalidad
#| fig-cap: Atenciones por guardia mensuales por patologías respiratorias por año

atenciones_trunc['mes'] = atenciones_trunc['ds'].dt.month
atenciones_trunc['Año'] = atenciones_trunc['ds'].dt.year

paleta = ["#3885bc", "#547cc4", "#7770c2", "#9b5fb7", "#b94aa0", "#cf317f", "#5CC17E", "#89D2A1", "#d81b58", "#d5202c"]

sns.lineplot(
  data = atenciones_trunc,
  x = "mes",
  y = "y",
  hue = "Año",
  palette=paleta,
  legend = True
)




plt.xlabel("Mes")
plt.ylabel("Atenciones")
plt.xticks(
    ticks=range(1, 13),
    labels=[
        "Ene", "Feb", "Mar", "Abr", "May", "Jun", "Jul", "Ago", "Sep", "Oct", "Nov", "Dic"
    ],
)

```

```{python}
#| label: fig-temp_estacional
#| fig-cap: Atenciones por guardia mensuales por patologías respiratorias por año

temperatura_trunc['Hora'] = temperatura_trunc['ds'].dt.hour
temperatura_trunc['Día'] = temperatura_trunc['ds'].dt.day

paleta = ["#3885bc", "#547cc4", "#A58D78", "#7A6552", "#7770c2", "#9b5fb7", "#b94aa0", "#cf317f", "#5CC17E", "#89D2A1", "#d81b58", "#d5202c"]

sns.lineplot(
  data = temperatura_trunc,
  x = "Hora",
  y = "y",
  hue = "Día"
)

plt.xlabel("Hora")
plt.ylabel("Temperatura (Cº)")
```

## 7.2 Salidas de modelos arima {#salidas_arima}

**Atenciones modelo 1**

```{python}
print(resultados_arima['salida_arima_atenciones_1'])
```

**Atenciones modelo 2**

```{python}
print(resultados_arima['salida_arima_atenciones_2'])
```

**Atenciones modelo selección automática**

```{python}
print(resultados_arima['salida_arima_atenciones_auto'])
```

**Trabajadores modelo 1**

```{python}
print(resultados_arima['salida_arima_trabajadores_1'])
```

**Trabajadores modelo selección automática**

```{python}
print(resultados_arima['salida_arima_trabajadores_auto'])
```

**Temperatura modelo selección automática**

```{python}
print(resultados_arima['salida_arima_temperatura_auto'])
```

**Temperatura modelo 1**

```{python}
print(resultados_arima['salida_arima_temperatura_1'])
```

**Temperatura modelo 2**

```{python}
print(resultados_arima['salida_arima_temperatura_2'])
```

**Temperatura modelo 3**

```{python}
print(resultados_arima['salida_arima_temperatura_3'])
```

## 7.3 Comprobación de supuestos de modelos arima {#supuestos_arima}

**Modelo atenciones selección automática**

```{python}
#| fig-cap: "Comprobación de supuestos del modelo automático para las atenciones en guardia"
resid_check(resultados_arima['resid_arima_atenciones_auto'], ds = atenciones_trunc['ds'])
```

**Modelo trabajadores 1**

```{python}
#| fig-cap: "Comprobación de supuestos del modelo manual para los trabajadores en el rubro de la enseñanza"
resid_check(resultados_arima['resid_arima_trabajadores_1'], ds = trabajadores_trunc['ds'])
```

**Modelo trabajadores selección automática**

```{python}
#| fig-cap: "Comprobación de supuestos del modelo automático para los trabajadores en el rubro de la enseñanza"
resid_check(resultados_arima['resid_arima_trabajadores_auto'], ds = trabajadores_trunc['ds'])
```

**Modelo Temperatura 3**

```{python}
#| fig-cap: "Comprobación de supuestos del 3º modelo manual para la temperatura"
resid_check(resultados_arima['resid_arima_temperatura_3'], ds = temperatura_trunc['ds'])
```

# NOTAS

- Series a utilizar: 
      Estacionalidad y tendencia > trabajadores registrados en educacion en el ambito privado (tmb puede ser aalim/tabaco, servicios de seg/lim/inv, ) (https://www.argentina.gob.ar/trabajo/estadisticas/situacion-y-evolucion-del-trabajo-registrado) ACTUALIZAR HAY DATOS NUEVOS
      Estacionalidad > Demanda mensual residencial del consumo de luz o algun emae estacional, ENFERMEDADES RESPIRATORIAS
      variable exogena > Temperatura + humedad + presion atmosferica (https://www.smn.gob.ar/descarga-de-datos)
      
- TO DO: 
      Definir y aplicar las caracteristicas para la serie de temperatura
      Redactar EnbPI para XGBoost y Lightgbm
      Explicar Aplicacion, parametros usados
      Revisar Arima
      revisar transformers
      revisar medidas de evaluacion
      Volver a correr toda la aplicacion y verificar resultados
      verificar luego de la aplicacion cuales son las librerias que termino usando
      Verificar, actualizar y completar las descripciones de las funciones
      Actualizar los requirements


- Hacer dendograma de importancia de variables para los modelos de machine learning, timegpt (variables exogenas)


- Guardar las versiones de librerias, software y hardwate en la que se corre el codigo

- Hacer equivalencia del documento en GitBook

- leer fpp3 que tiene unas cosas que pueden servir , tambien https://otexts.com/fpppy/nbs/15-foundation-models.html#chronos


Nombrar ls 3 opcions de calculo de intervalo de xgboost
Tomar el tiempo como el ultimo modelo PERO EXPLICARLO

nomenclatura: 
  - conjunto de observaciones $z_1, ..., z_t, ..., z_n$
  - pronostico $z_{n+1}, ..., z_{n+l}, ...   , z_{n+h}$


# Mejoras a la investigacion (en las que no me voy a centrar)

- Hacer que los modelos elijan el mejor ajuste con otras medidas del error en lugar de minimizar el mape, como el interval score

- explorar otras medidas del error

- implementar boosting y ensamblaje con otros metodos

- Explorar otras caracteristicas para los metodos de ensamblaje

- medir el tiempo de ejecucion de los algoritmos de otra manera

- Series con distintas periodicidades (semestrales, trimestrales, etc) y distintas unidades (diarias. anuales, horarias, etc)

- Explorar mas sobre conformal predictions y demas opciones para crear intervalos de confianza

- usar ngboost para obtener pronosticos probabilisticos

- Validacion cruzada temporal para el entrenamiento de los modelos

# Preguntas nanda

Para la serie de temperatura: Como diferencio para sacar la estacionalidad? La humedad y la presion las tomo como variables exogenas pasadas o futuras?

La humedad tienen claros outlayers, que hago? no parecen afectar los pronosticos

Que hago con los modelos arima con los que no dan los supuestos? me la rebusco?

Para pronosticar a varios horizontes, puedo usar simplemente un horizonte mas largo y compararlo progresivamente? 

debo explicar sarimax? que tanto debo profundizar sobre la teoria de arima?

identificar con colores las series o los pronosticos?

Uso conformal predictions tmb para Lightgmb? o me quedo con el pronostico cuantilico?




