---
format: 
  pdf:
    papersize: A4
    crossref:
      fig-prefix: ""
      tbl-prefix: ""
engine: python
metadata:
  quarto-python:
    python: ".venv/Scripts/python.exe"
fig-pos: H
toc: false
lang: es
echo: False
warning: False
message: False
geometry: 
  - top= 25.4mm
  - left= 25.4mm
  - right = 25.4mm
  - bottom = 25.4mm
header-includes: 
  - \usepackage{multirow}
  - \usepackage{float}
---

```{python}
# LIBRERIAS

# Para mostrar los resultados
from Funciones import plot_forecast, load_env, autocorr_plot, resid_check, interval_score, summary_to_latex, tabla_resumen

# Para calcular MAPE
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

# Para la transformacion de box y cox
from scipy import stats

# Para testar estacionaridad
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss

# Para graficar
from plotnine import ggplot, aes, geom_line, geom_ribbon, geom_histogram, geom_rect, geom_segment, geom_hline, geom_point, scale_x_continuous, scale_x_date, theme, theme_bw, element_blank, labs, scale_color_manual, scale_fill_manual, scale_y_continuous, element_text, theme_set, annotate, after_stat, geom_boxplot
from plotnine.options import set_option, get_option

theme_set(theme_bw() + theme(figure_size=(6, 2)))
set_option("figure_size", (6, 2))

# Para manejo de datos
import pandas as pd
import numpy as np
```


\newgeometry{margin=0mm}

\pagenumbering{gobble}
 
\thispagestyle{empty}

\begin{figure}[htbp]
    \noindent
    \includegraphics[width=\paperwidth, height=\paperheight]{../Imgs/Portada_tesina.png}
\end{figure}


\newpage

\newpage


\restoregeometry


\section*{Agradecimientos}

Este trabajo no solo es el cierre de mi paso por la carrera sino también de una de las mejores etapas de mi vida. Estuvo llena de crecimiento, esfuerzo y alegría. Decir que aprendí mucho académicamente es desmerecer todos estos años, porque también estuvieron acompañados de enseñanzas sobre la vida misma. Y como crecer no es algo que uno pueda lograr sin ayuda de nadie, está claro que debo agradecer a un gran puñado de personas.

En primer lugar a mis papás, que me dejaron venir desde lejos a pesar de que no les gustara separarse, que hicieron que no tenga que preocuparme por llegar a fin de mes, y que nunca me presionaron ni me exigieron nada.

A mis hermanos, al que tengo lejos por darme siempre mensajes de ánimo y preocuparse de que no me faltara nada, y al que tengo cerca por ser un gran compañero de piso a pesar de mis constantes quejas.

A mi novia le tengo que agradecer por estar a mi lado, en las buenas y en las malas, por levantarme el ánimo cuando me iba mal o celebrar juntos si me iba bien, por hacerme empujar siempre para adelante y obligarme a nunca bajar los brazos.

Uno de mis más grandes agradecimientos tiene que ir a todos los amigos que hice durante la carrera, tanto para con los que compartí los primeros años como para con los que compartí los últimos. Sin ellos probablemente me hubiera rendido mil y una veces, pero gracias a saber que no estaba solo y que siempre había alguien dispuesto a ayudar pude seguir adelante y llegar hasta donde estoy hoy.

También debo agradecer a todos los profesores que tuve, que sin ellos nada de esto sería posible. Fueron los que me abrieron las puertas al conocimiento y me llenaron de curiosidad, aprendí muchísimo de cada uno de ellos y muchos se convirtieron en mi rol a seguir como profesional.

Muchas gracias especialmente a Nanda, mi directora, que fue ella quien decidió apostar por mí para desarrollar este tema que me encantó investigar. Siempre estuvo disponible cuando la necesité y movió todos los hilos para que mi trabajo se desarrollara de la forma más suave y rápida posible.

Por último pero no menos importante, muchas gracias a la Universidad Nacional de Rosario, y en especial a la Facultad de Ciencias Económicas y Estadística, que brinda el lugar, el material y las oportunidades para que uno pueda crecer sin techo alguno.

Este trabajo no es solo mío, tiene una pizca de todos los que me acompañaron estos duros pero increíbles años, muchas gracias a cada uno de ustedes.

\newpage

\section*{Resumen}

La predicción de series temporales desempeña un rol crucial en contextos como la salud, la economía, la energía y la gestión de recursos, donde anticipar el comportamiento futuro de una variable resulta fundamental para la toma de decisiones. Esta tesina compara el desempeño de distintos enfoques para el pronóstico de series temporales, incluyendo modelos estadísticos tradicionales (ARIMA, SARIMA), algoritmos de aprendizaje automático (XGBoost, LightGBM), redes neuronales recurrentes (LSTM) y modelos fundacionales preentrenados basados en transformadores (TimeGPT y Chronos).

La evaluación se llevó a cabo sobre tres series reales representativas, con diferentes estructuras temporales: (i) número mensual de atenciones por patologías respiratorias en un hospital pediátrico, (ii) empleo privado en el sector educativo, y (iii) temperatura horaria durante el mes de marzo. Para cada caso, se implementaron y ajustaron los modelos mencionados, comparando sus resultados mediante métricas puntuales (MAPE) y probabilísticas (*Interval Score*).

Los resultados muestran que no existe un modelo universalmente superior: mientras los modelos estadísticos ofrecen interpretabilidad y precisión, por otro lado son poco automatizables y requieren de un control manual que requiere tiempo y conocimiento. Los algoritmos de *boosting* presentan buena capacidad predictiva con bajo costo computacional, pero necesitan especial cuidado al seleccionar con que características entrenar el modelo y no ofrecen intervalos probabilísticos de forma nativa. Las redes LSTM, aunque potentes, muestran sensibilidad al sobreajuste y son exigentes computacionalmente. Por su parte, los modelos fundacionales ofrecen una alternativa rápida para personas que no tienen profundos conocimientos en el pronóstico de series de tiempo, aunque son modelos poco personalizables y su estructura interna no siempre es accesible.

Este trabajo aporta evidencia empírica y conceptual para una selección informada de modelos de pronóstico en función del contexto, destacando el potencial de las herramientas recientes como TimeGPT y Chronos, así como la importancia de incorporar métricas probabilísticas y técnicas de *conformal prediction* para mejorar la estimación de la incertidumbre.

Palabras clave: series temporales, predicción, *conformal predictions*, aprendizaje automático, redes neuronales, *transformers*, TimeGPT, *interval score*, Chronos. 

\newpage
\renewcommand{\contentsname}{Índice}
\tableofcontents
\newpage
\pagenumbering{arabic}

# 1. Introducción


La predicción de valores futuros en series de tiempo es una herramienta clave en múltiples ámbitos, tales como la economía, el comercio, la salud, la energía y el medio ambiente. En estos contextos, anticipar el comportamiento de una variable permite mejorar la planificación, asignar recursos de forma más eficiente y reducir la incertidumbre.

Actualmente, la ciencia de datos se encuentra en una etapa de constante expansión e innovación, impulsada por la gran cantidad de datos generados diariamente, por lo que en un contexto creciente de complejidad y exigencia temporal, resulta conveniente contar con herramientas que faciliten y acorten los tiempos de trabajo. Si bien los métodos más conocidos para trabajar series de tiempo son precisos, requieren de amplios conocimientos para encontrar un buen ajuste. Además, los modelos tradicionales como ARIMA son difíciles de automatizar, mientras que los algoritmos de aprendizaje automatizado que se utilizan actualmente pueden demandar largos tiempos de entrenamiento. Frente a estas limitaciones, en los últimos años se han desarrollado modelos capaces de seleccionar de forma automática el mejor ajuste para una serie temporal dada, sin requerir entrenamiento previo ni conocimientos especializados en análisis de series de tiempo. Estos son los denominados modelos fundacionales preentrenados, tales como TimeGPT o Chronos.

Sin embargo, aún persisten interrogantes sobre el desempeño de estos nuevos modelos y la falta de acceso al código fuente de algunos de estos limita la posibilidad de auditar sus resultados o replicar su implementación. Por ello, esta tesina propone realizar una comparación sistemática de modelos de pronóstico para series de tiempo, abordando tres enfoques metodológicos: modelos estadísticos tradicionales, algoritmos de *machine learning* y modelos de aprendizaje profundo. El objetivo es evaluar su desempeño en distintos contextos, utilizando métricas como el porcentaje del error absoluto medio (MAPE) y el *Interval Score*, con el fin de analizar ventajas, limitaciones y potenciales usos de cada uno.

Este análisis busca aportar una mirada crítica e informada sobre el uso de nuevas tecnologías en la predicción de series de tiempo, contribuyendo a la toma de decisiones metodológicas más sólidas desde una perspectiva estadística.

\newpage

# 2. Objetivos

## 2.1 Objetivo general

El objetivo de esta tesina es, en primer lugar, comparar la precisión, eficiencia y facilidad de pronosticar series de tiempo con distintos modelos, incluyendo enfoques estadísticos clásicos, algoritmos de *machine learning* y modelos de *deep learning*, analizando al mismo tiempo sus ventajas, limitaciones y condiciones de uso más apropiadas.

## 2.2 Objetivos específicos

- Implementar modelos clásicos de series de tiempo, como ARIMA y SARIMA, explicando y garantizando el cumplimiento de los fundamentos teóricos y supuestos que los sostienen.

- Aplicar modelos de aprendizaje automático supervisado, como XGBoost y LightGBM, explorando distintas configuraciones para garantizar el mejor ajuste.

- Desarrollar modelos de aprendizaje profundo, en particular redes LSTM, dando introducción a las redes neuronales y modelos de pronóstico más complejos.

- Realizar pronósticos con modelos fundacionales (TimeGPT, Chronos) y comprender su funcionamiento.

- Definir y aplicar métricas de evaluación (MAPE, *Interval Score*) para comparar el rendimiento de todos los modelos bajo un mismo conjunto de datos.

- Reflexionar valorativamente sobre los criterios de selección de modelos en función del contexto de aplicación, la complejidad computacional y la interpretabilidad de los resultados.

\newpage

# 3. Metodología {#metodologia}

El enfoque metodológico adoptado en esta tesina consiste en comparar el desempeño de distintos modelos de pronóstico aplicados a series temporales. Para ello, se seleccionan modelos representativos de tres enfoques principales: modelos estadísticos tradicionales, algoritmos de aprendizaje automático (*machine learning*) y modelos de aprendizaje profundo (*deep learning*).

El análisis se estructura en tres componentes fundamentales: una descripción conceptual de los modelos, su implementación práctica sobre series con diferentes características, y una evaluación cuantitativa comparativa a través de métricas de error.

## 3.1 Conceptos básicos de series de tiempo

Se denomina serie de tiempo a un conjunto de observaciones $\{z_1, z_2, ..., z_t, ..., z_n\}$ cuantitativas ordenadas en el tiempo, usualmente de forma equidistante, sobre una variable de interés. El análisis de series de tiempo tiene como objetivo sintetizar y extraer información estadística relevante, tanto para interpretar el comportamiento histórico de la variable como para generar pronósticos $\{ z_{n+1}, ...,z_{n+l}, ...., z_{n+h} \}$.

Dado que las series temporales pueden exhibir diversos patrones subyacentes, resulta útil descomponerlas en componentes separadas, cada una de las cuales representa una característica estructural específica del comportamiento de la serie.

- Estacionalidad: corresponde a las fluctuaciones periódicas que se repiten a intervalos regulares de tiempo. Un ejemplo típico es la temperatura, que tiende a disminuir en invierno y aumentar en verano, repitiendo este patrón anualmente.

- Tendencia (o tendencia-ciclo): refleja la evolución a largo plazo de la media de la serie, asociada a procesos de crecimiento o decrecimiento sostenido. Por ejemplo, la población mundial exhibe una tendencia creciente a lo largo del tiempo.

- Residuos: representa las variaciones no sistemáticas que no pueden ser explicadas por la tendencia ni la estacionalidad. Estas fluctuaciones, que suelen deberse a eventos impredecibles o factores exógenos, se asumen como aleatorias.

Otro concepto importante en series de tiempo es la estacionariedad. Se dice que una serie es débilmente estacionaria si la media y la variancia se mantienen constantes en el tiempo y la correlación entre distintas observaciones solo depende de la distancia en el tiempo entre estas. Por comodidad, cuando se mencione estacionariedad se estará haciendo referencia al cumplimiento de estas propiedades. 

## 3.2 Modelos estadísticos tradicionales para series temporales

Son llamados modelos estadísticos tradicionales a aquellos que surgen antes del auge del *machine learning* y los modelos de aprendizaje profundo. Son caracterizados por sus fuertes fundamentos estadísticos y su capacidad en capturar dependencias temporales en los datos.

### 3.2.1 SARIMA {#arima}

Los modelos $ARIMA$ (*AutoRegresive Integrated Moving Average*) son unos de los modelos de pronóstico tradicionales mejor establecidos. Son una generalización de los modelos autoregresivos (AR), que suponen que las observaciones futuras son función de las observaciones pasadas, y los modelos promedio móvil (MA), que pronostican las observaciones como funciones de los errores de observaciones pasadas. Además, estos modelos pueden adaptarse a series no estacionarias mediante la aplicación de diferenciaciones de orden $d$, las cuales implican restar a cada observación el valor registrado $d$ periodos anteriores.

Formalmente un modelo $ARIMA(p,d,q)$ se define como:

$$
\psi_p(B)(1-B)^dz_t = \theta_0 + \theta_q(B)\alpha_t
$${#eq-1}

Donde $z_t$ es la observación $t$-ésima, $\psi_p(B)$ y $\theta_q(B)$ son funciones de los rezagos ($B$), correspondientes a la parte autoregresiva y promedio móvil respectivamente, $d$ es el grado de diferenciación y $\alpha_t$ es el error de la $t$-ésima observación.

Se debe tener en cuenta estos aspectos importantes:

- Se dice que una serie es invertible si se puede escribir cada observación como una función de las observaciones pasadas más un error aleatorio. Por definición, todo modelo AR es invertible.

- Por definición, todo modelo MA es estacionario.

- $\psi_p(B) = 1 - \psi_1 B - \psi_2 B^2 - ... - \psi_p B^p$ es el polinomio característico de la componente AR y $\theta_q(B) = 1 - \theta_1 B - \theta_2 B^2 - ... - \theta_p B^q$ de la componente MA. Si las raíces de los polinomios característicos caen fuera del círculo unitario, entonces un proceso AR se puede escribir de forma MA y es estacionario, y a su vez un proceso MA se puede escribir de forma AR y es invertible.

- Un proceso $ARIMA$ es estacionario e invertible si su componente AR y MA lo son respectivamente.

Sin embargo este tipo de modelos no tienen en cuenta la posible estacionalidad que puede tener una serie, es por esto que se introducen los modelos $SARIMA(p,d,q)(P,D,Q)_s$ que agregan componentes AR, MA y diferenciaciones a la parte estacional de la serie con período $s$.

Se denomina función de autocorrelación a la función de los rezagos, entendiendo por rezago a la distancia ordinal entre dos observaciones, que grafica la autocorrelación entre pares de observaciones. Es decir que para cada valor $k$ se tiene la correlación entre todos los pares de observaciones a $k$ observaciones de distancia. En su lugar, la función de autocorrelación parcial calcula la correlación condicional de los pares de observaciones, removiendo la dependencia lineal de estas observaciones con las que se encuentran entre estas. Estas funciones son necesarias para poder identificar los modelos $SARIMA$ y se definen como:

$$
\rho_k = Corr(z_t, z_{t+k}) = \frac{Cov(z_t, z_{t+k})}{\sqrt{Var(z_t) \cdot Var(z_{t+k})}}
$${#eq-auto}

y
$$
\phi_{kk} = Corr(z_t, z_{t+k}|z_{t+1}, ..., z_{t+k-1})
$${#eq-autopar}

Los modelos $AR$ se caracterizan por tener autocorrelaciones significativas que decaen lentamente y autocorrelaciones parciales significativas únicas. Los modelos $MA$ se comportan de forma inversa, tienen autocorrelaciones significativas únicas y autocorrelaciones parciales que decaen progresivamente.

```{python}

# Ejemplos de procedimientos AR Y MA

def plot_ejemplo(autocorrelaciones, atype = 'acf', lags = 20):
    if atype == 'acf':
        lags = np.arange(lags+1)
        ylab = r'$\rho_k$'
    else:
        lags = np.arange(lags+1)
        lags = lags[1:]
        ylab = r'$\phi_{kk}$'

    # Defino las bandas limite
    upper_bound = 0.25
    lower_bound = -0.25

    # Cambio el color del punto si supera el limite
    col = np.where(
        autocorrelaciones['y'] < lower_bound,'b',
        np.where(autocorrelaciones['y'] > upper_bound, 'b', 'green'))

    return(ggplot(autocorrelaciones) +
    aes(x = 'lag', y = 'y') +
    geom_rect(ymin = lower_bound, ymax = upper_bound, xmax = np.inf, xmin = -np.inf, alpha = 0.01 ,fill = "#8ED081") +
    geom_rect(aes(ymin = -1, ymax = -np.inf, xmax = np.inf, xmin = -np.inf), alpha = 0.01 ,fill = "grey") +
    geom_rect(aes(ymin = 1, ymax = np.inf, xmax = np.inf, xmin = -np.inf), alpha = 0.01 ,fill = "grey") +
    geom_segment(aes(x = 'lag', xend = autocorrelaciones['lag'], yend = 'y', y = 0)) +
    geom_hline(yintercept = 0) +
    geom_hline(color = "green",yintercept = upper_bound, linetype = "dashed") +
    geom_hline(color = "green",yintercept = lower_bound, linetype = "dashed") +
    geom_point(color = "#8ED081", size = 0.7) +
    geom_point(aes(x = 'lag', y = 'y'), color = col, size = 0.7) +
    scale_y_continuous(limits = (-1,1)) +
    scale_x_continuous(breaks = list(range(0,max(autocorrelaciones['lag']), 2)), limits = (0,max(autocorrelaciones['lag']))) +
    labs(x = "Rezago (k)", y = ylab)
    )


ar_acf = pd.DataFrame({"y":[1, 0.74, 0.62, 0.56, 0.5, 0.46, 0.41, 0.37, 0.33, 0.3, 0.26, 0.23, 0.2, 0.17, 0.14, 0.11, 0.08, 0.06, 0.03, 0.01],
"lag" : np.arange(0,20)
})

ma_pacf = pd.DataFrame({"y":ar_acf['y']*[1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1],
"lag" : np.arange(0,20)
})

pico = pd.DataFrame({"y":[1, 0.7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
"lag" : np.arange(0,20)
})

#ar
plot_ejemplo(ar_acf, atype='acf').save("../Imgs/plotnine/ejemplo_ar_1.png", width=6, height=4/2.1, dpi=700)
plot_ejemplo(pico.tail(19), atype='pacf').save("../Imgs/plotnine/ejemplo_ar_2.png", width=6, height=4/2.1, dpi=700)

#ma
plot_ejemplo(pico, atype='acf').save("../Imgs/plotnine/ejemplo_ma_1.png", width=6, height=4/2.1, dpi=700)
plot_ejemplo(ma_pacf.tail(19), atype='pacf').save("../Imgs/plotnine/ejemplo_ma_2.png", width=6, height=4/2.1, dpi=700)

```

::: {#fig-ejemar layout-nrow=2}

![](../Imgs/plotnine/ejemplo_ar_1.png)

![](../Imgs/plotnine/ejemplo_ar_2.png)

Ejemplo de las autocorrelaciones de un proceso $AR(1)$.
:::

::: {#fig-ejemma layout-nrow=2}

![](../Imgs/plotnine/ejemplo_ma_1.png)

![](../Imgs/plotnine/ejemplo_ma_2.png)

Ejemplo de las autocorrelaciones de un proceso $MA(1)$.
:::

Para poder identificar un modelo $SARIMA$ a partir de las autocorrelaciones es necesario que la serie sea estacionaria. Entre los *tests* para probar la estacionariedad de una serie se encuentran el *test* aumentado de Dickey-Fuller y el de Kwiatkowski-Phillips-Schmidt-Shin. El *test* aumentado de Dickey-Fuller plantea como hipótesis nula que existe una raíz unitaria, indicando que la serie no es estacionaria y es necesaria una diferenciación, ante la alternativa de que no existe una raíz unitaria. Por otro lado, en el *test* Kwiatkowski-Phillips-Schmidt-Shin la hipótesis nula indica que la serie es estacionaria alrededor de una constante.

Un buen modelo $SARIMA$ debe cumplir las siguientes propiedades:

- Sus residuos se comportan como ruido blanco, es decir, están incorrelacionados y siguen una distribución normal, con media y variancia constantes.

- Es admisible, es decir, es invertible y estacionario.

- Es parsimonioso, en el sentido de que sus parámetros son significativos.

- Es estable en los parámetros, que se cumple cuando las correlaciones entre los parámetros no son altas.

Existen múltiples opciones para probar la normalidad de una distribución. En este trabajo se utiliza el *test* no paramétrico de Kolmogorov-Smirnov, cuya hipótesis nula sostiene que la muestra proviene de la distribución de referencia, que en este caso es la distribución normal. Para probar que los residuos no presentan correlación, se hace uso del *test* de Ljung-Box. La hipótesis nula de esta prueba indica que los datos no están correlacionados.

## 3.3 Modelos de aprendizaje automático

El aprendizaje automático (machine learning) es una rama de la inteligencia artificial que permite a las computadoras aprender de los datos y realizar tareas de forma autónoma. Aunque los métodos presentados no fueron diseñados específicamente para datos temporales, han demostrado ser útiles en múltiples contextos mediante diversas pruebas empíricas.

Los métodos de *machine learning*, a diferencia de los modelos tradicionales, se enfocan principalmente en identificar los patrones que describen el comportamiento del proceso que sean relevantes para pronosticar la variable de interés, y no se componen de reglas ni supuestos que tengan que seguir. Para la identificación de patrones, estos modelos requieren la generación de características. 

### 3.3.1 Introducción a árboles de decisión y ensamblado

Los árboles de decisión pueden ser explicados sencillamente como un conjunto extenso de estructuras condicionales *if-else*. El modelo pronosticará un cierto valor $x$ si una cierta condición es verdadera, u otro valor $y$ si es falsa. Es importante ver que no hay una tendencia lineal en este tipo de lógica, por lo que los árboles de decisión pueden ajustar tendencias no lineales. El resultado que se obtiene al aplicar esta técnica puede resumirse gráficamente como un tronco con diferentes ramas, y de esta característica surge su nombre.

Un árbol puede tener distinta cantidad de divisiones en un mismo nivel, llamadas hojas, y profundidad, las cuales determinan en qué medida el modelo se ajusta a los datos con los que se entrena. Lógicamente árboles más profundos y con más hojas suelen generar sobreajuste, es decir, un modelo que se adapta demasiado a los datos de entrenamiento y generaliza erróneamente.

![Ejemplo de árbol de decisión](../Imgs/arbol_decision.png){width=70%}


Los métodos de ensamblaje buscan mejorar la robustez y precisión de las predicciones combinando los resultados de varios estimadores base, como los árboles de decisión. Los modelos no se construyen infinitamente, sino que se busca minimizar una función de pérdida que incluye una penalización por la complejidad del modelo, limitando así la cantidad de árboles que se producen. Existen múltiples métodos de ensamblaje (Bosques aleatorios, XGBoost, LightGBM, CatBoost, entre otros) que se diferencian en la forma en la que se construyen los árboles. En esta tesina se usarán los algoritmos *eXtreme Gradient Boosting* (XGBoost) y *Light Gradient-Boosting Machine* (LightGBM).


Se denomina *Boosting* a un proceso iterativo, que consiste en la construcción de árboles de forma secuencial donde cada nuevo árbol busca predecir los residuos de los árboles anteriores. Es así entonces que el primer árbol buscará predecir los valores futuros de la serie, mientras que el segundo intentará predecir los valores reales menos los pronosticados por el primer árbol, el tercero tratará de inferir la diferencia entre los valores reales y el valor pronosticado del primer árbol menos los errores del segundo, y así sucesivamente. En cada iteración se pesan los puntos y se corrigen aquellos que tengan un mayor error por medio del descenso del gradiente. 

![Proceso de ensamblado](../Imgs/Ensamblaje.png){width=80%}

La dirección de máximo crecimiento para una función está determinada por su gradiente, y por consiguiente, la dirección contraria es la dirección de máximo decrecimiento. El descenso del gradiente es un algoritmo iterativo en el que, con el objetivo de minimizar una función de pérdida, se calcula en cada paso la derivada de la función de costo con respecto a cada parámetro en su valor actual. Luego, se actualizan los valores de los parámetros desplazandolos una pequeña magnitud $\eta$, llamada “tasa de aprendizaje”, de forma tal que la función de pérdida decrezca.

Sea $L(x)$ una función de pérdida y $\theta_0$ el valor actual de un parámetro del modelo, la actualización de dicho parámetro se realiza de la siguiente manera:

$$
\theta_1 = \theta_0 - \eta \cdot \nabla L(\theta_0)
$${#eq-grad}


![Ejemplo del descenso del gradiente en una función de pérdida](../Imgs/desc_gradiente.png){width=80%}

Este procedimiento iterativo se repite hasta lograr la convergencia o se llegue a un límite de iteraciones. Es por esto que la elección de $\eta$ es sumamente importante para lograr el objetivo, ya que valores grandes podrían ocasionar divergencia, mientras que valores pequeños pueden llevar a que el algoritmo necesite demasiadas iteraciones para lograr la convergencia.

### 3.3.2 Diferencias entre XGBoost y LightGBM

Las diferencias entre XGBoost y LightGBM radican en la forma en que cada uno identifica las mejores divisiones dentro de los árboles y de que forma los hacen crecer.

XGBoost utiliza un método exacto en el que se calcula la ganancia de las particiones con cada característica, entendiendo por ganancia a la contribución relativa de una característica en el contexto de un árbol particular. Se elige la mejor partición y repite el proceso. Si bien este método es preciso, es lento computacionalmente. Por otro lado, LightGBM usa un método más eficiente llamado *Gradient-Based One-Side Sample* (GOSS). GOSS calcula los gradientes para cada punto y lo usa para filtrar afuera aquellos puntos que tengan un bajo gradiente. Que un punto tenga un gradiente bajo significaría que está "mejor pronosticado" que el resto, y por lo tanto no es necesario enfocarse tanto en mejorar dicho punto. Además, LightGBM utiliza un procedimiento que acelera el ajuste cuando se tienen muchas características correlacionadas de las cuales elegir, denominado  *Exclusive Feature Bundle* (EFB). 

A la hora de hacer crecer los árboles, XGBoost lo hace nivel a nivel, es decir que primero se crean todas las divisiones de un nivel, y luego se pasa al siguiente, priorizando que el árbol sea simétrico y tenga la misma profundidad en todas sus ramas. LigthGBM, en cambio, se expande a partir de la hoja que más reduce el error, mejorando la precisión y eficiencia en series largas, pero arriesgándose a posibles sobreajustes si no se limita correctamente la profundidad de los árboles.

### 3.3.3 Intervalos de confianza en algoritmos de aprendizaje automático

Una de las principales ventajas de los modelos de aprendizaje automático respecto de los enfoques estadísticos tradicionales es que no requieren supuestos distribucionales estrictos sobre los errores. Sin embargo, esta flexibilidad también implica que, en general, no generan pronósticos probabilísticos de forma directa, lo que dificulta la construcción de intervalos de confianza.

Para subsanar esta limitación, se han desarrollado diversos métodos que permiten acompañar las predicciones puntuales con medidas de incertidumbre. A continuación, se describen dos de los enfoques más utilizados.

- Regresión cuantil con *boosting*: consiste en entrenar modelos para estimar directamente cuantiles de la distribución condicional de la variable objetivo, en lugar de su valor esperado. De este modo, pueden construirse intervalos de predicción utilizando, por ejemplo, los percentiles 5 y 95. Esta técnica está implementada en algunas variantes como LightGBM, pero no es directamente aplicable en XGBoost, lo que restringe su uso en ciertos entornos.

- *Conformal predictions*: se trata de una familia de métodos no paramétricos que permiten construir intervalos de predicción válidos bajo el supuesto de intercambiabilidad de las observaciones. Dado que este supuesto no se cumple en series temporales, donde existe dependencia temporal, se han desarrollado adaptaciones específicas para este tipo de datos.

Una de las más destacadas es el método *Ensemble Batch Prediction Intervals* (EnbPI), que permite aplicar *conformal predictions* en series temporales sin asumir independencia entre observaciones. Su procedimiento consiste en:

1. Seleccionar un modelo por ensamblado (como XGBoost o LightGBM).

2. Generar B muestras *bootstrap* por bloques, manteniendo así la estructura temporal de los datos.

3. Ajustar un modelo sobre cada una de las B muestras.

4. Para cada observación del conjunto de entrenamiento, calcular el residuo utilizando únicamente aquellos modelos que no la incluyeron.

5. Obtener las predicciones puntuales promediando los resultados de los B modelos.

6. Construir los intervalos de predicción sumando y restando los cuantiles empíricos de los residuos a las predicciones.

$$
IC_{Z_{n+l}; 1-\alpha}= Z_{n}(l) \pm Q_{1-\alpha}(e)
$${#eq-2}

Este método será el utilizado en esta tesina para estimar los intervalos de confianza en los algoritmos de aprendizaje automático y se aplica con la librería `MAPIE`. 

## 3.4 Modelos de aprendizaje profundo

El *deep learning* (aprendizaje profundo) es una rama del *machine learning* que tiene como base un conjunto de algoritmos que intentan modelar niveles altos de abstracción en los datos usando múltiples capas de procesamiento, con complejas estructuras o compuestas de varias transformaciones no lineales. 

Entre estos algoritmos se encuentran las redes neuronales, que imitan el funcionamiento del cerebro humano usando procesos que simulan la forma biológica en la que trabajan las neuronas para identificar fenómenos, evaluar opciones y llegar a conclusiones. 

### 3.4.1 Introducción a redes neuronales

Una red neuronal esta compuesta en grandes rasgos de 3 capas: entrada, oculta y salida. Dentro de cada capa se pueden encontrar neuronas y conexiones entre estas, donde cada neurona representa una variable y cada conexión un peso, y es por esto que a estas conexiones las llamaremos así en adelante. La suma de los pesos y las neuronas que no formen parte de la capa de entrada dan el total de parámetros que tiene que ajustar el modelo.

![Ejemplo de red neuronal completamente conectada](../Imgs/red_conectada.png){#fig-redneuro width=80%}

En la capa de entrada se introducen las variables explicativas, y luego cada neurona fuera de esta capa es una función de las neuronas anteriores conectadas a la misma. Estas funciones son llamadas funciones de activación.

![Obtención de una predicción en un red neuronal](../Imgs/conexiones_redneuro.png){#fig-conexiones width=80%}

Los parámetros se estiman buscando minimizar una función de costo, esto se logra con el descenso del gradiente y por medio de retropropagación. La retropropagación consiste en realizar una estimación inicial de la variable respuesta con los valores iniciales de la red neuronal, que pueden estar dados, por ejemplo, por una distribución normal, y de manera inversa a la dirección de la red neuronal calcular derivadas para encontrar la dirección de máximo decrecimiento de la función de costo para cada parámetro en la red neuronal. 

Existen distintos tipos de redes neuronales según la forma en la que se conectan las neuronas. Las *Feedforward Neural Networks* (FNN) son las redes neuronales más comunes y simples. Las redes neuronales recurrentes, del inglés *Recurrent Neural Networks* (RNN) utilizan bucles de retroalimentación que las hacen especialmente buenas en la predicción de datos secuenciales. Otro tipo de red neuronal son las *Convolutional Neural Networks* (CNN), las cuales son útiles para el reconocimiento de patrones en los datos.

### 3.4.2 *Long Short Term Memory* (LSTM)

Lo que caracterizan a las redes neuronales recurrentes son los bucles de retroalimentación que se presentan en la figura @fig-rnn. Mientras que cada neurona de entrada en una FNN es independiente, en las redes neuronales recurrentes se relacionan entre ellas y se retroalimentan.

![Ejemplo de RNN](../Imgs/red_recurrente.png){#fig-rnn width=80%}


Un problema frecuente en las RNN es su dificultad para capturar dependencias de largo plazo. Esto puede tener 2 causas, el desvanecimiento o la explosión del gradiente. El desvanecimiento del gradiente ocurre cuando, iteración tras iteración, el gradiente se aproxima a cero y se estabiliza, evitando que la red siga aprendiendo. Por el contrario, cuando el gradiente crece exponencialmente se habla de una explosión, esto lleva a inestabilidades en el aprendizaje, provocando que las actualizaciones de los parámetros sean erráticas e impredecibles.

Las redes neuronales con memoria a corto y largo plazo (LSTM) son un tipo de RNN que solucionan este problema mediante un algoritmo logístico de 3 puertas.

![Estructura *Long Short Term Memory*](../Imgs/lstm.png){width=80%}

**Puerta de guardado**

La puerta de guardado se encarga de decidir que proporción de la información a largo plazo mantener en la neurona de memoria en cada iteración. Esta puerta recibe la entrada y el estado de la RNN, y las pasa como argumentos de una función sigmoide. 

$$
\sigma(x) =  \frac{1}{1+e^{-x}}
$${#eq-3}

Sean $S_{t-1}$ la información de la memoria a corto plazo actual de la red, $x_t$ la entrada actual, y $W_t$ y $B_t$ los pesos y sesgos de la puerta de guardado respectivamente:

$$
K_t = \sigma(W_k \times [S_{t-1}, x_t] + B_k)
$${#eq-4}

$$
Old_t = K_t \times C_{t-1}
$${#eq-5}

Donde $C_{t-1}$ es la información a largo plazo guardada actualmente y $Old_t$ lo que se mantendrá para la próxima iteración de la red.

Si $K_t$ es igual a 1, significa que la información guardada debe ser mantenida perfectamente. Si $K_t$ fuera igual a 0, la información a largo plazo debe ser descartada completamente.

**Puerta de entrada**

La puerta de entrada controla que información añadir a la neurona de memoria. Propone un nuevo valor para la información a largo plazo y decide que proporción de esta sumar al valor actual. Sea $tanh(x)$ la función tangente hiperbólica y $W_n$ y $B_n$ pesos y sesgos respectivamente, el nuevo valor propuesto para la información a largo plazo es:

$$
N_t = \tanh(W_n\times[S_{t-1},x_t]+B_n)
$${#eq-nt}

Y la proporción de esta que se sumará al valor actual esta dada por $I_t$, cuyos pesos y sesgos son $W_i$ y $B_i$.

$$
I_t = \sigma(W_i\times[S_{t-1},x_t]+B_i)
$${#eq-6}

Por lo que el nuevo valor de la información a largo plazo de la red es:
$$
C_t = Old_t + New_t
$${#eq-ct}

Donde $New_t = I_t \times N_t${#eq-7}


**Puerta de salida**

La puerta de salida se encarga de extraer la información más importante del estado actual de la neurona para usar como salida. Sean $W_o$ y $B_o$ los pesos y sesgos de la puerta de salida:

$$
O_t = \sigma(W_o \times [S_{t-1},x_t] + B_o)
$${#eq-8}

$$
S_t = O_t \times tanh(C_t)
$${#eq-9}

Donde $S_t$ es la nueva información a corto plazo en la red neuronal.

Las 3 puertas son logísticas para que sea sencillo aplicar la retropropagación. Este sistema de puertas evita los problemas de desvanecimiento y explosión del gradiente, y evita que se acumulen muchos estados por largos períodos de tiempo, eligiendo que información es relevante guardar.

### 3.4.3 Modelos transformadores

Otro tipo de modelo de aprendizaje profundo son los *transformer models* (modelos transformadores), los cuales son significativamente más eficientes al entrenar y realizar inferencias que las RNNs; gracias al uso de mecanismos de atención, presentados en la publicación '[*Attention is all you need*](https://arxiv.org/pdf/1706.03762)' de Google. Esots mecanismos capturan dependencias y relaciones en la secuencias de valores que se alimentan al modelo, logrando poner en contexto a cada observación. 

Los modelos transformadores fueron creados originalmente con el propósito de generar texto. Sin embargo, tanto TimeGPT como Chronos explotan esta tecnología para el pronóstico de series de tiempo. Ambos modelos son preentrenados, lo cual significa que la optimización de parámetros y pesos fue realizada antes de usarse el modelo. Esto se logra entrenando y generalizando el modelo en un conjunto de datos extenso, por lo general de fuentes públicas. El preentrenamiento permite que el modelo adquiera conocimientos generales sobre la estructura y los patrones de los datos, los cuales luego pueden ser reutilizados en tareas concretas mediante técnicas como *fine-tuning* (ajuste fino). Los modelos preentrenados constituyen una gran innovación, lo que mejora la accesibilidad, precisión, eficiencia computacional y velocidad del pronóstico.

Dado que los modelos de lenguaje de texto utilizan diccionarios de *tokens*, que son segmentos de caracteres representados vectorialmente según ciertos parámetros, es necesario tokenizar los valores de la serie temporal. El diccionario de *tokens* con el que operan los modelos de lenguaje no es infinito, por lo tanto es necesario proyectar las observaciones a un set finito de *tokens*. Para cumplir esto, Chronos escala y discretiza las observaciones. TimeGTP por su parte usa las mismas observaciones como *tokens*, esto dado que, si bien su arquitectura es la de un modelo transformador, esta no está basada en ningún modelo de lenguaje existente, y en cambio trabaja con un modelo especializado en series de tiempo entrenado para minimizar el error de pronóstico.

Para el escalado se aplica a las observaciones una transformación del tipo $f(x_i) = (x_i-m)/s$. Existen variadas técnicas de escalado eligiendo apropiadamente $m$ y $s$, pero se opta por elegir $m=0$ y $s = \frac{1}{C}\sum^C_{i=i}|x_i|$ debido a que preserva los valores iguales a cero, los cuáles pueden ser importante de destacar en numerosas aplicaciones.

Sin embargo, estos valores siguen siendo números reales y no pueden ser procesados directamente por un modelo de lenguaje. Es por esto que se discretizan las observaciones. Se seleccionan $B$ centros de intervalos en la recta real, $c_1, c_2, ..., c_B$, y $B-1$ extremos $b_i$ que los separen, $c_i < b_i < c_{i+1}$ para $i \in \{1,...,B-1 \}$. Las funciones de discretización $q:\Re \rightarrow \{1,2,...,B\}$, y de descuantificación $d: \{1,2,...,B\} \rightarrow \Re$ se definen como:

$$
q(x)= \left \{ \begin{matrix} 1 \ \ \ \ \ \ \ \ \text{si} \ \ -\infty \leq x < b_1 \hfill \\ 
2 \ \ \ \ \ \ \ \ \text{si} \ \ b_1 \leq x < b_2 \hfill \\
 \ \vdots \hfill \\
B \ \ \ \ \ \ \ \ \text{si} \ \ b_{B-1} \leq x < \infty \hfill \end{matrix} \right. \hspace{2, cm} \text{y} \hspace{2, cm} d(j) = c_j
$${#eq-10}

Una vez se hayan transformado las observaciones para poder ser leídas por el modelo, el funcionamiento de un transformador es el siguiente: 

![Diagrama de la estructura del modelo transformador de TimeGPT](../Imgs/modelo_timegpt.png){#fig-modelo}

**Codificador**

1. Representación vectorial: Cada *token* es transformado en un vector (vector de entrada) con muchas dimensiones ($\vec E$). Las dimensiones corresponden a diferentes características que el modelo definió en el preentrenado con una numerosa cantidad de parámetros.

2. Codificación posicional: Un set de valores adicionales o vectores son añadidos a los vectores de entrada antes de alimentarlos al modelo ($\vec E \Leftarrow \vec E + \vec P$). Estas codificaciones posicionales tienen patrones específicos que agregan la información posicional del token.

3. Atención multi-cabezal (del inglés *Multi-Head Attention*): La autoatención opera en múltiples 'cabezales de atención' para capturar los diferentes tipos de relaciones entre *tokens*. Una cabeza de atención verifica el contexto en el que se presenta el token, y manipula los valores del vector que lo representa para añadir esta información contextual.

  La verificación del contexto funciona gracias a una matriz denominada *Query* ($W_Q$) que examina ciertas características definidas con anterioridad en el preentrenado. El vector de entrada ($\vec E$) es multiplicado por esta matriz, resultando en un vector de consultas ($\vec Q$) para cada token. Una matriz de claves ($W_K$) que comprueba las relaciones con las características en la matriz de consultas, y tiene las mismas dimensiones que esta, es también post-multiplicada por $\vec E$ generando así el vector de claves ($\vec K$). Luego, se forma una nueva matriz a partir de los productos cruzados entre los vectores $\vec K$ y $\vec Q$ de cada token, se divide por la raíz de la dimensión de los vectores^[Es útil para mantener estabilidad numérica, la cual describe cómo los errores en los datos de entrada se propagan a través del algoritmo. En un método estable, los errores debidos a las aproximaciones se atenúan a medida que la computación procede.] ($\sqrt{d_k}$) y se normaliza con softmax^[ $softmax(x) = \frac{e^{x_i/t}}{\sum_j e^{x_j/t}}$] por columna^[Aplicar softmax hace que cada columna se comporte como una distribución de probabilidad.] ($\vec S$). Valores altos indican que un token (de las columnas) esta siendo influenciado por el comportamiento de otro token (de las filas).

![Verificación del contexto](../Imgs/attention_matrix.png)

  Una vez conocido que *tokens* son relevantes para otros *tokens*, es necesario saber de que forma son afectados. Una matriz de valores ($W_V$) es post-multiplicada por cada vector de entrada resultando en los vectores de valor ($\vec V$), los cuales son son multiplicados a cada columna. La suma por filas devuelven el vector $\Delta \vec E$ que se suma al vector de entrada original de cada token.

![Influencia de *tokens* sobre otros luego de aplicar softmax](../Imgs/attention_matrix_softmax.png)

$$
\Delta \vec E_j = \sum_i S_j \cdot \vec V_i
$${#eq-11}

  Con múltiples 'cabezas', cada una con sus propias matrices $W_K$, $W_Q$ y $W_V$, se generan múltiples $\Delta \vec E$ que se suman y se añaden al vector de entrada original. 

$$
\vec E \Leftarrow \vec E+ \sum_h \Delta \vec E_h
$${#eq-12}

  Todo el mecanismo de atención se puede resumir con la siguiente función:

$$
\text{Atencion}(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$${#eq-13}

  Es importante notar que todas las matrices $Q$, $K$ y $V$ son preentrenadas.


4. Suma y normalizado: Este paso hace referencia a la ecuación @eq-12, donde, en lugar de simplemente pasar los datos por las capas y modificar directamente el vector, las conexiones residuales se añaden sobre el vector de entrada en la salida de cada capa.

  Dado que las redes neuronales profundas sufren de inestabilidad en los pesos al actualizarlos, una normalización al vector estabiliza el entrenamiento y mejora la convergencia. 

5. Red neuronal convolucional (CNN): A diferencia de otros modelos transformadores tradicionales, TimeGPT incorpora *CNN*s para descubrir dependencias locales y patrones de corto plazo, tratando de minimizar una función de costo. *Chronos*, por otro lado, utiliza una *Feed-Forward Network*. 

**Decodificador**

1. Representación vectorial (desplazado hacia la derecha): La entrada del decodificador son los *tokens* desplazados hacia la derecha.

2. Codificación posicional

3. *Masked multi-head attention* (Atención multicabezal enmascarada): Las predicciones deben realizarse únicamente con los valores previos a cada token. Como consecuencia, antes de aplicar la transformación softmax, se debe reemplazar todos los valores debajo de la diagonal principal de la matriz $QK$ por $-\infty$. Esto evita que los *tokens* sean influenciados por *tokens* anteriores.

4. *Multi-head attention*: Se usan las matrices de claves y valores que da como salida el codificador, y la matriz de consultas es la salida de la capa de atención multicabezal enmascarada.

5. Conexión lineal: Es una capa completamente conectada que traduce las representaciones de atributos aprendidas en predicciones relevantes.


### 3.4.4 Diferencias entre TimeGPT y Chronos

TimeGPT es un modelo transformador preentrenado (de aquí las siglas GPT, *generative pre-trained transformer*) para el pronóstico de series de tiempo. Puede producir predicciones en diversas áreas y aplicaciones con gran precisión, sin necesidad de entrenamiento adicional. El mismo fue desarrollado por Nixtla y tuvo su primera beta privada en Agosto de 2023, volviéndose accesible a todo público desde el 18 de julio de 2024, sin embargo es de código cerrado.

Chronos es una familia de modelos transformadores preentrenados para series de tiempo basados en arquitecturas de modelos de lenguaje. Fue publicado por Amazon en marzo de 2024 y su código es de libre acceso.

Nixtla desarrolló para TimeGPT un modelo con arquitectura *transformer* que puede trabajar directamente con series de tiempo. Por otro lado, Chronos transforma los datos de las series para poder alimentarlos a los modelos de lenguaje existentes. Dado que al tranformar los valores los datos se vuelven discretos, Chronos busca minimizar la entropía cruzada entre las distribuciones de las categorías reales contra las predichas.

La función de pérdida utilizada por Chronos está dada por:
$$
\ell(\theta) = \sum^{h+1}_{l=1} \sum^{|\nu_{ts}|}_{i=1} \mathbfcal{1}_{z_{n+l+1}=i} log \ \mathcal{p}_\theta(z_{n+l+1}=i| z_{1:n+l})
$${#eq-14}

Donde $|\nu_{ts}|$ es el tamaño del diccionario de *tokens*, el cual depende del número de intervalos creados. $z_{n+h+1}$ es la serie transformada en *tokens*, cuyas primeras $n$ observaciones se utilizan como entrenamiento para pronosticar las siguientes $h$, y se agrega al final un token $\texttt{EOS}$ que se utiliza comúnmente en los modelos de lenguaje para denotar el final de la secuencia. $\mathcal{p}_\theta$ es la probabilidad estimada por el modelo bajo la parametrización $\theta$.

Es importante notar que no es una función que detecta distancias, por lo que se espera que el modelo asocie a los intervalos cercanos gracias a la información en el conjunto de entrenamiento. Es decir, Chronos aplica regresión por clasificación.

Otra diferencia entre TimeGPT y Chronos es el tipo de red neuronal que utilizan para detectar patrones en los datos. Mientras que el primero hace uso de las CNN, el segundo aplica *Feed-Forward Networks*. Luego de la conexión lineal, Chronos necesita volver a aplicar softmax para obtener las probabilidades del pronóstico, procedimiento que no es necesario por parte de TimeGPT.   

## 3.5 Métricas de evaluación


Para comparar el rendimiento de los modelos se utilizan métricas cuantitativas. Para los pronósticos puntuales se usará el porcentaje del error absoluto medio (MAPE), mientras que para los pronósticos probabilísticos se aplicará el *Interval Score*, propuesto por Gneiting y Raftery (2007), que penaliza tanto la amplitud de los intervalos como la falta de cobertura. Estas comparaciones permiten evaluar la precisión, la robustez y la eficiencia de cada enfoque.

Sea $e_l = z_{n+l} - \hat z_n(l)$ el error de la l-ésima predicción, donde $\hat z_n(l)$ representa el pronóstico $l$ pasos hacia adelante, algunas medidas del error para pronósticos $h$ pasos hacia adelante son:

- Error Cuadrático Medio (*Mean Square Error, MSE*):

$$
MSE = \frac{1}{h}\sum^h_{l=1}e_l^2
$${#eq-15}

- Error absoluto medio (*Mean Absolute Error, MAE*):

$$
MAE = \frac{1}{h}\sum^h_{l=1}|e_l|
$${#eq-16}

- Porcentaje del error absoluto medio (*Mean Absolute Percentage Error, MAPE*)

$$
MAPE = (\frac{1}{h}\sum^h_{l=1}|\frac{e_l}{Z_{n+l}}|)\cdot 100 \%
$${#eq-17}

El problema de estos errores es que solo tienen en cuenta la estimación puntual, y por lo general, es buena idea trabajar con pronósticos probabilísticos para cuantificar la incertidumbre de los valores futuros de la variable. Gneiting y Raftery (2007, JASA) propusieron en [*Strictly Proper Scoring Rules, Prediction, and Estimation*](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf) una nueva medida del error que tiene en cuenta los intervalos probabilísticos de la estimación, llamándola *Interval Score*:

$$
S = \frac{1}{h}\sum_{l=1}^h (W_l + O_l + U_l) 
$${#eq-18}

Donde:

$$
W_l = IS_l - II_l \hspace{1, cm}
$${#eq-19}

$$
O_l = \left \{ \begin{matrix} \frac{2}{\alpha}(Z_n(l) - Z_{n+l}) \hspace{10,mm} \text{si } Z_n(l) > Z_{n+l} \\ 0 \hspace{1,cm} \text{en otro caso} \hfill \end{matrix} \right. \hspace{1, cm} U_l = \left \{ \begin{matrix} \frac{2}{\alpha}(Z_{n+l} - Z_n(l)) \hspace{10,mm} \text{si } Z_n(l) < Z_{n+l} \\ 0 \hspace{1,cm} \text{en otro caso} \hfill \end{matrix} \right.
$${#eq-20}

Siendo $IS_l$ e $II_l$ los extremos superior e inferior del intervalo del l-ésimo pronóstico respectivamente. Es fácil darse cuenta que $W$ es una penalización por el ancho del intervalo, y que $O$ y $U$ son penalizaciones por sobre y subestimación respectivamente.

Aquel modelo que minimice el MAPE y el *Interval Score* será considerado el mejor.

## 3.6 Selección de parámetros y validación del modelo

La correcta elección de los parámetros del modelo constituye una de las tareas más importantes para lograr un buen ajuste de los datos. No resulta conveniente utilizar todos los datos disponibles para este fin, ya que esto puede conducir a un sobreajuste (*overfitting*). Se considera que un modelo presenta sobreajuste cuando se ajusta en exceso a los datos de entrenamiento, comprometiendo su capacidad de generalización frente a datos no observados. Para evitar este problema se aplican técnicas específicas de validación que permiten seleccionar los parámetros sin comprometer la capacidad predictiva del modelo.

La validación *holdout* consiste en reservar una parte del conjunto de entrenamiento como validación. Se ajustan las distintas configuraciones de parámetros sobre el resto de los datos de entrenamiento, y se prueban las métricas de evaluación sobre el conjunto reservado para validar. Luego, se ajusta el modelo con la mejor combinación de parámetros utilizando los datos de entrenamiento y de validación. Debido a la ordinalidad de los datos, las observaciones del conjunto de validación deben ser posteriores a las del conjunto de entrenamiento.

$$
\text{Conjunto de entrenamiento total : } \{\underbrace{z_1, ...,  z_c}_{\text{Entrenamiento}}, \underbrace{z_{c+1}, ..., z_n}_{\text{Validación}} \}
$${#eq-21}

En series que presentan estacionalidad, se prioriza que el conjunto de validación cubra al menos un ciclo. Esto tiene el objetivo de poder evaluar el ajuste del modelo en todo el ciclo estacional.

Exclusivamente para los modelos de la familia ARIMA, se utiliza el método de Box-Jenkins. El método consiste en comparar aquellos modelos que cumplan los supuestos, y elegir como mejor combinación de parámetros aquella que minimize el AIC (*Akaike Information Criterion*), medida de ajuste que penaliza por la cantidad de parámetros.

\newpage

# 4. Aplicación

La aplicación empírica de esta tesina tiene como objetivo implementar, ajustar y comparar los modelos presentados en la [sección 3](#metodologia), empleando un conjunto de series temporales seleccionadas. Para este fin, se utilizó el lenguaje de programación `Python`, junto con librerías de código abierto ampliamente reconocidas.

Se trabajó con series temporales reales, obtenidas de fuentes públicas y confiables. Cada serie fue analizada desde tres enfoques metodológicos: modelos estadísticos tradicionales, algoritmos de aprendizaje automático y modelos de aprendizaje profundo. En cada caso se exploraron distintas configuraciones de parámetros, explicando su significado y función en el ajuste.


  - Los modelos estadísticos clásicos (ARIMA y SARIMA) se ajustaron mediante la librería `pmdarima`. La selección de parámetros se efectuó tanto de manera manual como automática, empleando como criterio principal el Criterio de Información de Akaike (AIC).

  - Para el enfoque de aprendizaje automático, se emplearon algoritmos de boosting, específicamente XGBoost y LightGBM, implementados con las librerías `xgboost` y `lightgbm` respectivamente.

  - En el caso de los modelos de aprendizaje profundo, se entrenaron redes LSTM utilizando la librería `scalecast`.

  - Finalmente, se exploraron dos modelos fundacionales preentrenados: TimeGPT, accedido a través de la API de Nixtla mediante la librería `nixtla`, y Chronos, una familia de modelos preentrenados desarrollada por *Amazon Web Services*, cuya implementación se llevó a cabo con la librería `autogluon`.

Para cada modelo se generaron pronósticos puntuales y probabilísticos, con un nivel de confianza del 80%. Su desempeño se evaluó apartir de dos métricas: el *Mean Absolute Percentage Error* (MAPE) y el *Interval Score*. La elección de la mejor combinación de hiperparámetros se realizó en función del MAPE, con excepción de los modelos ARIMA. Los resultados fueron sintetizados en tablas comparativas y visualizaciones gráficas, acompañadas de un análisis crítico.

## 4.1 Series analizadas

```{python}
# TEMP

atenciones_guardia = pd.read_excel(io='../Datos/Atenciones de guardia en el HNVV por patologías respiratorias (vigiladas por epidemiología).xlsx' )

# Aseguro que la columna fecha tenga el formato adecuado
atenciones_guardia['fec'] = pd.to_datetime(atenciones_guardia['fec'], format='%Y-%m-%d')

# Filtro las columnas importantes y las renombro
atenciones_guardia = atenciones_guardia[['fec', 'frec']]
atenciones_guardia.columns = ['ds','y']

# --------------------------------------------------------------------
# Cargamos los datos
trabajadores = pd.read_excel(io='../Datos/trabajoregistrado_2502_estadisticas.xlsx', sheet_name= 'A.2.1', thousands='.', decimal=',', header=1, usecols='A,M', skipfooter=5, skiprows=84)

# Renombramos las columnas
trabajadores.columns = ['ds', 'y']

# Asignamos formato fecha
meses = {
    'ene': '01', 'feb': '02', 'mar': '03', 'abr': '04', 
    'may': '05', 'jun': '06', 'jul': '07', 'ago': '08', 
    'sep': '09', 'oct': '10', 'nov': '11', 'dic': '12'
}

trabajadores['ds'] = trabajadores['ds'].str.replace('*','')
trabajadores['ds'] = trabajadores['ds'].apply(
    lambda x: '01-' + x.replace(x.split('-')[0], meses.get(x.split('-')[0].lower(), '')).replace(x.split('-')[1], '20' + x.split('-')[1])
)

trabajadores['ds'] = pd.to_datetime(trabajadores['ds'], format='%d-%m-%Y')

# Eliminamos los datos del 2025 para tener solo años completos
trabajadores = trabajadores[trabajadores['ds'].dt.year != 2025]

# --------------------------------------------------------------------

import glob

# Cargamos todos los archivos txt
ruta = glob.glob('../Datos/Datos meteorologicos/*.txt')
tiempo_region = pd.concat([pd.read_fwf(f, skiprows=[1], dtype={'FECHA' : str, 'HORA': str} , encoding='cp1252') for f in ruta], ignore_index=True)

# Filtramos los datos de rosario
tiempo_rosario = tiempo_region[tiempo_region['NOMBRE'] == 'ROSARIO AERO']

# Creamos la columna fecha y hora
tiempo_rosario['HORA'] = tiempo_rosario['HORA'].apply(lambda x: '0' + x if len(x) != 2 else x)
tiempo_rosario.loc[:,'ds'] = tiempo_rosario['FECHA'].apply(lambda x: x[0:2] + '-' + x[2:4] + '-' + x[4:len(x)])
tiempo_rosario['ds'] = pd.to_datetime(tiempo_rosario['ds'] + ' ' + tiempo_rosario['HORA'], format='%d-%m-%Y %H')

# Nos quedamos con las columnas utiles y renombramos la respuesta
tiempo_rosario = tiempo_rosario[['ds', 'TEMP', 'HUM', 'PNM']]
tiempo_rosario.columns = ['ds', 'y', 'HUM', 'PNM'] # % de Humedad y Presion a nivel del mar en hectopascales 

```

```{python}
# Creamos datasets con la info que conocemos de las series
atenciones_trunc = atenciones_guardia.head(len(atenciones_guardia)-12).copy()
trabajadores_trunc = trabajadores.head(len(trabajadores)-12).copy()
temperatura_trunc = tiempo_rosario.head(len(tiempo_rosario)-24).copy()
```

Con el propósito de evaluar el desempeño de los modelos bajo diferentes condiciones, se seleccionaron tres series temporales con características heterogéneas:

- Atenciones de guardia mensuales por patologías respiratorias en un hospital de la ciudad de Rosario.

- Número mensual de personas registradas con empleo asalariado en el sector educativo privado de Argentina.

- Temperatura horaria en la ciudad de Rosario.

La primera serie corresponde al número mensual de atenciones de guardia por patologías respiratorias (Códigos CIE10: J09–J18, J21, J22 y J44) en el Hospital de Niños Víctor J. Vilela de la ciudad de Rosario. La información fue provista por la Dirección General de Estadística de la Municipalidad de Rosario^[**Dirección General de Estadística**. (s.f.) *Situación epidemiológica de
problemas de salud priorizados*. Municipalidad de Rosario.]. La serie presenta una marcada estacionalidad, con picos en los meses de invierno y una disminución significativa en el año 2020, atribuida a las medidas sanitarias adoptadas durante la pandemia de COVID-19. Este patrón estacional se aprecia con mayor claridad en el gráfico @fig-atenciones_estacionalidad del anexo.

```{python}
# Guardo la imagen para poder usarla en la presentación

(ggplot(atenciones_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Atenciones")
  ).save("../Imgs/plotnine/atenciones.png", width=6, height=4, dpi=700)
```

```{python}
#| fig-cap: "Atenciones en guardia por enfermedades respiratorias"

(ggplot(atenciones_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Atenciones")
  )
```

La segunda serie corresponde a la cantidad mensual de personas con empleo asalariado en el área de enseñanza, registradas en el sector privado en Argentina. La serie presenta una tendencia creciente a lo largo del tiempo, con descensos marcados en diciembre y enero, que se visualizan con el gráfico @fig-trabajadores_estacionalidad del anexo. Asimismo, se observan impactos atribuibles a la pandemia de COVID-19. Los datos provienen del informe Situación y evolución del Trabajo Registrado de la Secretaría de Trabajo, Empleo y Seguridad Social^[**Secretaría de Trabajo, Empleo y Seguridad Social**. (2025). *Situación y evolución del Trabajo Registrado*. [https://www.argentina.gob.ar/trabajo/estadisticas/situacion-y-evolucion-del-trabajo-registrado](https://www.argentina.gob.ar/trabajo/estadisticas/situacion-y-evolucion-del-trabajo-registrado)].

```{python}
(ggplot(trabajadores_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#5299CB", size = 0.3) +
  geom_line(color = "#5299CB") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Trabajadores (miles)")
  ).save("../Imgs/plotnine/trabajadores.png", width=6, height=4, dpi=700)
```

```{python}
#| fig-cap: "Personas con empleo asalariado en el área de enseñanza y registradas en el sector privado"
#| label: fig-trabajadores

(ggplot(trabajadores_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#5299CB", size = 0.3) +
  geom_line(color = "#5299CB") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Trabajadores (miles)")
  )
```

Por último, se analizaron las temperaturas horarias en Rosario correspondientes a los primeros días de marzo de 2025, considerando su relación con la humedad relativa y la presión atmosférica estándar. El patrón estacional diario se aprecia claramente en el gráfico @fig-temp_estacional del anexo: la temperatura se mantiene relativamente estable entre la noche y la mañana, y aumenta de forma pronunciada hacia la tarde. Los datos fueron relevados a partir de la página del Servicio Meteorológico Nacional^[**Servicio Meteorológico Nacional**. (s.f.). *Datos horarios*. [https://www.smn.gob.ar/descarga-de-datos](https://www.smn.gob.ar/descarga-de-datos)].


```{python}

(ggplot(temperatura_trunc) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#E44E58", size = 0.3) +
  geom_line(color = "#E44E58") +
  scale_x_date(date_labels = "%d", date_breaks = "1 day") +
  labs(x = "Día", y = "Temperatura (Cº)") +
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 8),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/temperatura1.png", width=6, height=4/3.1, dpi=700)

(ggplot(temperatura_trunc) +
  aes(x = 'ds', y  = 'HUM') + 
  geom_point(color = "black", size = 0.3) +
  geom_line(color = "black") +
  scale_x_date(date_labels = "%d", date_breaks = "1 day") +
  labs(x = "Día", y = "Humedad (%)") +
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/temperatura2.png", width=6, height=4/3.1, dpi=700)

(ggplot(temperatura_trunc) +
  aes(x = 'ds', y  = 'PNM') + 
  geom_point(color = "black", size = 0.3) +
  geom_line(color = "black") +
  scale_x_date(date_labels = "%d", date_breaks = "1 day") +
  labs(x = "Día", y = "Pres. atm. (hPa)") +
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 8),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/temperatura3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-temp layout-nrow=3}

![](../Imgs/plotnine/temperatura1.png)

![](../Imgs/plotnine/temperatura2.png)

![](../Imgs/plotnine/temperatura3.png)

Temperatura (Cº), humedad (%) y presión atmosférica (hPa) en Rosario por hora.

:::

Es importante señalar que, en este análisis, tanto la humedad como la presión atmosférica se suponen conocidas en las horas a pronosticar. Si bien esta condición no refleja una situación realista, en algunos contextos ciertas variables exógenas pueden disponerse con antelación. La inclusión de esta serie tuvo como finalidad evaluar la capacidad de los modelos para explotar información adicional. Cabe destacar que algunos de los modelos analizados permiten incorporar variables exógenas sin requerir el conocimiento de sus valores futuros. No obstante, para asegurar la comparabilidad de los resultados, en este trabajo se asumió que tanto la humedad como la presión atmosférica eran conocidas a futuro en todos los casos.

## 4.2 Ajuste y evaluación de modelos

```{python}

# Primero cargamos los ambientes donde tenemos todos los resultados

globals().update(load_env('../Codigo/Ambiente/Amb_Aplicacion.pkl'))
globals().update(load_env('../Codigo/Ambiente/Amb_Aplicacion_chronos.pkl'))

# Guardamos los resultados de Chronos en las metricas
metricas_1.loc[len(metricas_1)] = ['Chronos', 3, resultados_1_chronos3['mape'], resultados_1_chronos3['score'], resultados_1_chronos3['tiempo']]
metricas_1.loc[len(metricas_1)] = ['Chronos', 6, resultados_1_chronos6['mape'], resultados_1_chronos6['score'], resultados_1_chronos6['tiempo']]
metricas_1.loc[len(metricas_1)] = ['Chronos', 12, resultados_1_chronos['mape'], resultados_1_chronos['score'], resultados_1_chronos['tiempo']]

metricas_2.loc[len(metricas_2)] = ['Chronos', 3, resultados_2_chronos3['mape'], resultados_2_chronos3['score'], resultados_2_chronos3['tiempo']]
metricas_2.loc[len(metricas_2)] = ['Chronos', 6, resultados_2_chronos6['mape'], resultados_2_chronos6['score'], resultados_2_chronos6['tiempo']]
metricas_2.loc[len(metricas_2)] = ['Chronos', 12, resultados_2_chronos['mape'], resultados_2_chronos['score'], resultados_2_chronos['tiempo']]

metricas_3.loc[len(metricas_3)] = ['Chronos', 6, resultados_3_chronos6['mape'], resultados_3_chronos6['score'], resultados_3_chronos6['tiempo']]
metricas_3.loc[len(metricas_3)] = ['Chronos', 12, resultados_3_chronos12['mape'], resultados_3_chronos12['score'], resultados_3_chronos12['tiempo']]
metricas_3.loc[len(metricas_3)] = ['Chronos', 24, resultados_3_chronos['mape'], resultados_3_chronos['score'], resultados_3_chronos['tiempo']]
```

### 4.2.1 Modelización con ARIMA y SARIMAX
 
Debido a la complejidad que implica modelar series de alta frecuencia con variables exógenas mediante ARIMA, los únicos modelos que se probaron para la serie de temperatura son el obtenido a partir del método automático de selección de modelos, descrito más adelante, y correcciones del mismos.

Previo a proponer modelos, es importante realizar un breve análisis exploratorio sobre las series. El objetivo de este análisis es verificar que estas sean estacionarias, lo cual implica identificar y eliminar posibles tendencias o patrones sobre la variancia y/o media de las series. 

Como primer instancia, se realizan gráficos de cajas con el propósito de verificar que la variancia en cada período se mantenga constante. De lo contrario, se debe aplicar una transformación a la variable de interés.

```{python}

(
  ggplot(atenciones_trunc) +
  aes(x = atenciones_trunc["ds"].dt.year, y = atenciones_trunc["y"], group = atenciones_trunc["ds"].dt.year) +
  geom_boxplot(fill = "#6BC78A") +
  labs(x = "Año", y = "Atenciones") + 
  scale_x_continuous(breaks = range(min(atenciones_trunc["ds"].dt.year), max(atenciones_trunc["ds"].dt.year)+1))+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
).save("../Imgs/plotnine/boxplot1.png", width=6, height=4/2.7, dpi=700)


(
  ggplot(trabajadores_trunc) +
  aes(x = trabajadores_trunc["ds"].dt.year, y = trabajadores_trunc["y"], group = trabajadores_trunc["ds"].dt.year) +
  geom_boxplot(fill = "#5299CB") +
  labs(x = "Año", y = "Trabajadores (miles)") + 
  scale_x_continuous(breaks = range(min(trabajadores_trunc["ds"].dt.year), max(trabajadores_trunc["ds"].dt.year)+1))+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 8),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
).save("../Imgs/plotnine/boxplot2.png", width=6, height=4/2.7, dpi=700)

```


::: {#fig-box layout-ncol=1 fig-cap="Gráficos de caja por período."}

![Distribución del número de atenciones en guardia por año](../Imgs/plotnine/boxplot1.png){#fig-boxplot1}

![Distribución del número de trabajadores asalariados en el rubro de la enseñanza privada por año](../Imgs/plotnine/boxplot2.png){#fig-boxplot2}

<!-- Gráficos de caja por período. -->

:::

```{python}
# Transformacion de box y cox
atenciones_ajust, atenciones_lambda = stats.boxcox(atenciones_trunc["y"])
temperatura_ajust, temperatura_lambda = stats.boxcox(temperatura_trunc["y"])

atenciones_ajust = atenciones_trunc.copy()
atenciones_ajust['y'] = np.log(atenciones_trunc['y'])
```

Se observa en el gráfico @fig-boxplot1 ciertas diferencias en la variabilidad anual de las atenciones. Como consecuencia, se calcula $\lambda$ para la transformación de Box y Cox, resultando igual a  `{python} round(atenciones_lambda, 2)`. Con este resultado, se concluye que aplicar la transformación logaritmo a las atenciones en guardia por patologías respiratorias es necesario.

```{python}
(ggplot(atenciones_ajust) +
  aes(x = 'ds', y  = 'y') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Log(Atenciones)")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 9),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/atencionesLog.png", width=6, height=4/2.7, dpi=700)
```

![Logaritmo de las atenciones mensuales por guardia](../Imgs/plotnine/atencionesLog.png)

El siguiente paso es verificar que no se presenten patrones estacionales ni tendencias en la media de los datos. De presentarse ambas, el orden en el que se hagan las diferenciaciones no afecta el resultado. Sin embargo, si se aplica primero la diferenciación regular, el patrón estacional permanecerá presente; en cambio, al comenzar con la diferenciación estacional, es posible que la tendencia desaparezca simultáneamente.

La función de autocorrelacion muestral (FAM) y la función de autocorrelacion parcial muestral (FAPM) para la serie transformada de atenciones son las siguientes:

```{python}
autocorr_plot(atenciones_ajust['y'], lags=50).save("../Imgs/plotnine/autocorrAtenciones1.png", width=6, height=4/2.1, dpi=700)

autocorr_plot(atenciones_ajust['y'], lags=36, atype='pacf').save("../Imgs/plotnine/autocorrAtenciones2.png", width=6, height=4/2.1, dpi=700)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/autocorrAtenciones1.png)

![](../Imgs/plotnine/autocorrAtenciones2.png)

Autocorrelaciones del logaritmo de atenciones mensuales por guardia.
:::
```{python}
unit_root_atenciones = adfuller(atenciones_ajust['y'])
kpss_atenciones = kpss(atenciones_ajust['y'], regression='c')
```

En el primer gráfico se observa como las autocorrelaciones parciales descienden lentamente de forma sinusoidal, mostrando valores significativos cerca de los puntos estacionales. El segundo gráfico muestra una función de autocorrelación parcial con un valor significativo en el primer rezago, y luego en los rezagos 8 y 25. Con un *p-value* del `{python} round(unit_root_atenciones[1], 4)` no se rechaza la hipótesis nula del test aumentado de Dickey-Fuller, y por lo tanto no se puede rechazar la existencia de una raíz unitaria, lo que a su vez indica que la serie no es estacionaria.  

Para la serie de trabajadores las funciones de autocorrelación muestral son las siguientes:

```{python}

autocorr_plot(trabajadores_trunc['y'], lags=50).save("../Imgs/plotnine/autocorrTrabajadores1.png", width=6, height=4/2.1, dpi=700)

autocorr_plot(trabajadores_trunc['y'], lags=48, atype='pacf').save("../Imgs/plotnine/autocorrTrabajadores2.png", width=6, height=4/2.1, dpi=700)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/autocorrTrabajadores1.png)

![](../Imgs/plotnine/autocorrTrabajadores2.png)

Autocorrelaciones del número mensual de trabajadores asalariados en el área de enseñanza privada.
:::

```{python}
unit_root_trabajadores = adfuller(trabajadores_trunc['y'])
kpss_trabajadores = kpss(trabajadores_trunc['y'], regression='c')
```

Se observa nuevamente un descenso gradual de la FAM, exhibiendo valores significativos en los rezagos estacionales de la serie. El test de Dickey-Fuller rechaza la hipótesis nula con un *p-value* del `{python} round(unit_root_trabajadores[1], 4)`. Sin embargo, se observa en el gráfico @fig-trabajadores que existe una tendencia positiva en la media de la serie. Esto se confirma con el test de Kwiatkowski-Phillips-Schmidt-Shin, cuya hipótesis nula sostiene que la serie es estacionaria alrededor de una constante, y esta se rechaza con un *p-value* igual a `{python} round(kpss_trabajadores[1], 4)`. Estos resultados indican que una diferenciación en la parte regular es necesaria.

Dado que las series no presentan estacionariedad, se aplica una diferenciación estacional a la serie de atenciones y una regular a la de trabajadores.

```{python}

atenciones_ajust["y_sin_est"] = atenciones_ajust['y'].diff(12)
trabajadores_trunc["y_sin_est"] = trabajadores_trunc['y'].diff(1)

(ggplot(atenciones_ajust) +
  aes(x = 'ds', y  = 'y_sin_est') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Log(Atenciones) sin est.")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 8),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/diffe1.png", width=6, height=4/2.7, dpi=700)

(ggplot(trabajadores_trunc) +
  aes(x = 'ds', y  = 'y_sin_est') + 
  geom_point(color = "#5299CB", size = 0.3) +
  geom_line(color = "#5299CB") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Trabajadores (diferenciada)")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 7),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/diffe2.png", width=6, height=4/2.7, dpi=700)
  

```

::: {#fig-diffe layout-ncol=1 fig-cap="Series diferenciadas"}

![Logaritmo del número de atenciones en guardias sin estacionalidad](../Imgs/plotnine/diffe1.png){#fig-atenSinEst}

![Número de trabajadores asalariados en el rubro de la educación privada diferenciada](../Imgs/plotnine/diffe2.png){#fig-trabSinEst}

<!-- Logaritmo del número de atenciones en guardias y trabajadores asalariados diferenciados estacionalmente. -->
:::

```{python}
unit_root_atenciones_diff = adfuller(atenciones_ajust['y_sin_est'].dropna())
kpss_atenciones_diff = kpss(atenciones_ajust['y_sin_est'].dropna(), regression='c')

unit_root_trabajadores_diff = adfuller(trabajadores_trunc['y_sin_est'].dropna())
kpss_trabajadores_diff = kpss(trabajadores_trunc['y_sin_est'].dropna(), regression='c')
```

No se observa en la figura @fig-diffe ningún patrón en la media a través del tiempo. Esto se confirma con el test de Kwiatkowski-Phillips-Schmidt-Shin, que devuelve *p-values* mayores a 0.1 para las 2 series estacionarias.

Las FAM y FAPM para la serie del logaritmo de atenciones diferenciada estacionalmente son:

```{python}
autocorr_plot(atenciones_ajust['y_sin_est'], lags=50).save("../Imgs/plotnine/autocorrAtencionesDiff1.png", width=6, height=4/2.1, dpi=700)

autocorr_plot(atenciones_ajust['y_sin_est'], lags=30, atype='pacf').save("../Imgs/plotnine/autocorrAtencionesDiff2.png", width=6, height=4/2.1, dpi=700)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/autocorrAtencionesDiff1.png)

![](../Imgs/plotnine/autocorrAtencionesDiff2.png)

Autocorrelaciones del logaritmo de atenciones por guardia mensuales sin estacionalidad.
:::

La FAM desciende lentamente en los primeros rezagos y los rezagos estacionales son significativos. A su vez, en la FAPM se destacan el primer y decimotercer rezago. Estos resultados conllevan a proponer un modelo con una componente AR en la parte estacionaria y una componente MA en la estacional, formando el modelo $SARIMA(1,0,0)(0,1,1)_{12}$, el cual se lo identifica como AT-1. En la FAPM se ignoran los rezagos significativos 4, 5, y 18 porque no tienen sentido en el contexto del problema.

Para la serie del número de trabajadores diferenciada las funciones de autocorrelación son:

```{python}
autocorr_plot(trabajadores_trunc['y_sin_est'], lags=50).save("../Imgs/plotnine/autocorrTrabajadoresDiff1.png", width=6, height=4/2.1, dpi=700)

autocorr_plot(trabajadores_trunc['y_sin_est'], lags=42, atype='pacf').save("../Imgs/plotnine/autocorrTrabajadoresDiff2.png", width=6, height=4/2.1, dpi=700)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/autocorrTrabajadoresDiff1.png)

![](../Imgs/plotnine/autocorrTrabajadoresDiff2.png)

Autocorrelaciones del número mensual de trabajadores asalariados en el área de enseñanza privada sin estacionalidad.
:::

La FAM presenta valores significativos en todos los rezagos estacionales, los cuales descienden paulativamente. Este patrón es característico de una componente AR en la parte estacional. También son significativos el primer y segundo rezago, lo que puede indicar componentes AR(1) o MA(2) en la parte regular de la serie. En la FAPM se destacan los rezagos 1, 2, 11, 12 y 13. Con estas observaciones se proponen los modelos $SARIMA(1,1,0)(1,0,0)_{12}$ (TR-1) y $SARIMA(1,1,0)(1,0,1)_{12}$ (TR-2).

La función `auto_arima` de la librería `pmdarima` elige modelos de la familia ARIMA de forma automática. Utilizando esta herramienta se propone el modelo $SARIMA(0,1,0)(1,0,0)_{12}$ (AT-2) para la serie de atenciones en guardia, mientras que para la serie de trabajadores se selecciona el modelo $SARIMA(2,0,0)(2,1,0)_{12}$ (TR-3).

Los modelos ARIMA suponen que los residuos se comportan como ruido blanco, por lo que se deberá verificar que no presenten correlación y que se distribuyan de manera aproximadamente normal, con media y variancia constantes.

```{python}
resid_check(resultados_arima['resid_arima_AT1'], ds = atenciones_trunc['ds'], arima_df= resultados_arima['salida_arima_AT1'].shape[0]-2, time='%Y', name='resid_arima_AT1')
```

::: {#fig-pronarima2 layout-nrow=2}

![](../Imgs/plotnine/resid_arima_AT1_1.png)

![](../Imgs/plotnine/resid_arima_AT1_2.png)

![](../Imgs/plotnine/resid_arima_AT1_3.png)

![](../Imgs/plotnine/resid_arima_AT1_4.png)

Comprobación de supuestos del modelo AT-1 para la serie de atenciones.
:::

La figura @fig-pronarima2 muestra la comprobación de supuestos del modelo AT-1. En el gráfico superior izquierdo se presenta la distribución de los residuos estandarizados, acompañada con el *test* de Kolmogorov-Smirnov, que con un *p-value* de 0.0521 no rechaza la hipótesis nula que sostiene que la distribución es normal. El gráfico superior derecho muestra los residuos estandarizados a través del tiempo, con el objetivo de comprobar la ausencia de patrones sistemáticos. Para el modelo AT-1 esto se cumple y se destacan únicamente 2 *outlayers*. En los gráficos inferiores, de izquierda a derecha, se visualizan las funciones de autocorrelación y autocorrelación parcial de los residuos estandarizados, junto al *test* de Ljung-Box. Este *test* contrasta la hipótesis nula de que las correlaciones entre los valores no difieren significativamente de cero. El valor que se muestra en el gráfico es el menor *p-value* registrado, producto de haber realizado el *test* para los 30 primeros rezagos. Con un *p-value* menor a 0.0001 no se puede concluir que los residuos están incorrelacionados, por lo tanto no se comportan como ruido blanco y se debe descartar el modelo.

Se probaron varias correcciones al modelo sin lograr resultados satisfactorios. Debido a esto, se decide agregar una diferenciación en la parte regular.

```{python}

atenciones_ajust["y_diff"] = atenciones_ajust['y_sin_est'].diff(1)

(ggplot(atenciones_ajust) +
  aes(x = 'ds', y  = 'y_diff') + 
  geom_point(color = "#6BC78A", size = 0.3) +
  geom_line(color = "#6BC78A") +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +
  labs(x = "Año", y = "Log(Atenciones) sin est./ten.")+
  theme(plot_margin = 0,
        panel_grid_minor= element_blank(),
        
        axis_title=element_text(size = 7),
        axis_text= element_text(size = 7),
        legend_title=element_text(size = 8),
        legend_text= element_text(size = 6))
  ).save("../Imgs/plotnine/diff1.png", width=6, height=4/2.7, dpi=700)

```

::: {#fig-diff}

![](../Imgs/plotnine/diff1.png)

Logaritmo del número de atenciones en guardias sin estacionalidad ni tendencia.
:::

La FAM y la FAPM de esta serie son:

```{python}

autocorr_plot(atenciones_ajust["y_diff"], lags=25).save("../Imgs/plotnine/autocorr1.png", width=6, height=4/2.1, dpi=700)

autocorr_plot(atenciones_ajust["y_diff"], lags=25, atype='pacf').save("../Imgs/plotnine/autocorr2.png", width=6, height=4/2.1, dpi=700)
```

::: {#fig-pronarima1 layout-nrow=2}

![](../Imgs/plotnine/autocorr1.png)

![](../Imgs/plotnine/autocorr2.png)

Autocorrelaciones del logaritmo del número de atenciones sin estacionalidad ni tendencia.
:::

Se observan valores significativos en el cuarto y doceavo rezago, lo que podría significar que existen componentes MA en la parte estacional o regular de la serie. Se proponen entonces los modelos $SARIMA(0,1,1)(0,1,0)_{12}$ (AT-3) y $SARIMA(0,1,0)(0,1,1)_{12}$ (AT-4).

Para la serie de temperaturas en Rosario la función `auto_arima` propone el modelo $SARIMAX(1,1,1)(2,0,1)_{24}$ (TE-1). Sin embargo, habiendo analizado los coeficientes, presentes en la tabla @tbl-salida_te1 del anexo, se concluye que el modelo no goza de estacionariedad en parte estacional. Como correcciones, se postulan los modelos $SARIMAX(1,1,1)(1,0,1)_{24}$ (TE-2), $SARIMAX(1,1,0)(2,0,1)_{24}$ (TE-3) y $SARIMAX(1,1,0)(1,0,1)_{24}$ (TE-4), de los cuales únicamente el último cumple con todas las propiedades requeridas.

\begin{table}[H]
\centering
\caption{Cumplimiento de las condiciones de estacionariedad e invertibilidad de los modelos ajustados.}
\label{tab:condicionesEstacionariedad}
\begin{tabular}{|lclccccc|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{ID}} & \multicolumn{1}{c|}{\multirow{2}{*}{Modelo}}      & \multicolumn{1}{c|}{\multirow{2}{*}{AIC}}                                                                   & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Parámetros\\ significativos*\end{tabular}}} & \multicolumn{2}{c|}{Parte regular}                    & \multicolumn{2}{c|}{Parte estacional} \\ \cline{5-8} 
\multicolumn{1}{|l|}{}                    & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{}                                                                                       & \multicolumn{1}{c|}{}                                                                                      & \multicolumn{1}{c|}{Est.} & \multicolumn{1}{c|}{Inv.} & \multicolumn{1}{c|}{Est.}    & Inv.   \\ \hline
\multicolumn{8}{|c|}{Atenciones en guardia}                                                                                                                                                                                                                                                                                                                                                                              \\ \hline
\multicolumn{1}{|l|}{AT-1}                & \multicolumn{1}{c|}{$SARIMA(1,0,0)(0,1,1)_{12}$}  & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_AT1']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{Si}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{1}{|l|}{AT-2}                & \multicolumn{1}{c|}{$SARIMA(0,1,0)(1,0,0)_{12}$}  & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_atenciones_auto']['AIC'].iloc[0], 1)`}   & \multicolumn{1}{c|}{Si}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{1}{|l|}{AT-3}                & \multicolumn{1}{c|}{$SARIMA(0,1,1)(0,1,0)_{12}$}  & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_AT3']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{No}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{1}{|l|}{AT-4}                & \multicolumn{1}{c|}{$SARIMA(0,1,0)(0,1,1)_{12}$}  & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_AT4']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{Si}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{8}{|c|}{Trabajadores}                                                                                                                                                                                                                                                                                                                                                                                       \\ \hline
\multicolumn{1}{|l|}{TR-1}                & \multicolumn{1}{c|}{$SARIMA(1,1,0)(1,0,0)_{12}$}  & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_TR1']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{Si}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{1}{|l|}{TR-2}                & \multicolumn{1}{c|}{$SARIMA(1,1,0)(1,0,1)_{12}$}  & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_TR2']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{Si}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{1}{|l|}{TR-3}                & \multicolumn{1}{c|}{$SARIMA(2,0,0)(2,1,0)_{12}$}  & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_trabajadores_auto']['AIC'].iloc[0], 1)`} & \multicolumn{1}{c|}{Si}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{8}{|c|}{Temperatura}                                                                                                                                                                                                                                                                                                                                                                                        \\ \hline
\multicolumn{1}{|l|}{TE-1}                & \multicolumn{1}{c|}{$SARIMAX(1,1,1)(2,0,1)_{24}$} & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_temperatura_auto']['AIC'].iloc[0], 1)`}  & \multicolumn{1}{c|}{No}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{No}      & Si     \\ \hline
\multicolumn{1}{|l|}{TE-2}                & \multicolumn{1}{c|}{$SARIMAX(1,1,1)(1,0,1)_{24}$} & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_TE2']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{No}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{1}{|l|}{TE-3}                & \multicolumn{1}{c|}{$SARIMAX(1,1,0)(2,0,1)_{24}$} & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_TE3']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{No}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{1}{|l|}{TE-4}                & \multicolumn{1}{c|}{$SARIMAX(1,1,0)(1,0,1)_{24}$} & \multicolumn{1}{l|}{`{python} round(resultados_arima['salida_arima_TE4']['AIC'].iloc[0], 1)`}               & \multicolumn{1}{c|}{Si}                                                                                    & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}   & \multicolumn{1}{c|}{Si}      & Si     \\ \hline
\multicolumn{8}{|l|}{}                                                                                                                                                                     \\ \hline
\multicolumn{8}{|l|}{(*): Todos los parámetros del modelo deben ser significativos}              

\\ \hline                                                                                         
\end{tabular}
\end{table}

Con el propósito de no poblar de gráficos el documento, tanto las salidas como la comprobación de supuestos de todos los modelos se encuentran en los anexos [8.2](#salidas_arima) y [8.3](#supuestos_arima) respectivamente. La tabla \ref{tab:supuestosArima} resume la información de la comprobación de supuestos. 

\begin{table}[H]
\centering
\caption{Cumplimiento de los supuestos de los modelos ajustados.}
\label{tab:supuestosArima}
\begin{tabular}{|ccccc|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{ID}} & \multicolumn{1}{c|}{\multirow{2}{*}{Modelo}}      & \multicolumn{1}{c|}{\multirow{2}{*}{Normalidad}} & \multicolumn{1}{c|}{\multirow{2}{*}{Homocedasticidad}} & \multirow{2}{*}{Incorrelación} \\
\multicolumn{1}{|c|}{}                    & \multicolumn{1}{c|}{}                             & \multicolumn{1}{c|}{}                            & \multicolumn{1}{c|}{}                                  &                                \\ \hline
\multicolumn{5}{|c|}{Atenciones en guardia}                                                                                                                                                                                                \\ \hline
\multicolumn{1}{|c|}{AT-1}                & \multicolumn{1}{c|}{$SARIMA(1,0,0)(0,1,1)_{12}$}  & \multicolumn{1}{c|}{Si}                          & \multicolumn{1}{c|}{Si}                                & No                             \\ \hline
\multicolumn{1}{|c|}{AT-2}                & \multicolumn{1}{c|}{$SARIMA(0,1,0)(1,0,0)_{12}$}  & \multicolumn{1}{c|}{Si}                          & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{1}{|c|}{AT-3}                & \multicolumn{1}{c|}{$SARIMA(0,1,1)(0,1,0)_{12}$}  & \multicolumn{1}{c|}{Si}                          & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{1}{|c|}{AT-4}                & \multicolumn{1}{c|}{$SARIMA(0,1,0)(0,1,1)_{12}$}  & \multicolumn{1}{c|}{Si}                          & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{5}{|c|}{Trabajadores}                                                                                                                                                                                                         \\ \hline
\multicolumn{1}{|c|}{TR-1}                & \multicolumn{1}{c|}{$SARIMA(1,1,0)(1,0,0)_{12}$}  & \multicolumn{1}{c|}{No*}                         & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{1}{|c|}{TR-2}                & \multicolumn{1}{c|}{$SARIMA(1,1,0)(1,0,1)_{12}$}  & \multicolumn{1}{c|}{No*}                         & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{1}{|c|}{TR-3}                & \multicolumn{1}{c|}{$SARIMA(2,0,0)(2,1,0)_{12}$}  & \multicolumn{1}{c|}{No}                          & \multicolumn{1}{c|}{No}                                & No                             \\ \hline
\multicolumn{5}{|c|}{Temperatura}                                                                                                                                                                                                          \\ \hline
\multicolumn{1}{|c|}{TE-1}                & \multicolumn{1}{c|}{$SARIMAX(1,1,1)(2,0,1)_{24}$} & \multicolumn{1}{c|}{No*}                         & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{1}{|c|}{TE-2}                & \multicolumn{1}{c|}{$SARIMAX(1,1,1)(1,0,1)_{24}$} & \multicolumn{1}{c|}{No*}                         & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{1}{|c|}{TE-3}                & \multicolumn{1}{c|}{$SARIMAX(1,1,0)(2,0,1)_{24}$} & \multicolumn{1}{c|}{No*}                         & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{1}{|c|}{TE-4}                & \multicolumn{1}{c|}{$SARIMAX(1,1,0)(1,0,1)_{24}$} & \multicolumn{1}{c|}{No*}                         & \multicolumn{1}{c|}{Si}                                & Si                             \\ \hline
\multicolumn{5}{|l|}{}                                                                                                                                                                                                                     \\ \hline
\multicolumn{5}{|l|}{(*): Si bien se rechaza la normalidad, esto se puede deber a ciertos \emph{outlayers}.}                                                                                                                               \\ \hline
\end{tabular}
\end{table}

```{python}

plot_forecast(data = atenciones_guardia, forecast = resultados_arima['pred_AT4'], pred_color = '#6BC78A', line_color='black', label = 'SARIMA', long = 36, ylabel= 'Atenciones', legend_position = 'none').save("../Imgs/plotnine/pronAT4.png", width=6, height=4/2.7, dpi=700)

plot_forecast(data = atenciones_guardia, forecast = resultados_1_arima['pred'], pred_color = '#6BC78A', line_color='black', label = 'SARIMA', ylabel= 'Atenciones', legend_position = (0.3,0.98), long = 36).save("../Imgs/plotnine/pronAT2.png", width=6, height=4/2.7, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_arima['pred_TR1'], line_color='black', pred_color = '#5299CB', label = 'SARIMA', long = 36, ylabel= 'Trabajadores (miles)', legend_position = (0.3,0.98)).save("../Imgs/plotnine/pronTR1.png", width=6, height=4/2.7, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_arima['pred_TR2'], line_color='black', pred_color = '#5299CB', label = 'SARIMA', long = 36, ylabel= 'Trabajadores (miles)', legend_position = 'none').save("../Imgs/plotnine/pronTR2.png", width=6, height=4/2.7, dpi=700)

plot_forecast(data = tiempo_rosario[['ds','y']], forecast = resultados_arima['pred_TE4'], pred_color = '#E23C47', line_color='black', label = 'SARIMAX', long = 72, xlabel='Día', ylabel= 'Temperatura (Cº)', legend_position = (0.3,0.98)).save("../Imgs/plotnine/pronTE4.png", width=6, height=4/2.7, dpi=700)

```

A partir de los resultados presentados en las tablas \ref{tab:condicionesEstacionariedad} y \ref{tab:supuestosArima} los modelos candidatos seleccionados y con los que se pronostican las series son:

- Modelos AT-2 y AT-4 para la serie de atenciones

- Modelos TR-1 y TR-2 para la serie de trabajadores

- Modelo TE-4 para la serie de temperaturas.

```{python}
# CALCULO DE MAPES

# Atenciones guardia automatico
# Horizonte 3
mape_atenciones_auto_3 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], y_pred=resultados_1_arima['pred']['pred'].head(3))
# Horizonte 6
mape_atenciones_auto_6 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], y_pred=resultados_1_arima['pred']['pred'].head(6))

# Atenciones guardia modelo 2
mape_AT4 = mean_absolute_percentage_error(y_true=atenciones_guardia.tail(12)['y'], y_pred=resultados_arima['pred_AT4']['pred'])
# Horizonte 3
mape_AT4_3 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], y_pred=resultados_arima['pred_AT4']['pred'].head(3))
# Horizonte 6
mape_AT4_6 = mean_absolute_percentage_error(y_true=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], y_pred=resultados_arima['pred_AT4']['pred'].head(6))

# Trabajadores modelo 1
mape_TR1 = mean_absolute_percentage_error(y_true=trabajadores.tail(12)['y'], y_pred=resultados_arima['pred_TR1']['pred'])
# Horizonte 3
mape_TR1_3 = mean_absolute_percentage_error(y_true=(trabajadores.head(len(trabajadores)-9)).tail(3)['y'], y_pred=resultados_arima['pred_TR1']['pred'].head(3))
# Horizonte 6
mape_TR1_6 = mean_absolute_percentage_error(y_true=(trabajadores.head(len(trabajadores)-6)).tail(6)['y'], y_pred=resultados_arima['pred_TR1']['pred'].head(6))

# Trabajadores modelo 2
mape_TR2 = mean_absolute_percentage_error(y_true=trabajadores.tail(12)['y'], y_pred=resultados_arima['pred_TR2']['pred'])
# Horizonte 3
mape_TR2_3 = mean_absolute_percentage_error(y_true=(trabajadores.head(len(trabajadores)-9)).tail(3)['y'], y_pred=resultados_arima['pred_TR2']['pred'].head(3))
# Horizonte 6
mape_TR2_6 = mean_absolute_percentage_error(y_true=(trabajadores.head(len(trabajadores)-6)).tail(6)['y'], y_pred=resultados_arima['pred_TR2']['pred'].head(6))

# Temperatura modelo 4
mape_TE4 = mean_absolute_percentage_error(y_true=tiempo_rosario.tail(24)['y'], y_pred=resultados_arima['pred_TE4']['pred'])
# Horizonte 6
mape_TE4_6 = mean_absolute_percentage_error(y_true=(tiempo_rosario.head(len(tiempo_rosario)-18)).tail(6)['y'], y_pred=resultados_arima['pred_TE4']['pred'].head(6))
# Horizonte 12
mape_TE4_12 = mean_absolute_percentage_error(y_true=(tiempo_rosario.head(len(tiempo_rosario)-12)).tail(12)['y'], y_pred=resultados_arima['pred_TE4']['pred'].head(12))


# ----------------- CALCULO DE INTERVAL SCORES -----------------------
# Atenciones automatico
# Horizonte 3
is_atenciones_auto_3 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], lower = resultados_1_arima['pred']['lower'].head(3), upper = resultados_1_arima['pred']['upper'].head(3), alpha = 0.2)
# Horizonte 6
is_atenciones_auto_6 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], lower = resultados_1_arima['pred']['lower'].head(6), upper = resultados_1_arima['pred']['upper'].head(6), alpha = 0.2)

# Atenciones modelo 2
is_AT4 = interval_score(obs=atenciones_guardia.tail(12)['y'], lower = resultados_arima['pred_AT4']['lower'], upper = resultados_arima['pred_AT4']['upper'], alpha = 0.2)
# Horizonte 3
is_AT4_3 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-9)).tail(3)['y'], lower = resultados_arima['pred_AT4']['lower'].head(3), upper = resultados_arima['pred_AT4']['upper'].head(3), alpha = 0.2)
# Horizonte 6
is_AT4_6 = interval_score(obs=(atenciones_guardia.head(len(atenciones_guardia)-6)).tail(6)['y'], lower = resultados_arima['pred_AT4']['lower'].head(6), upper = resultados_arima['pred_AT4']['upper'].head(6), alpha = 0.2)

# Trabajadores modelo 1
is_TR1 = interval_score(obs=trabajadores.tail(12)['y'], lower = resultados_arima['pred_TR1']['lower'], upper = resultados_arima['pred_TR1']['upper'], alpha = 0.2)
# Horizonte 3
is_TR1_3 = interval_score(obs=(trabajadores.head(len(trabajadores)-9)).tail(3)['y'], lower = resultados_arima['pred_TR1']['pred'].head(3), upper = resultados_arima['pred_TR1']['pred'].head(3), alpha = 0.2)
# Horizonte 6
is_TR1_6 = interval_score(obs=(trabajadores.head(len(trabajadores)-6)).tail(6)['y'], lower = resultados_arima['pred_TR1']['lower'].head(6), upper = resultados_arima['pred_TR1']['upper'].head(6), alpha = 0.2)

# Trabajadores modelo 2
is_TR2 = interval_score(obs=trabajadores.tail(12)['y'], lower = resultados_arima['pred_TR2']['lower'], upper = resultados_arima['pred_TR2']['upper'], alpha = 0.2)
# Horizonte 3
is_TR2_3 = interval_score(obs=(trabajadores.head(len(trabajadores)-9)).tail(3)['y'], lower = resultados_arima['pred_TR2']['pred'].head(3), upper = resultados_arima['pred_TR2']['pred'].head(3), alpha = 0.2)
# Horizonte 6
is_TR2_6 = interval_score(obs=(trabajadores.head(len(trabajadores)-6)).tail(6)['y'], lower = resultados_arima['pred_TR2']['lower'].head(6), upper = resultados_arima['pred_TR2']['upper'].head(6), alpha = 0.2)


# Temperatura
is_TE4 = interval_score(obs=tiempo_rosario.tail(24)['y'], lower = resultados_arima['pred_TE4']['lower'], upper = resultados_arima['pred_TE4']['upper'], alpha = 0.2)
# Horizonte 6
is_TE4_6 = interval_score(obs=(tiempo_rosario.head(len(tiempo_rosario)-18)).tail(6)['y'], lower = resultados_arima['pred_TE4']['lower'].head(6), upper = resultados_arima['pred_TE4']['upper'].head(6), alpha = 0.2)
# Horizonte 12
is_TE4_12 = interval_score(obs=(tiempo_rosario.head(len(tiempo_rosario)-12)).tail(12)['y'], lower = resultados_arima['pred_TE4']['lower'].head(12), upper = resultados_arima['pred_TE4']['upper'].head(12), alpha = 0.2)




# -------------------------------------------------------
# Agregamos estas medidas a las tablas resumen
metricas_1.loc[len(metricas_1)] = np.NAN
metricas_1.loc[len(metricas_1)] = np.NAN
metricas_1 = metricas_1.shift().shift()
metricas_1.loc[0] = ['ARIMA', 3, mape_AT4_3, is_AT4_3, 0]
metricas_1.loc[1] = ['ARIMA', 6, mape_AT4_6, is_AT4_6, 0]
metricas_1.loc[2] = ['ARIMA', 12, mape_AT4, is_AT4, 0]

metricas_2.loc[len(metricas_2)] = np.NAN
metricas_2.loc[len(metricas_2)] = np.NAN
metricas_2 = metricas_2.shift().shift()
metricas_2.loc[0] = ['ARIMA', 3, mape_TR2_3, is_TR2_3, 0]
metricas_2.loc[1] = ['ARIMA', 6, mape_TR2_6, is_TR2_6, 0]
metricas_2.loc[2] = ['ARIMA', 12, mape_TR2, is_TR2, 0]

metricas_3.loc[len(metricas_3)] = np.NAN
metricas_3.loc[len(metricas_3)] = np.NAN
metricas_3 = metricas_3.shift().shift()
metricas_3.loc[0] = ['ARIMA', 6, mape_TE4_6, is_TE4_6, 0]
metricas_3.loc[1] = ['ARIMA', 12, mape_TE4_12, is_TE4_12, 0]
metricas_3.loc[2] = ['ARIMA', 24, mape_TE4, is_TE4, 0]
```

::: {#fig-pronarima1 layout-ncol=1 fig-cap="Pronósticos del número de atenciones en guardia por patologías respiratorias con SARIMA."}

![Pronósticos con el modelo AT-2](../Imgs/plotnine/pronAT2.png){fig-subcap="ARIMA(AT2)"}

![Pronósticos con el modelo AT-4](../Imgs/plotnine/pronAT4.png){fig-subcap="ARIMA(AT4)"}

<!-- Pronósticos del número de atenciones en guardia por patologías respiratorias con ARIMA. -->
:::

| ID   |Modelo                      | Horizonte | MAPE    | *Interval Score* |
|------|-----------------------------|-----------|---------|------------------|
|      |                             | 3         | `{python} round(mape_atenciones_auto_3, 4)`     | `{python} round(is_atenciones_auto_3, 4)`        |
| AT-2 | $SARIMA(0,1,0)(1,0,0)_{12}$ | 6         | `{python} round(mape_atenciones_auto_6, 4)`     | `{python} round(is_atenciones_auto_6, 4)`        |
|      |                             | 12        | `{python} round(resultados_1_arima['mape'], 4)` | `{python} round(resultados_1_arima['score'], 4)` |
|      |                             |           |                                                 |                                                  |
|      |                             | 3         | `{python} round(mape_AT4_3, 4)`                 | `{python} round(is_AT4_3, 4)`                    |
| AT-4 | $SARIMA(0,1,0)(0,1,1)_{12}$ | 6         | `{python} round(mape_AT4_6, 4)`                 | `{python} round(is_AT4_6, 4)`                    |
|      |                             | 12        | `{python} round(mape_AT4, 4)`                   | `{python} round(is_AT4, 4)`                      |
: Métricas de evaluación para la serie del número de atenciones en guardia por patologías respiratorias con SARIMA. {#tbl-arimaAtenciones}


Se observa en la tabla @tbl-arimaAtenciones que para la serie de atenciones en guardia por patologías respiratorias el modelo seleccionado de forma automática, identificado como AT-2, parece pronosticar mejor en los primeros 6 meses. Sin embargo, este no logra captar el aumento de atenciones en invierno, y es por esto que en el pronóstico a 12 meses el modelo AT-4 obtiene mejores resultados. Se puede destacar la alta incertidumbre del modelo gracias a la amplitud de los intervalos de confianza y los altos valores en los *Interval Scores*.

Un problema a destacar es que el intervalo de confianza para las atenciones por guardia toma valores negativos, algo ilógico para variables de conteo. Este problema no es único de ARIMA y se verá reflejado en los pronósticos de los demás modelos. Una solución es ajustar los modelos sobre el logaritmo de la variable, y luego transformando los valores pronosticados a la escala original. Esto queda propuesto para una futura extención.


::: {#fig-pronarimatr layout-ncol=1 fig-cap="Pronóstico del número de trabajadores asalariados en el rubro de la enseñanza privada con SARIMA."}

![Pronósticos con el modelo TR-1](../Imgs/plotnine/pronTR1.png){fig-subcap="ARIMA(TR1)"}

![Pronósticos con el modelo TR-2](../Imgs/plotnine/pronTR2.png){fig-subcap="ARIMA(TR2)"}

:::


| ID   |Modelo                      | Horizonte | MAPE    | *Interval Score* |
|------|-----------------------------|-----------|---------|------------------|
|      |                             | 3         | `{python} round(mape_TR1_3, 4)` | `{python} round(is_TR1_3, 4)` |
| TR-1 | $SARIMA(1,1,0)(1,0,0)_{12}$ | 6         | `{python} round(mape_TR1_6, 4)` | `{python} round(is_TR1_6, 4)` |
|      |                             | 12        | `{python} round(mape_TR1, 4)`   | `{python} round(is_TR1, 4)`   |
|      |                             |           |                                 |                               |
|      |                             | 3         | `{python} round(mape_TR2_3, 4)` | `{python} round(is_TR2_3, 4)` |
| TR-2 | $SARIMA(1,1,0)(1,0,1)_{12}$ | 6         | `{python} round(mape_TR2_6, 4)` | `{python} round(is_TR2_6, 4)` |
|      |                             | 12        | `{python} round(mape_TR2, 4)`   | `{python} round(is_TR2, 4)`   |
: Métricas de evaluación para la serie del número de trabajadores asalariados en el rubro de la enseñanza privada con SARIMA. {#tbl-arimaTrabajadores}

Los pronósticos de los modelos AT-1 y AT-2 sobre el número de trabajadores asalariados en el rubro de la enseñanza privada son cercanos a los valores reales de la serie. Ambos logran aprender correctamente el patrón de los datos, logrando a su vez medidas satisfactorias. El modelo AT-1 se destaca en el pronóstico a corto plazo mientras que el AT-2, cuyo AIC es menor, lo supera luego de los 3 primeros meses, logrando un MAPE del `{python} round(mape_TR2, 4)` pronosticando a largo plazo. Esto significa que la diferencia media absoluta entre los valores reales y los pronosticados es únicamente del `{python} round(mape_TR2*100, 4)`%. 

![Pronóstico de la temperatura por hora en la ciudad de Rosario con el modelo SARIMAX TE-4.](../Imgs/plotnine/pronTE4.png)

| ID   |Modelo                      | Horizonte | MAPE    | *Interval Score* |
|------|-----------------------------|-----------|---------|------------------|
|      |                             | 6       | `{python} round(mape_TE4_6, 4)` | `{python} round(is_TE4_6, 4)` |
| TE-4 | $SARIMA(1,1,0)(1,1,0)_{24}$ | 12      | `{python} round(mape_TE4_12, 4)` | `{python} round(is_TE4_12, 4)` |
|      |                             | 24      | `{python} round(mape_TE4, 4)`   | `{python} round(is_TE4, 4)`   |
: Métricas de evaluación para la serie de temperaturas por hora en la ciudad de Rosario con SARIMAX. {#tbl-arimaTemperatura}

La diferencia media absoluta entre los valores reales y los pronosticados es de apenas del `{python} round(is_TE4_6*100, 4)`% en el pronóstico del modelo TE-4 sobre las temperaturas en las primeras 6 horas. Sin embargo, esta diferencia aumenta en gran medida en los demás horizontes, al igual que la amplitud del intervalo de confianza, generando *Interval Scores* altos a medida que se aumenta el horizonte.


### 4.2.2 Modelos de aprendizaje automático

Los modelos de pronóstico basados en el aprendizaje automático no tienen supuestos que cumplir, por lo que el modelaje se vuelve sencillo y automatizable. Algunos aspectos importantes en lo que se debe tener especial cuidado y atención es en la selección de características y parámetros del modelo.

Tanto para XGBoost como para LightGBM las características elegidas para todas las series son las siguientes:

- Identificación temporal

- El promedio de las 3 observaciones anteriores 

- El desvío estándar de las 3 observaciones anteriores

- El valor del primer rezago

- El valor del segundo rezago

- El valor del rezago estacional

Con el objetivo de seleccionar los mejores hiperparámetros, se elaboró una grilla con diferentes combinaciones de valores para cada parámetro. Se ajustaron modelos con cada combinación en la grilla y se evaluaron sobre un conjunto de validación.

XGBoost y LightGBM permiten parametrizar los modelos de varias formas, los parámetros que se decidieron probar en este trabajo son los siguientes:

- Número de árboles que se contruyen paralelamente en cada iteración ($A$). Opciones: 20, 50, 100, 150.

- Profundidad máxima del árbol ($P$). Opciones: 2, 3, 4, 5.

- Número máximo de hojas del árbol ($H$). Opciones: 2, 4, 8, 16.

- Tasa de aprendizaje en el método del gradiente ($\eta$). Opciones: 0.001, 0.1, 0.2.

- Proporción de características que se usa en cada árbol ($C$). Opciones: 0.7, 1.

En las tablas @tbl-resultadosxgb y @tbl-resultadoslgbm se muestran para cada serie que combinación de parámetros, de las 384 posibles, fue la que menor MAPE produjo sobre el conjunto de validación, aplicando XGBoost o LightGBM respectivamente. Además, se presenta el MAPE e *Interval Score* que devuelve el modelo entrenado con todos los datos de entrenamiento y evaluado sobre el conjunto de prueba.

\newpage

| Serie                    | Hor. | $A$ | $P$ | $H$ | $\eta$  | $C$ | MAPE  | Interval Score |
|--------------------------|------|-----|-----|-----|-----|-----|-------|----------------|
|                          | 3    | `{python} resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_xgb3['grilla'][resultados_1_xgb3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_xgb3['mape'], 4)` | `{python} round(resultados_1_xgb3['score'], 4)` |
| Atenciones    | 6    | `{python} resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_xgb6['grilla'][resultados_1_xgb6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_xgb6['mape'], 4)` | `{python} round(resultados_1_xgb6['score'], 4)` |
|                          | 12   | `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_xgb['grilla'][resultados_1_xgb['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_xgb['mape'], 4)` | `{python} round(resultados_1_xgb['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 3    | `{python} resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`  | `{python} resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_xgb3['grilla'][resultados_2_xgb3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_xgb3['mape'], 4)` | `{python} round(resultados_2_xgb3['score'], 4)` |
| Trabajadores | 6    | `{python} resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_xgb6['grilla'][resultados_2_xgb6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_xgb6['mape'], 4)` | `{python} round(resultados_2_xgb6['score'], 4)` |
|                          | 12   | `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_xgb['grilla'][resultados_2_xgb['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_xgb['mape'], 4)` | `{python} round(resultados_2_xgb['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 6    | `{python} resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_xgb6['grilla'][resultados_3_xgb6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_xgb6['mape'], 4)` | `{python} round(resultados_3_xgb6['score'], 4)` |
| Temperatura              | 12   | `{python} resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]` | `{python} resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_xgb12['grilla'][resultados_3_xgb12['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_xgb12['mape'], 4)` | `{python} round(resultados_3_xgb12['score'], 4)` |
|                          | 24   | `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['max_leaves'].values[0]` | `{python} float(resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_xgb['grilla'][resultados_3_xgb['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_xgb['mape'], 4)` | `{python} round(resultados_3_xgb['score'], 4)` |
: Modelos XGBoost seleccionados y métricas de evaluación. {#tbl-resultadosxgb}

```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_xgb['pred'], pred_color = '#6BC78A', line_color='black', label = 'XGBoost', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/xgboost1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_xgb['pred'], pred_color = '#5299CB', line_color='black', label = 'XGBoost', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/xgboost2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_xgb['pred'], pred_color = '#E44E58', line_color='black', label = 'XGBoost', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/xgboost3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronxgb layout-nrow=3}

![](../Imgs/plotnine/xgboost1.png)

![](../Imgs/plotnine/xgboost2.png)

![](../Imgs/plotnine/xgboost3.png)

Pronósticos con XGBoost.

:::

Los pronósticos con XGBoost fueron satisfactorios sobre las 3 series. Es importante notar que en la serie de atenciones el pronóstico puntual no capta del todo el pico de atenciones en invierno, sin embargo sí está contemplado por el intervalo de confianza del 80%. El intervalo se obtuvo usando *Ensemble Batch Prediction Intervals*. Otro aspecto interesante es que los intervalos para la serie de temperaturas son estrechos, lo que indica una mejora en la incertidumbre del modelo con respecto a los resultados vistos con el modelo SARIMAX.

| Serie                    | Hor. | $A$ | $P$ | $H$ | $\eta$  | $C$ | MAPE  | Interval Score |
|--------------------------|------|-----|-----|-----|-----|-----|-------|----------------|
|                          | 3    | `{python} resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_lgbm3['grilla'][resultados_1_lgbm3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_lgbm3['mape'], 4)` | `{python} round(resultados_1_lgbm3['score'], 4)` |
| Atenciones    | 6    | `{python} resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_lgbm6['grilla'][resultados_1_lgbm6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_lgbm6['mape'], 4)` | `{python} round(resultados_1_lgbm6['score'], 4)` |
|                          | 12   | `{python} resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_1_lgbm['grilla'][resultados_1_lgbm['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_1_lgbm['mape'], 4)` | `{python} round(resultados_1_lgbm['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 3    | `{python} resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`  | `{python} resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_lgbm3['grilla'][resultados_2_lgbm3['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_lgbm3['mape'], 4)` | `{python} round(resultados_2_lgbm3['score'], 4)` |
| Trabajadores | 6    | `{python} resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_lgbm6['grilla'][resultados_2_lgbm6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_lgbm6['mape'], 4)` | `{python} round(resultados_2_lgbm6['score'], 4)` |
|                          | 12   | `{python} resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_2_lgbm['grilla'][resultados_2_lgbm['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_2_lgbm['mape'], 4)` | `{python} round(resultados_2_lgbm['score'], 4)` |
|                          |      |                                                                                                                                 |                                                                                                                          |                                                                                                                           |                                                                                                                                     |                                                                                                                                        |                                               |                                                |
|                          | 6    | `{python} resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`   | `{python} resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_lgbm6['grilla'][resultados_3_lgbm6['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_lgbm6['mape'], 4)` | `{python} round(resultados_3_lgbm6['score'], 4)` |
| Temperatura              | 12   | `{python} resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]` | `{python} resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_lgbm12['grilla'][resultados_3_lgbm12['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_lgbm12['mape'], 4)` | `{python} round(resultados_3_lgbm12['score'], 4)` |
|                          | 24   | `{python} resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['n_estimators'].values[0]`     | `{python} resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['max_depth'].values[0]` | `{python} resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['num_leaves'].values[0]` | `{python} float(resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['learning_rate'].values[0])` | `{python} float(resultados_3_lgbm['grilla'][resultados_3_lgbm['grilla']['Seleccionado'] == True].head(1)['colsample_bytree'].values[0])` | `{python} round(resultados_3_lgbm['mape'], 4)` | `{python} round(resultados_3_lgbm['score'], 4)` |
: Modelos LightGBM seleccionados y métricas de evaluación. {#tbl-resultadoslgbm}

```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_lgbm['pred'], pred_color = '#6BC78A', line_color='black', label = 'LightGBM', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/lgbm1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_lgbm['pred'], pred_color = '#5299CB', line_color='black', label = 'LightGBM', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/lgbm2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_lgbm['pred'], pred_color = '#E44E58', line_color='black', label = 'LightGBM', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/lgbm3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronlgbm layout-nrow=3}

![](../Imgs/plotnine/lgbm1.png)

![](../Imgs/plotnine/lgbm2.png)

![](../Imgs/plotnine/lgbm3.png)

Pronósticos con LightGBM.

:::

Los pronósticos con LightGBM son ligeramente peores a los conseguidos con XGBoost en todos los casos. Esta diferencia es especialmente importante en la serie de temperaturas, ya que en el pronóstico a 24 horas los intervalos de confianza en las primeras 6 horas no suelen cubrir a las observaciones, lo que podría indicar una subestimación en la variabilidad de los pronósticos. Este modelo tampoco pudo identificar el pico de atenciones en guardia en los meses invernales.

Resulta interesante destacar en las tablas @tbl-resultadosxgb y @tbl-resultadoslgbm como XGBoost suele elegir bosques más sencillos, con menos árboles, que a su vez son menos profundos y tienen menos hojas. Por otro lado, se ve también la tendencia de LightGBM a expandir los árboles por las hojas. También se observa que los modelos optaron, en general, por elegir una tasa de aprendizaje y una proporción de características utilizadas altas, elecciones que podrían llevar a un sobreajuste. Aún así, esto no se vio evidenciado en los resultados.

### 4.2.3 Redes neuronales (LSTM)

Los modelos basados en aprendizaje profundo se encargan de definir las características más relevantes y los parámetros más adecuados de forma automática. En las redes neuronales, como es el caso de LSTM, solo se tendrá que elegir la estructura del modelo. 

Las redes neuronales tienen tendencia a sobreajustar, para evitar esto existen numerosas alternativas. En primer lugar se puede optar por detener el entrenamiento antes de completar todas las iteraciones, si es que no se ve mejora en la función de pérdida, esto se conoce como *early stopping* o detención temprana en español. Otra opción es "ignorar" una cierta proporción de neuronas en cada iteración, que es equivalente a entrenar distintas redes neuronales. Esta última técnica es denominada *dropout*.

Para cada serie se entrenaron modelos LSTM con 300 iteraciones de entrenamiento y una tasa de aprendizaje de 0.001. Se adoptó una paciencia para el *early stopping* de 10, lo cual quiere decir que si la función de pérdida no muestra mejoras significativas en 10 iteraciones seguidas se deteniene el entrenamiento. Además, se probaron las siguientes características para la estructura del modelo:

- Capas y neuronas en el modelo ($N$). Opciones: [32], [12, 24], [24, 42].

- Rezagos con los que se entrena el modelo ($R$). Opciones: 1, 12, 24.

- Proporción de *dropout* en cada capa ($D$). Opciones: 0.1, 0.3.

- Función de activación ($A$). Opciones: ReLu, tangente hiperbólica (tanh).


| Serie                    | Hor. | $N$ | $R$ | $D$ | $A$ | MAPE  | _Interval Score_ |
|--------------------------|------|--------|-----|-----|-----|-------|------------------|
|              | 3    | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`       | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`               | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`          | `{python} resultados_1_lstm3['grilla'][resultados_1_lstm3['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`       | `{python} round(resultados_1_lstm3['mape'], 4)`  | `{python} round(resultados_1_lstm3['score'], 4)`  |
| Atenciones   | 6    | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm6['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`    | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm6['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_1_lstm6['grilla'][resultados_1_lstm6['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_1_lstm6['mape'], 4)`  | `{python} round(resultados_1_lstm6['score'], 4)`  |
|              | 12   | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`     | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`     | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`     | `{python} resultados_1_lstm['grilla'][resultados_1_lstm['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`     | `{python} round(resultados_1_lstm['mape'], 4)`   | `{python} round(resultados_1_lstm['score'], 4)`   |
|              |      |                                                                                                                                |                                                                                                                                       |                                                                                                                                     |                                                                                                                                     |                                                  |                                                   |
|              | 3    | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`   | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_2_lstm3['grilla'][resultados_2_lstm3['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_2_lstm3['mape'], 4)`  | `{python} round(resultados_2_lstm3['score'], 4)`  |
| Trabajadores | 6    | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`   | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_2_lstm6['grilla'][resultados_2_lstm6['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_2_lstm6['mape'], 4)`  | `{python} round(resultados_2_lstm6['score'], 4)`  |
|              | 12   | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`     | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`     | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`     | `{python} resultados_2_lstm['grilla'][resultados_2_lstm['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`     | `{python} round(resultados_2_lstm['mape'], 4)`   | `{python} round(resultados_2_lstm['score'], 4)`   |
|              |      |                                                                                                                                |                                                                                                                                       |                                                                                                                                     |                                                                                                                                     |                                                  |                                                   |
|              | 6    | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`   | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`   | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`   | `{python} resultados_3_lstm6['grilla'][resultados_3_lstm6['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`   | `{python} round(resultados_3_lstm6['mape'], 4)`  | `{python} round(resultados_3_lstm6['score'], 4)`  |
| Temperatura  | 12   | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['units'].values[0]` | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]` | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]` | `{python} resultados_3_lstm12['grilla'][resultados_3_lstm12['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]` | `{python} round(resultados_3_lstm12['mape'], 4)` | `{python} round(resultados_3_lstm12['score'], 4)` |
|              | 24   | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['units'].values[0]`     | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['lags'].values[0]`     | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['dropout'].values[0]`     | `{python} resultados_3_lstm['grilla'][resultados_3_lstm['grilla']['Seleccionado'] == True].head(1)['activation'].values[0]`     | `{python} round(resultados_3_lstm['mape'], 4)`   | `{python} round(resultados_3_lstm['score'], 4)`   |
: Modelos LSTM seleccionados y métricas de evaluación. {#tbl-resultadoslstm}


```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_lstm['pred'], pred_color = '#6BC78A', line_color='black', label = 'LSTM', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/lstm1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_lstm['pred'], pred_color = '#5299CB', line_color='black', label = 'LSTM', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/lstm2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_lstm['pred'], pred_color = '#E44E58', line_color='black', label = 'LSTM', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/lstm3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronlstm layout-nrow=3}

![](../Imgs/plotnine/lstm1.png)

![](../Imgs/plotnine/lstm2.png)

![](../Imgs/plotnine/lstm3.png)

Pronósticos con LSTM.

:::

Al igual que en los métodos de aprendizaje automatizado, los intervalos de confianza se obtuvieron usando *conformal predictions*, pero se realizaron con funciones ya integradas en la librería `scalecast`.

Los pronósticos sobre todas las series fueron satisfactorios, sin embargo, se destaca la falta de ajuste en el pico de atenciones por guardia. Se puede apreciar como en la serie de atenciones, que no tenía un patrón estacional muy marcado, ajustaron mejor aquellos modelos con un solo rezago de entrenamiento, mientras que en la serie de trabajadores, la cual contaba con una estacionalidad marcada, ajustaron mejor aquellos con 24 rezagos. Además, estas dos series se ven diferenciadas por la proporción de *dropout* con la que cuentan los modelos seleccionados. Estos aspectos tal vez se deban a las diferencias en variabilidad estacional que tienen las series, observable a partir del gráfico @fig-box. Otra curiosidad es la presencia de redes de una sola capa de 32 neuronas para los pronósticos a largo plazo, mientras que aquellas redes con dos capas, de 24 y 42 neuronas, son especialmente prominentes en los pronósticos a medio plazo.

### 4.2.4 Modelos fundacionales (TimeGPT y Chronos)

La ventaja de los modelos fundacionales por sobre las redes neuronales convencionales radica en que el ajuste de parámetros se realiza con un preentrenamiento en grandes conjuntos de datos. Esto significa que no se requiere ninguna intervención manual. No es necesario definir características, ajustar parámetros, ni especificar la forma del modelo. Este modo de pronóstico se conoce como *zero-shot*.

Si se deseara un ajuste más controlado sobre la serie se podría hacer uso del "ajuste fino" (*fine tuning*). El ajuste fino consiste en evaluar la función de pérdida con los parámetros preestablecidos y realizar iteraciones extra de entrenamiento para ajustar el modelo específicamente al conjunto de datos, con el objetivo de minimizar aún más el error del pronóstico. Sin embargo, luego de numerosas pruebas, esta herramienta no logró cambios significativos en el pronóstico de las series estudiadas por sobre las configuraciones base, logrando en ciertos casos resultados peores y aumentos excesivos en los tiempos de procesamiento. Es por esto que no se utiliza esta característica en los resultados finales, pero se la propone para futuras aplicaciones.


```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_gpt['pred'], pred_color = '#6BC78A', line_color='black', label = 'TimeGPT', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/gpt1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_gpt['pred'], pred_color = '#5299CB', line_color='black', label = 'TimeGPT', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/gpt2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_gpt['pred'], pred_color = '#E44E58', line_color='black', label = 'TimeGPT', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/gpt3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-prongpt layout-nrow=3}

![](../Imgs/plotnine/gpt1.png)

![](../Imgs/plotnine/gpt2.png)

![](../Imgs/plotnine/gpt3.png)

Pronósticos con TimeGPT.

:::

| Serie                    | Hor. | MAPE  | _Interval Score_ |
|--------------------------|------|-------|------------------|
|                          |3     |`{python} round(resultados_1_gpt3['mape'], 4)`|`{python} round(resultados_1_gpt3['score'], 4)`|
|Atenciones                |6     |`{python} round(resultados_1_gpt6['mape'], 4)`|`{python} round(resultados_1_gpt6['score'], 4)`|
|                          |12    |`{python} round(resultados_1_gpt['mape'], 4)`|`{python} round(resultados_1_gpt['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_2_gpt3['mape'], 4)`|`{python} round(resultados_2_gpt3['score'], 4)`|
|Trabajadores                |6     |`{python} round(resultados_2_gpt6['mape'], 4)`|`{python} round(resultados_2_gpt6['score'], 4)`|
|                          |12    |`{python} round(resultados_2_gpt['mape'], 4)`|`{python} round(resultados_2_gpt['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_3_gpt6['mape'], 4)`|`{python} round(resultados_3_gpt6['score'], 4)`|
|Temperatura                |6     |`{python} round(resultados_3_gpt12['mape'], 4)`|`{python} round(resultados_3_gpt12['score'], 4)`|
|                          |12    |`{python} round(resultados_3_gpt['mape'], 4)`|`{python} round(resultados_3_gpt['score'], 4)`|
: Métricas de evaluación para los ajustes con TimeGPT. {#tbl-metricasTimegpt}

Los pronósticos con TimeGPT sobre las series de temperatura y atenciones fueron regulares al largo plazo, donde no pudo detectar correctamente los patrones estacionales de las series. Por otro lado, si logra pronosticar correctamente las observaciones futuras en la serie de trabajadores. Los problemas de pronóstico de TimeGPT parecen estar en el corto y largo plazo, ya que en la tabla @tbl-metricasTimegpt se puede ver como para los pronósticos a medio plazo las métricas de evaluación son menores que para el resto de horizontes. Se observa en el intervalo de confianza para la serie de atenciones como la incertidumbre en el período de invierno es mucho mayor que en otros puntos del ciclo estacional, período que fue especialmente complicado de pronósticar para varios de los modelos.

Chronos, al ser ser una familia de modelos, cuenta con múltiples opciones para realizar pronósticos *zero-shot*. En esta tesina se utiliza el modelo `bolt-small`, el cual cuenta con 48 millones de parámetros.

```{python}
plot_forecast(data = atenciones_guardia, forecast = resultados_1_chronos['pred'], pred_color = '#6BC78A', line_color='black', label = 'Chronos', long = 36, ylabel= 'Atenciones', legend_position = (0.3,0.98)
).save("../Imgs/plotnine/chronos1.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = trabajadores, forecast = resultados_2_chronos['pred'], pred_color = '#5299CB', line_color='black', label = 'Chronos', long = 36, ylabel= 'Trabajadores', legend_position='none'
).save("../Imgs/plotnine/chronos2.png", width=6, height=4/3.1, dpi=700)

plot_forecast(data = tiempo_rosario, forecast = resultados_3_chronos['pred'], pred_color = '#E44E58', line_color='black', label = 'Chronos', long = 72, xlabel='Día', ylabel= 'Temperatura', legend_position='none'
).save("../Imgs/plotnine/chronos3.png", width=6, height=4/3.1, dpi=700)

```

::: {#fig-pronchronos layout-nrow=3}

![](../Imgs/plotnine/chronos1.png)

![](../Imgs/plotnine/chronos2.png)

![](../Imgs/plotnine/chronos3.png)

Pronósticos con Chronos.

:::


| Serie                    | Hor. | MAPE  | _Interval Score_ |
|--------------------------|------|-------|------------------|
|                          |3     |`{python} round(resultados_1_chronos3['mape'], 4)`|`{python} round(resultados_1_chronos3['score'], 4)`|
|Atenciones                |6     |`{python} round(resultados_1_chronos6['mape'], 4)`|`{python} round(resultados_1_chronos6['score'], 4)`|
|                          |12    |`{python} round(resultados_1_chronos['mape'], 4)`|`{python} round(resultados_1_chronos['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_2_chronos3['mape'], 4)`|`{python} round(resultados_2_chronos3['score'], 4)`|
|Trabajadores                |6     |`{python} round(resultados_2_chronos6['mape'], 4)`|`{python} round(resultados_2_chronos6['score'], 4)`|
|                          |12    |`{python} round(resultados_2_chronos['mape'], 4)`|`{python} round(resultados_2_chronos['score'], 4)`|
| |      | | |
|                          |3     |`{python} round(resultados_3_chronos6['mape'], 4)`|`{python} round(resultados_3_chronos6['score'], 4)`|
|Temperatura                |6     |`{python} round(resultados_3_chronos12['mape'], 4)`|`{python} round(resultados_3_chronos12['score'], 4)`|
|                          |12    |`{python} round(resultados_3_chronos['mape'], 4)`|`{python} round(resultados_3_chronos['score'], 4)`|
: Métricas de evaluación para los ajustes con Chronos. {#tbl-metricasChronos}

Chronos detecta correctamente los cambios de temperatura en las 24 horas, pero por otro lado, el pronóstico sobre las demás series es deficiente. Mientras que en la serie de atenciones, a medida que se incrementa el horizonte las métricas empeoran, para la serie de trabajadores mejoran.

## 4.3 Comparación de resultados y análisis final

Para las comparativas en la primera serie se utiliza el modelo SARIMA AT-4, dado que obtuvo mejores métricas y un AIC menor que el modelo AT-2, mientras que para la serie de trabajadores se utiliza el modelo TR-2.

**Atenciones en guardia por patologías respiratorias en el hospital HNVV**

```{python}
tabla_resumen(metricas_1, '../Imgs/tabla_resumen_1')
```

![Comparación de métricas de evaluación entre distintos modelos y horizontes para la serie de atenciones.](../Imgs/tabla_resumen_1.png){width=80%}

El modelo ARIMA superó a los modelos más modernos en términos de MAPE a corto y largo plazo, pero sus *Interval Scores* son grandes a comparación de otros modelos con MAPEs similares, provocado por la gran amplitud en los intervalos de confianza. Por otro lado, los pronósticos de Chronos fueron considerablemente peores en los horizontes largos. Sin embargo, ningún modelo se destacó claramente en ningún horizonte. 

En comparación con el resto de series, el número de atenciones mensuales en guardia fue especialmente difícil de pronosticar, evidenciado por los altos MAPEs e *Interval Scores*.

**Trabajadores asalariados en el rubro de educación privada**

```{python}
tabla_resumen(metricas_2, '../Imgs/tabla_resumen_2')
```

![Comparación de métricas de evaluación entre distintos modelos y horizontes para la serie de trabajadores.](../Imgs/tabla_resumen_2.png){width=80%}

La clara tendencia y estacionalidad del número de trabajadores asalariados en el rubro de la enseñanza privada provocó que sea excepcionalmente fácil de pronosticar, logrando métricas de evaluación bajas en todos los horizontes. Se destacan sobre todo TimeGPT en el corto plazo y LSTM en los horizontes más largos. 


**Temperaturas horarias en Rosario**

```{python}
tabla_resumen(metricas_3, '../Imgs/tabla_resumen_3')
```

![Comparación de métricas de evaluación entre distintos modelos y horizontes para la serie de temperaturas.](../Imgs/tabla_resumen_3.png){width=80%}

Para la serie de temperaturas, la cual cuenta con la humedad y la presión atmosférica como variables exógenas, tanto ARIMA como LSTM tuvieron resultados superiores al resto de modelos en el corto plazo. Pronosticando medio día hacia adelante, TimeGPT y LSTM fueron los modelos con mejor rendimiento, mientras que en el pronóstico de un día entero, XGBoost superó con creces al resto de modelos.

\newpage

# 5. Conclusiones	

Se evidenció que ningún modelo es universalmente superior al resto ni existe combinación única de hiperparámetros que logre el mejor ajuste en cualquier circunstancia. A lo largo del trabajo se observaron las diferencias, ventajas y desventajas de cada método. 

Los modelos ARIMA requieren un ajuste manual, que además de ser demandante temporalmente, su aplicación a diferentes series implicaría comenzar desde el inicio. En términos de resultados, logró MAPEs bajos en comparación al resto de modelos en casi todos los escenarios, sin embargo, evidenciaron mucha incertidumbre en sus resultados, provocando que la amplitud de sus pronósticos probabilísticos desencadenaran *Interval Scores* altos.

Los algoritmos de aprendizaje automático precisan la definición de una serie de características que facilitan a los modelos a ajustarse a los datos. Ante nuevas series, sería prudente cambiar las características a otras más acordes a los datos. En relación a otros modelos, no se destacaron especialmente en ninguna instancia, con excepción de XGBoost pronosticando a largo plazo la serie de temperaturas. No obstante, los algoritmos de aprendizaje automatizado obtuvieron resultados acertados consistentemente en todas las series analizadas y con baja incertidumbre.

El único requerimiento para ajustar redes neuronales LSTM es la definición de la estructura del modelo, ya que tanto el ajuste de hiperparámetros como la elección de características se realizan de forma automática. A pesar de haber sido el modelo de pronóstico más exigente computacionalmente, obtuvo métricas favorables a través de todas las series y horizontes.

Finalmente, los modelos basados en arquitecturas *transformer*, como TimeGPT y Chronos, no precisan de ningún ajuste manual, únicamente los datos y su periodicidad son necesarios como entrada para los modelos. TimeGPT tuvo problemas para detectar los patrones estacionales de las series, sin embargo, logró buenas métricas en el corto y medio plazo. Chronos, por otro lado, presentó resultados menos satisfactorios. Esto podría solucionarse eligiendo otro modelo de la familia de Chronos diferente a `bolt-small`, o haciendo uso del ajuste fino. Cabe destacar que, si bien TimeGPT y Chronos destacan por su facilidad de uso, son poco personalizables, dejando pocas alternativas ante ajustes deficientes. TimeGPT tiene además la problemática de no ser de código abierto, lo que evita conocer el proceso con el que se ajustan los modelos.

A la fecha en la que se comenzó este trabajo, la API de TimeGPT ofrecía un plan gratuito con un número de consultas mensuales limitadas, sin embargo, a lo largo del desarrollo los planes de suscripción cambiaron. Se eliminó el plan gratuito y se reemplazó por un mes de prueba con consultas limitadas para usuarios nuevos únicamente. Este cambio afectó significativamente el desarrollo del trabajo.

Este documento presenta tres contribuciones claves al campo de la estadística. En primer lugar, la introducción de los modelos transformadores para el pronóstico de series temporales. En segundo lugar, se exploró el *Interval Score* como medida del error para evaluar el desempeño de los modelos ante pronósticos probabilísticos. Por último, se presenta la construcción de intervalos de pronóstico por medio de *conformal predictions*, que no depende de conocer la distribución de los residuos.

\newpage

# 6. Mejoras y extensiones a la investigación

En esta tesina se buscó abordar el tema de la forma más amplia posible sin sacrificar profundidad. Sin embargo, por la amplitud del mismo se dejaron fuera numerosos temas interesantes que se podrían tratar en otros trabajos.

En primer lugar, se podrían estudiar series con características distintas y aplicar las respectivas correcciones a aquellas series acotadas, como el caso de la cantidad de atenciones en guardia y el número de trabajadores, que al ser variables de conteo no pueden ser negativas.

En la selección de los modelos, se eligió como mejor aquel que minimizara el MAPE, pero se podría haber hecho la elección en base al *Interval Score* o alguna otra medida de error. A su vez, podría explorarse el uso de distintas medidas de error probabilísticas diferentes al *Interval Score*, tales como *Scaled quantile loss*, *Weighted quantile loss* o *Implicit quantile loss*.

*Boosting* y el ensamblaje de modelos no esta limitado únicamente a los árboles de decisión, por lo que se prone indagar como funcionan estas técnicas en otros modelos, como en redes neuronales.

Para obtener pronósticos probabilísticos en los algoritmos de aprendizaje automático se optó por EnbPI, pero queda propuesto probar otros métodos o alternativas, como *Natural Gradient Boosting* (NGBoost).

Al comparar los modelos también se pudo haber estudiado el tiempo de cómputo en los ajustes. Una prueba de esto fue realizada, pero los métodos utilizados para obtener estos resultados no fueron del todo adecuados, y ante la falta de alternativas se decidió recortar estos resultados y no mostrarlos empíricamente. Sin embargo, se hicieron menciones a los resultados a lo largo del trabajo. A raíz de esto, se propone estudiar distintas metodologías para medir los tiempos de cómputo de los modelos.

Otra expansión a la investigación se puede dar en el ajuste de hiperparámetros, y características en el caso de los algoritmos de aprendizaje automatizado. Si bien con la búsqueda de parámetros se intentó abordar múltiples opciones, por cuestiones de tiempo y exigencia computacional es imposible explorarlas todas. Es por esto que aún queda un amplio campo de investigación en este aspecto y en la aplicación del ajuste fino en modelos fundacionales preentrenados. 

\newpage

# 7. Bibliografía

**Alammar, J.** (27 de junio de 2018). *The Illustrated Transformer*. Jay Alammar. [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)

**Ansari et al.** (2024). *Chronos: Learning the Language of Time Series*. Transactions on Machine Learning Research. [https://arxiv.org/abs/2403.07815](https://arxiv.org/abs/2403.07815)

**Awan, A.** (2 de septiembre de 2024). *Time Series Forecasting With TimeGPT*. Datacamp. [https://www.datacamp.com/tutorial/time-series-forecasting-with-time-gpt](https://www.datacamp.com/tutorial/time-series-forecasting-with-time-gpt)

**Bermejo, J.** (21 de mayo de 2024). *Redes neuronales*. Facultad de Ciencias Económicas y Estadística de la Universidad Nacional de Rosario.

**Chen, Y., Yao, X.** (2023). *Conformal prediction for time series*. Proceedings of Machine Learning Research. [https://arxiv.org/abs/2010.09107](https://arxiv.org/abs/2010.09107)

**Elhariri, K.** (1 de marzo de 2022). *The Transformer Model*. Medium. [https://medium.com/data-science/attention-is-all-you-need-e498378552f9](https://medium.com/data-science/attention-is-all-you-need-e498378552f9)

**GeeksforGeeks**. (s.f.). *What is LSTM – Long short term memory?*. Recuperado el 15 de julio de 2025 de [https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/](https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/)

**Gilliland, M., Sglavo, U., & Tashman, L.** (2016). *Forecast Error Measures: Critical Review and Practical Recommendations*. John Wiley & Sons Inc.

**Gneiting, T., & Raftery A. E.** (2007). *Strictly Proper Scoring Rules, Prediction, and Estimation*. Journal of the American Statistical Association, 102(477), 359–378. [https://doi.org/10.1198/016214506000001437](https://doi.org/10.1198/016214506000001437)

**Hyndman, R. J., & Athanasopoulos, G.** (2021). *Forecasting: principles and practice (3rd ed.).*
OTexts. [https://otexts.com/fpp3/](https://otexts.com/fpp3/)

**Hyndman, R.J., Athanasopoulos, G., Garza, A., Challu, C., Mergenthaler, M., & Olivares, K.G.** (2024). *Forecasting: Principles and Practice, the Pythonic Way*. OTexts. [OTexts.com/fpppy](OTexts.com/fpppy).

**IBM**. (s.f.). *Explainers*. Recuperado el 14 de marzo de 2025 de [https://www.ibm.com/think/topics](https://www.ibm.com/think/topics)

**Kamtziris, G.** (27 de febrero de 2023). *Time Series Forecasting with XGBoost and LightGBM: Predicting Energy Consumption.* Medium. [https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-460b675a9cee](https://medium.com/@geokam/time-series-forecasting-with-xgboost-and-lightgbm-predicting-energy-consumption-460b675a9cee)

**Keith, M.** (22 de septiembre de 2023). *Five Practical Applications of the LSTM Model for Time Series, with Code*. Towards data science. [https://towardsdatascience.com/five-practical-applications-of-the-lstm-model-for-time-series-with-code-a7aac0aa85c0/](https://towardsdatascience.com/five-practical-applications-of-the-lstm-model-for-time-series-with-code-a7aac0aa85c0/)

**Korstanje, J.** (2021). A*dvanced Forecasting with Python*. Apress.

**Nielsen, A.** (2019). *Practical Time Series Analysis: Prediction with Statistics and Machine Learning*. O'Reilly Media.

**Nixtla**. (s.f.). *About TimeGPT*. Recuperado en diciembre de 2024 de [https://docs.nixtla.io/docs/getting-started-about_timegpt](https://docs.nixtla.io/docs/getting-started-about_timegpt)

**Parmezan, A., Souza, V., & Batista, G.** (1 de mayo de 2019). *Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model*. Information Sciences. [https://www.sciencedirect.com/science/article/abs/pii/S0020025519300945](https://www.sciencedirect.com/science/article/abs/pii/S0020025519300945)

**Prunello, M., & Marfetán, D.** (12 de mayo de 2024). *Árboles de decisión*. Facultad de Ciencias Económicas y Estadística de la Universidad Nacional de Rosario.

**Sabino Parmezan, A. R., Souza, V. M. A., & Batista, G. E. A. P. A.** (2019). *Evaluation of statistical and machine learning models for time series prediction: Identifying the state‑of‑the‑art and the best conditions for the use of each model*. Information Sciences, 484, 302–337. [https://doi.org/10.1016/j.ins.2019.01.076](https://doi.org/10.1016/j.ins.2019.01.076)

**Sanderson, G.** [3Blue1Brown]. (2024). *Attention in transformers, step-by-step | DL6 [Video]*. Youtube. [https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1204s](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1204s)

**Sanderson, G.** [3Blue1Brown]. (2024). *Transformers (how LLMs work) explained visually | DL5 [Video]*. Youtube. [https://www.youtube.com/watch?v=wjZofJX0v4M](https://www.youtube.com/watch?v=wjZofJX0v4M)

**Shastri, Y.** (26 de abril de 2024). *Attention Mechanism in LLMs: An Intuitive Explanation*. Datacamp. [https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition](https://www.datacamp.com/blog/attention-mechanism-in-llms-intuition)

**Silberstein, E.** (7 de noviembre de 2024). *Tracing the Transformer in Diagrams*. Medium. [https://medium.com/data-science/tracing-the-transformer-in-diagrams-95dbeb68160c](https://medium.com/data-science/tracing-the-transformer-in-diagrams-95dbeb68160c)

**Valeriy, M.** (11 de agosto de 2023). *Demystifying EnbPI: Mastering Conformal Prediction Forecasting*. Medium. [https://valeman.medium.com/demystifying-enbpi-mastering-conformal-prediction-forecasting-d49e65532416](https://valeman.medium.com/demystifying-enbpi-mastering-conformal-prediction-forecasting-d49e65532416)

**Vaswani et al.** (2017). *Attention is all you need*. Google. [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)


\newpage

# 8. Anexo

## 8.1 Gráficos estacionales

```{python}
#| label: fig-atenciones_estacionalidad
#| fig-cap: Atenciones por guardia mensuales por patologías respiratorias por año
#| fig-height: 2

atenciones_trunc['mes'] = atenciones_trunc['ds'].dt.month
atenciones_trunc['Año'] = atenciones_trunc['ds'].dt.year

(
  ggplot(atenciones_trunc) +
  aes(y = "y", x = "mes", group = "Año", color = "factor(Año)") +
  geom_point(size = 0.3) +
  geom_line() +
  scale_x_continuous(breaks = range(1,13), limits = (1,12), labels=[
        "Ene", "Feb", "Mar", "Abr", "May", "Jun", "Jul", "Ago", "Sep", "Oct", "Nov", "Dic"
    ]) +
  labs(color = "Año", x = "Mes", y = "Atenciones") +
  theme(figure_size=(6, 3))
)
```

```{python}
#| label: fig-trabajadores_estacionalidad
#| fig-cap: Número mensual de trabajadores asalariados en el área de enseñanza privada por año
#| fig-height: 2

trabajadores_trunc['mes'] = trabajadores_trunc['ds'].dt.month
trabajadores_trunc['Año'] = trabajadores_trunc['ds'].dt.year

(
  ggplot(trabajadores_trunc) +
  aes(y = "y", x = "mes", group = "Año", color = "factor(Año)") +
  geom_point(size = 0.3) +
  geom_line() +
  scale_x_continuous(breaks = range(1,13), limits = (1,12), labels=[
        "Ene", "Feb", "Mar", "Abr", "May", "Jun", "Jul", "Ago", "Sep", "Oct", "Nov", "Dic"
    ]) +
  labs(color = "Año", x = "Mes", y = "Trabajadores (miles)") +
  theme(figure_size=(6, 3))
)
```

```{python}
#| label: fig-temp_estacional
#| fig-cap: Atenciones por guardia mensuales por patologías respiratorias por año
#| fig-height: 2

temperatura_trunc['Hora'] = temperatura_trunc['ds'].dt.hour
temperatura_trunc['Día'] = temperatura_trunc['ds'].dt.day

(
  ggplot(temperatura_trunc) +
  aes(y = "y", x = "Hora", group = "Día", color = "Día") +
  geom_point(size = 0.3) +
  geom_line() + 
  labs(y = "Temperatura (Cº)", color = "Día") +
  scale_x_continuous(breaks = range(0,24)) +
  theme(legend_position= "bottom", figure_size=(6, 3)
  )
)
```

## 8.2 Salidas de modelos ARIMA {#salidas_arima}

**Salidas de modelos ARIMA para la serie de atenciones**

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA AT-1 para la serie de atenciones."

summary_to_latex(resultados_arima['salida_arima_AT1'])
```

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA AT-2 para la serie de atenciones."

summary_to_latex(resultados_arima['salida_arima_atenciones_auto'])
```

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA AT-3 para la serie de atenciones."

summary_to_latex(resultados_arima['salida_arima_AT3'])
```


```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA AT-4 para la serie de atenciones."

summary_to_latex(resultados_arima['salida_arima_AT4'])
```

**Salidas de modelos ARIMA para la serie de trabajadores**

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA TR-1 para la serie de trabajadores."

summary_to_latex(resultados_arima['salida_arima_TR1'])
```

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA TR-2 para la serie de trabajadores."

summary_to_latex(resultados_arima['salida_arima_TR2'])
```

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA TR-3 para la serie de trabajadores."

summary_to_latex(resultados_arima['salida_arima_trabajadores_auto'])
```

**Salidas de modelos ARIMA para la serie de temperaturas**

```{python}
#| output: asis
#| label: tbl-salida_te1
#| tbl-cap: "Salida modelo ARIMA TE-1 para la serie de temperaturas."

summary_to_latex(resultados_arima['salida_arima_temperatura_auto'])
```


```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA TE-2 para la serie de temperaturas."

summary_to_latex(resultados_arima['salida_arima_TE2'])
```

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA TE-3 para la serie de temperaturas."

summary_to_latex(resultados_arima['salida_arima_TE3'])
```

```{python}
#| output: asis
#| tbl-cap: "Salida modelo ARIMA TE-4 para la serie de temperaturas."

summary_to_latex(resultados_arima['salida_arima_TE4'])
```

## 8.3 Comprobación de supuestos de modelos arima {#supuestos_arima}

**Comprobación de supuestos de modelos ARIMA para la serie de atenciones sobre los residuos estandarizados**

```{python}
resid_check(resultados_arima['resid_arima_AT1'], ds = atenciones_trunc['ds'], arima_df= resultados_arima['salida_arima_AT1'].shape[0]-2, time='%Y', name='resid_arima_AT1')
```

::: {#fig-residAtencionesAuto layout-nrow=2}

![](../Imgs/plotnine/resid_arima_AT1_1.png)

![](../Imgs/plotnine/resid_arima_AT1_2.png)

![](../Imgs/plotnine/resid_arima_AT1_3.png)

![](../Imgs/plotnine/resid_arima_AT1_4.png)

Comprobación de supuestos del modelo ARIMA AT-1 para la serie de atenciones.
:::


```{python}
resid_check(resultados_arima['resid_arima_atenciones_auto'], ds = atenciones_trunc['ds'], arima_df= resultados_arima['salida_arima_atenciones_auto'].shape[0]-2, time='%Y', name='resid_arima_AT2')
```

::: {#fig-residAtencionesAuto layout-nrow=2}

![](../Imgs/plotnine/resid_arima_AT2_1.png)

![](../Imgs/plotnine/resid_arima_AT2_2.png)

![](../Imgs/plotnine/resid_arima_AT2_3.png)

![](../Imgs/plotnine/resid_arima_AT2_4.png)

Comprobación de supuestos del modelo ARIMA AT-2 para la serie de atenciones.
:::


```{python}
resid_check(resultados_arima['resid_arima_AT3'], ds = atenciones_trunc['ds'], arima_df= resultados_arima['salida_arima_AT3'].shape[0]-2, time='%Y', name='resid_arima_AT3')
```

::: {#fig-residAtencionesAuto layout-nrow=2}

![](../Imgs/plotnine/resid_arima_AT3_1.png)

![](../Imgs/plotnine/resid_arima_AT3_2.png)

![](../Imgs/plotnine/resid_arima_AT3_3.png)

![](../Imgs/plotnine/resid_arima_AT3_4.png)

Comprobación de supuestos del modelo ARIMA AT-3 para la serie de atenciones.
:::


```{python}
resid_check(resultados_arima['resid_arima_AT4'], ds = atenciones_trunc['ds'], arima_df= resultados_arima['salida_arima_AT4'].shape[0]-2, time='%Y', name='resid_arima_AT4')
```

::: {#fig-residAtencionesAuto layout-nrow=2}

![](../Imgs/plotnine/resid_arima_AT4_1.png)

![](../Imgs/plotnine/resid_arima_AT4_2.png)

![](../Imgs/plotnine/resid_arima_AT4_3.png)

![](../Imgs/plotnine/resid_arima_AT4_4.png)

Comprobación de supuestos del modelo ARIMA AT-4 para la serie de atenciones.
:::


**Comprobación de supuestos de modelos ARIMA para la serie de trabajadores sobre los residuos estandarizados**


```{python}
resid_check(resultados_arima['resid_arima_TR1'], ds = trabajadores_trunc['ds'], arima_df= resultados_arima['salida_arima_TR1'].shape[0]-2, time='%Y', name='resid_arima_TR1')
```

::: {#fig-residTrabajadores1 layout-nrow=2}

![](../Imgs/plotnine/resid_arima_TR1_1.png)

![](../Imgs/plotnine/resid_arima_TR1_2.png)

![](../Imgs/plotnine/resid_arima_TR1_3.png)

![](../Imgs/plotnine/resid_arima_TR1_4.png)

Comprobación de supuestos del modelo ARIMA TR-1 para la serie de trabajadores.
:::

```{python}
resid_check(resultados_arima['resid_arima_TR2'], ds = trabajadores_trunc['ds'], arima_df= resultados_arima['salida_arima_TR2'].shape[0]-2, time='%Y', name='resid_arima_TR2')
```

::: {#fig-residTrabajadores1 layout-nrow=2}

![](../Imgs/plotnine/resid_arima_TR2_1.png)

![](../Imgs/plotnine/resid_arima_TR2_2.png)

![](../Imgs/plotnine/resid_arima_TR2_3.png)

![](../Imgs/plotnine/resid_arima_TR2_4.png)

Comprobación de supuestos del modelo ARIMA TR-2 para la serie de trabajadores.
:::

```{python}
resid_check(resultados_arima['resid_arima_trabajadores_auto'], ds = trabajadores_trunc['ds'], arima_df= resultados_arima['salida_arima_trabajadores_auto'].shape[0]-2, time='%Y', name='resid_arima_TR3')
```

::: {#fig-residTrabajadoresAuto layout-nrow=2}

![](../Imgs/plotnine/resid_arima_TR3_1.png)

![](../Imgs/plotnine/resid_arima_TR3_2.png)

![](../Imgs/plotnine/resid_arima_TR3_3.png)

![](../Imgs/plotnine/resid_arima_TR3_4.png)

Comprobación de supuestos del modelo ARIMA TR-3 para la serie de trabajadores.
:::


**Comprobación de supuestos de modelos ARIMA para la serie de temperaturas sobre los residuos estandarizados**

```{python}
resid_check(resultados_arima['resid_arima_temperatura_auto'], ds = temperatura_trunc['ds'], arima_df= resultados_arima['salida_arima_temperatura_auto'].shape[0]-2, time='%d', name='resid_arima_TE1')
```

::: {#fig-residTemp3 layout-nrow=2}

![](../Imgs/plotnine/resid_arima_TE1_1.png)

![](../Imgs/plotnine/resid_arima_TE1_2.png)

![](../Imgs/plotnine/resid_arima_TE1_3.png)

![](../Imgs/plotnine/resid_arima_TE1_4.png)

Comprobación de supuestos del modelo ARIMA TE-1 para la serie de temperaturas.
:::

```{python}
resid_check(resultados_arima['resid_arima_TE2'], ds = temperatura_trunc['ds'], arima_df= resultados_arima['salida_arima_TE2'].shape[0]-2, time='%d', name='resid_arima_TE2')
```

::: {#fig-residTemp3 layout-nrow=2}

![](../Imgs/plotnine/resid_arima_TE2_1.png)

![](../Imgs/plotnine/resid_arima_TE2_2.png)

![](../Imgs/plotnine/resid_arima_TE2_3.png)

![](../Imgs/plotnine/resid_arima_TE2_4.png)

Comprobación de supuestos del modelo ARIMA TE-2 para la serie de temperaturas.
:::

```{python}
resid_check(resultados_arima['resid_arima_TE3'], ds = temperatura_trunc['ds'], arima_df= resultados_arima['salida_arima_TE3'].shape[0]-2, time='%d', name='resid_arima_TE3')
```

::: {#fig-residTemp3 layout-nrow=2}

![](../Imgs/plotnine/resid_arima_TE3_1.png)

![](../Imgs/plotnine/resid_arima_TE3_2.png)

![](../Imgs/plotnine/resid_arima_TE3_3.png)

![](../Imgs/plotnine/resid_arima_TE3_4.png)

Comprobación de supuestos del modelo ARIMA TE-3 para la serie de temperaturas.
:::

```{python}
resid_check(resultados_arima['resid_arima_TE4'], ds = temperatura_trunc['ds'], arima_df= resultados_arima['salida_arima_TE4'].shape[0]-2, time='%d', name='resid_arima_TE4')
```

::: {#fig-residTemp3 layout-nrow=2}

![](../Imgs/plotnine/resid_arima_TE4_1.png)

![](../Imgs/plotnine/resid_arima_TE4_2.png)

![](../Imgs/plotnine/resid_arima_TE4_3.png)

![](../Imgs/plotnine/resid_arima_TE4_4.png)

Comprobación de supuestos del modelo ARIMA TE-4 para la serie de temperaturas.
:::

