---
title: "Tesis"
format: pdf
echo: False
warning: False
---

# Carga de datos y librerias

```{python}
# CARGA DE LIBRERIAS ------------------

# Para el manejo de estructuras de datos
import pandas as pd
import numpy as np

# Para dar formato fecha
from datetime import datetime

# Para graficos
import matplotlib.pyplot as plt
import seaborn as sns

# Para cargar las claves
import creds

# Para realizar consultas a la base de datos
import urllib.parse
import requests

# Para medir el tiempo que tarda en ajustar los modelos
import time

# Cargamos funciones
from Funciones import get_api_call, interval_score, save_env, load_env
from tuner_fun import Tuner

# Para los modelos de SKtime
from sktime.forecasting.base import ForecastingHorizon
from sktime.forecasting.fbprophet import Prophet
from sktime.forecasting.arima import AutoARIMA
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error
```

```{python}
# Definimos una semilla
seed = 11072001
```


```{python}
# Llamada a la API y carga de datos

api_call = get_api_call(["364.3_LITORAL_GAGAS__11"], start_date="2016-01")

json = requests.get(api_call).json()

datos = pd.DataFrame(json['data'], columns = ['fecha', 'consumo'])

datos['fecha'] = pd.to_datetime(datos['fecha'], format='%Y-%m-%d')

datos.columns = ['ds', 'y']
```


```{python}
# datos = pd.read_csv(filepath_or_buffer= 'Datos/exportaciones-actividad-saldocomercial-rangos-exportacion-empresa-exportadora-mensual.csv')

# datos = datos[['indice_tiempo','litoral_gas']]

# datos.columns = ['fecha', 'consumo']

# datos['fecha'] = pd.to_datetime(datos['fecha'], format='%Y-%m-%d')

# datos.dropna(inplace=True)
```

```{python}
# Tabla con los primeros y ultimos datos
print(datos.head(3),datos.tail(3))

# Grafico los datos
plt.plot(datos['ds'], datos['y'], marker = '.')
plt.title('Consumo de gas natural')
plt.xlabel('Año')
plt.ylabel('Consumo (millones de metros cúbicos)')
plt.show()

```


```{python}
# Opciones de pronóstico

# Largo del pronóstico

long_pred = 12

# Nivel de significación del intervalo de predicción

alpha = 0.2


# Dividimos el conjunto de datos que queremos pronosticar
corte = len(datos)-long_pred

datos.columns = ['ds', 'y']

datos_train = datos[:corte]
datos_test = datos[corte:]


```

```{python}
# Creamos un dataframe donde vamos a guardar todos los resultados
metricas = pd.DataFrame(columns=(['Modelo', 'MAPE', 'Interval Score', 'Tiempo']))
```

# MODELOS

## Tradicionales

```{python}
# AUTOARIMA

# Definimos el horizonte de pronostico
fh = ForecastingHorizon(datos_test.index, is_relative=False)


# Ajustamos el modelo
timer_comienzo = time.time() # Empiezo a medir cuanto tarda en ajustar
forecaster = AutoARIMA(
    start_p= 0, start_q= 0, sp= 12, max_p=3, max_q= 3, suppress_warnings= True, max_d=2, max_D= 2
).fit(datos_train['y'])
  
# Obtenemos predicciones
pred = forecaster.predict(fh)
pred_int = forecaster.predict_interval(fh, coverage=1-alpha/2)
timer_final = time.time()
tiempo = timer_final - timer_comienzo

# Calculamos MAPE
mape = mean_absolute_percentage_error(datos_test['y'], pred)

# Calculamos Interval Score
pred_int.columns = ['lower', 'upper']
score = interval_score(obs=datos_test['y'], lower=pred_int['lower'], upper=pred_int['upper'], alpha = alpha)

# Graficamos el pronostico
plt.plot(datos['ds'], datos['y'])
sns.lineplot(x = datos['ds'], y= pred, color = 'red', label = 'Prediccion')
plt.fill_between(
    datos_test['ds'], pred_int['lower'], pred_int['upper'], color = 'red', alpha = 0.3, label = f'IC: {1-alpha/2}%')


metricas.loc[len(metricas)] = ['AutoARIMA', mape, score, tiempo]

```


```{python}
# PROPHET

# Definimos el horizonte de pronostico
fh = ForecastingHorizon(datos_test.index, is_relative=False)


# Ajustamos el modelo
timer_comienzo = time.time() # Empiezo a medir cuanto tarda en ajustar
forecaster = Prophet().fit(datos_train['y'])
  
# Obtenemos predicciones
pred = forecaster.predict(fh)
pred_int = forecaster.predict_interval(fh, coverage=1-alpha/2)
timer_final = time.time()
tiempo = timer_final - timer_comienzo

# Calculamos MAPE
mape = mean_absolute_percentage_error(datos_test['y'], pred)

# Calculamos Interval Score
pred_int.columns = ['lower', 'upper']
score = interval_score(obs=datos_test['y'], lower=pred_int['lower'], upper=pred_int['upper'], alpha = alpha)

# Graficamos el pronostico
plt.plot(datos['ds'], datos['y'])
sns.lineplot(x = datos['ds'], y= pred, color = 'red', label = 'Prediccion')
plt.fill_between(
    datos_test['ds'], pred_int['lower'], pred_int['upper'], color = 'red', alpha = 0.3, label = f'IC: {1-alpha/2}%')


metricas.loc[len(metricas)] = ['Prophet', mape, score, tiempo]
```

## Machine learning

```{python}
# XGBoost

params = {
    "max_depth": [3,5],
    "learning_rate": 0.1,
    "n_estimators": 100
}

pred, mape, score, tiempo, resultados_xgb = Tuner(forecaster_fun= 'XGBoost', datos=datos, parametros=params, alpha= alpha)

metricas.loc[len(metricas)] = ['XGBoost', mape, score, tiempo]
```

```{python}
# LightGBM

params = {
    "max_depth": [3,5],
    "learning_rate": 0.1,
    "n_estimators": 100
}

pred, mape, score, tiempo, resultados_lightgbm = Tuner(forecaster_fun= 'LightGBM', datos=datos, parametros=params, alpha= alpha)

metricas.loc[len(metricas)] = ['LightGBM', mape, score, tiempo]
```

## Deep learning

```{python}
# LSTM

parametros = {
    'max_steps' : [20]}

pred, mape, score, tiempo, resultados_lstm = Tuner(forecaster_fun= 'LSTM', datos=datos, parametros= parametros, alpha= alpha)

metricas.loc[len(metricas)] = ['LSTM', mape, score, tiempo]
```

```{python}
# TIME GPT

parametros = {
    'finetune_loss' : ['mape']}

pred, mape, score, tiempo, resultados_gpt = Tuner(forecaster_fun= 'TimeGPT', datos=datos, parametros= parametros, alpha= alpha)

metricas.loc[len(metricas)] = ['TimeGPT', mape, score, tiempo]

```


```{python}
save_env(filename='Ambiente/modelizacion.pkl')
```
# ARREGLAR

- averiguar como funciona el coverage, (es un intervalo probabilistico siquiera?)

- Los intervalos de TimeGPt estan muy chicos

- Corregir los intervalos de XGBoost Y LSTM

- Agregar mas parametros a todos los modelos